<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Herman-blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Icaurs - Hexo Theme"><meta name="msapplication-TileImage" content="/img/favicon1.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Icaurs - Hexo Theme"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Herman-blog"><meta property="og:url" content="https://hermanteng19.github.io/"><meta property="og:site_name" content="Herman-blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://hermanteng19.github.io/img/og_image.png"><meta property="article:author" content="Herman Teng"><meta property="article:tag" content="Data, Analysis, JavaScript, Python, SQL, NoSQL, Web, Finance, Banking"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hermanteng19.github.io"},"headline":"Herman-blog","image":["https://hermanteng19.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Herman Teng"},"description":""}</script><link rel="icon" href="/img/favicon1.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/avatar1.png" alt="Herman-blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><a class="image is-7by3" href="/2022/06/10/Github-Pages-Hosts-A-Website-with-A-Custom-Domain/"><img class="fill" src="/img/github_docsify.jpg" alt="Github Pages Hosts A Website with A Custom Domain"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-06-10T21:26:45.000Z" title="2022-06-10T21:26:45.000Z">2022-06-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-06-10T21:29:57.864Z" title="2022-06-10T21:29:57.864Z">2022-06-10</time></span><span class="level-item">a few seconds read (About 5 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/10/Github-Pages-Hosts-A-Website-with-A-Custom-Domain/">Github Pages Hosts A Website with A Custom Domain</a></h1><div class="content"><p>This is a testing document.</p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2022/06/02/A-Little-Thoughts-on-DP-203-MS-Azure-Data-Engineering-Associate-Exam/"><img class="fill" src="/img/dp-203_certs.png" alt="A Little Thoughts on DP-203: MS Azure Data Engineering Associate Exam"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-06-03T02:11:57.000Z" title="2022-06-03T02:11:57.000Z">2022-06-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-06-03T21:03:23.426Z" title="2022-06-03T21:03:23.426Z">2022-06-03</time></span><span class="level-item">20 minutes read (About 2927 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/02/A-Little-Thoughts-on-DP-203-MS-Azure-Data-Engineering-Associate-Exam/">A Little Thoughts on DP-203: MS Azure Data Engineering Associate Exam</a></h1><div class="content"><p>The evolution of cloud computing over the past years has been becoming the most impacted event in both technology and business world. More and more companies from almost all industries such as financial, manufacture, education, Hi-Tech have migrated or are deploying their computing platform onto cloud, cloud provides the capabilities to be able to integrate company’s all resources into a focal place, which breaks the isolations and obstacles on all kinds of business operations and implementations, one of the most important digital assets among those resources is data. In that circumstance, data engineer with cloud computing background and experience becomes highly demanded in workplace and job market, and a shortcut to crack into this career path is getting a certification in the subject.<br>Each cloud provider like Amazon, Microsoft and Google offer a certification specialized to their data engineering services, a very first question is which one should we choose to start with, my answer is Microsoft Azure. Here are some reasons why I draw that conclusion</p>
<ul>
<li>Microsoft almost dominates business software and application therefore it has the most potential customers to choose its services to be able to make their business process being migrated transparently and consistently.</li>
<li>Microsoft Azure is constantly growing these years, market share growth rate is even over Amazon AWS. It is acquiring 1000 customers a day. Moreover, Azure Cloud is used by 95% of Fortune 500 companies.</li>
<li>Most of the skills are easily transferable to other Cloud Providers such as <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services">AWS</a> and <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/architecture/gcp-professional/services">Google Cloud</a></li>
</ul></div><a class="article-more button is-small is-size-7" href="/2022/06/02/A-Little-Thoughts-on-DP-203-MS-Azure-Data-Engineering-Associate-Exam/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2022/05/30/Online-Document-Resume-Deployment-on-Github-Pges-with-Docsify/"><img class="fill" src="/img/github_docsify.jpg" alt="Online Document/Resume Deployment on Github Pges with Docsify"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-05-31T02:42:02.000Z" title="2022-05-31T02:42:02.000Z">2022-05-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-06-02T02:13:27.501Z" title="2022-06-02T02:13:27.501Z">2022-06-01</time></span><span class="level-item">12 minutes read (About 1747 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/05/30/Online-Document-Resume-Deployment-on-Github-Pges-with-Docsify/">Online Document/Resume Deployment on Github Pges with Docsify</a></h1><div class="content"><p>Pandemic changes the world dramatically, digitalization and virtualization like tide are becoming unstoppable and deeply impact the way we are living. Meta is coming up and becomes prevalent under this circumstance. We are to be pushed to think seriously how to build personal platform to present ourselves and allocate our resources efficiently, one of the most important personal documents is <strong>resume</strong>,  but for people who don’t have development background that is never easy even so many technologies we can utilize, because the basic web technologies such as <code>html</code>, <code>css</code>, <code>javascript</code>, <code>node.js</code>, <code>mysql</code> and <code>mongodb</code> are still used as cornerstone and unavoidable, some web framework like <code>react.js</code>, <code>angular</code> and <code>vue.js</code> are even more overwhelming for non technical person. The question is now what is the most cost efficient way to deploy and manage online resources by using the least technology? Cost efficient here includes both economic and work effort, you might say there are a lot of web services like Wix.com to provide no coding and one stop web solution but it is expensive so that there is no perfect solution but least effort one, after given a quit a lot of tries, there is a way by using technologies with the shortest learning curve to be able to quickly deploy your digital resume online for free, they are</p>
<ul>
<li>Markdown</li>
<li>Basic git commands</li>
<li>Free github repository</li>
</ul></div><a class="article-more button is-small is-size-7" href="/2022/05/30/Online-Document-Resume-Deployment-on-Github-Pges-with-Docsify/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/"><img class="fill" src="/img/sparkpythonribbon.jpg" alt="Setup SPARK Environment Locally for Big Data Development"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-02-27T21:51:47.000Z" title="2021-02-27T21:51:47.000Z">2021-02-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-10T03:16:00.769Z" title="2021-06-10T03:16:00.769Z">2021-06-09</time></span><span class="level-item"><a class="link-muted" href="/categories/IT/">IT</a></span><span class="level-item">8 minutes read (About 1231 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/">Setup SPARK Environment Locally for Big Data Development</a></h1><div class="content"><p>Spark has been reported to be one of the most valuable tech skills to learn by data professionals and demand for Spark and Big Data skill has exploded in recent years. As one of the latest technologies in the big data space, Spark is quickly becoming one of the most powerful Big Data tools for data processing and machine learning, its ability to run programs up to 100x faster than Hadoop MapReduce in memory. </p>
<p>Unlike single program installation, Spark environment setup is not that straight forward but needs a series of installations and configurations with sequence order requirement and dependencies from operating system to software. The main purpose of Spark is dealing with Big Data which means data is too big to allocate into a single server but multiple servers to comprise cluster. Because it’s cluster environment so that Spark is naturally installed in Linux server, therefore, from the operating system perspective, Linux is the only option, which decide the entire process is going to heavy rely on CLI instead of GUI so that the basic Linux CLI command is required before rolling up your sleeves and get your hand dirty.</p>
<p>But if you don’t want to setup environment by yourself, there is also a good solution provided by Databrick. Databrick is a company started by the creator of Spark that provides clusters that run on the top of AWS and adds a convience of having a notebook system already set up and the ability to quickly add files either from storage like Amazon S3 or from your local computer. I has a free community version that supports a 6 GB cluster. Because it is a web-based service so that you can easily follow the wizard on Databrick web portal to finish setup. We don’t show that here because it’s out of our scope.</p>
<p>Before we begin, something is very necessary to emphasis here. Spark is written by Scala, Scala is written by Java, so we have to follow the sequence to make sure Java installed then followed by Scala. Now let’s start it.</p></div><a class="article-more button is-small is-size-7" href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/"><img class="fill" src="/img/pyvirtualenvribbon1.jpg" alt="Python Development Bootstrap on Managing Environment and Packages"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-10T13:20:54.000Z" title="2021-01-10T13:20:54.000Z">2021-01-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-10T03:27:34.284Z" title="2021-06-10T03:27:34.284Z">2021-06-09</time></span><span class="level-item"><a class="link-muted" href="/categories/IT-BI/">IT, BI</a></span><span class="level-item">29 minutes read (About 4276 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/">Python Development Bootstrap on Managing Environment and Packages</a></h1><div class="content"><p>If you have basic python experience and knowledge and willing to develop a real python application or product, I think that post is right for you. Let’s recap how do we start to use <code>Python</code>, we either go to official site to download python, install it on system wise then jump start to write “Hello, world!” or install <code>Anaconda</code> to get a long list of pre-installed libraries and then do the same thing. I would say that is totally ok by using python globally or system wise at the beginning stage, but as we accumulate enough python scripts and  do the real python project, we may found something really painful during our development</p>
<ul>
<li>Version confusion: different python versions reside together as well as pip versions</li>
<li>Package redundancy: too many libraries in one place, some of them you use only for some practice and never use them again</li>
<li>Package version conflict: that is the most severe one and headache, imagine<ul>
<li>your early development based on python2, you move to python3 afterwards, but you found your early applications break after you upgrade package version on which your python2 applications dependent, because those libraries are not backwards compatible</li>
<li>even all your development based on python3, you still stuck on the situation that  one library of your new development needs high version sub-dependency package but your early development rely on the same sub-dependency package but with lower version</li>
<li>when you work with team, you need to pass your work to other teammate to test, your application breaks due to there no unique environment and packages between your machines</li>
</ul>
</li>
</ul>
<p>Therefore, for real python production development, the first step should set up a proper virtual environment to be able to compatibility and collaboration before jump to coding. Thanks to community contributors, we have multiple choices to meet our different requirements and needs, there are 3 ways to do that, they are </p>
<ol>
<li>Python early official solution: venv (virtualenv) and pip</li>
<li>Python latest official solution: pipenv</li>
<li>Conda environment and package management tool</li>
</ol>
<p>now let’s see how do we do the configuration.</p></div><a class="article-more button is-small is-size-7" href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/01/08/Increase-Disk-Space-for-Linux-Virtual-Machine-Created-by-VMware/"><img class="fill" src="/img/linuxlvmribbon.jpg" alt="Increase Disk Space for Linux Virtual Machine Created by VMware"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-08T22:31:16.000Z" title="2021-01-08T22:31:16.000Z">2021-01-08</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-11-27T20:17:41.829Z" title="2021-11-27T20:17:41.829Z">2021-11-27</time></span><span class="level-item"><a class="link-muted" href="/categories/IT-BI/">IT, BI</a></span><span class="level-item">7 minutes read (About 1029 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/08/Increase-Disk-Space-for-Linux-Virtual-Machine-Created-by-VMware/">Increase Disk Space for Linux Virtual Machine Created by VMware</a></h1><div class="content"><p>Virtual machine is a major way to setup <code>Linus</code> development environment in <code>Windows PC</code>. One of the great things about newly Linux distro such as <code>RHEL, Ubuntu, Centos</code> etc is that most of them adopt to a LVM (Logical Volume Manager) filesystem which is a natural fit for Linux virtualized system like <code>VMware</code> or <code>VitualBox</code>.</p>
<p>The main characteristic for LVM is that it is able to adjust filesystem volume dynamically by integrating multiple partitions which feel like one partition on one disk after LVM adjustment, moreover, adding and remove partitions to increase and shrink disk space also become very easy than before and this feature applies virtualized hard drive and makes it very easy to grow the disk space within few steps setup. You might ask what is the point to do that, I would say if you make virtual machine as your main development environment, the disk space is going to run out quickly as time goes by when more and more tools and libraries are installed. I have my VM hard disk initial 20GB by default setting but after I setup all my environment items my root directory only has 300MB space left. </p>
<p>I will grow disk space with my Linux virtual machine to 40GB:</p>
<p>Before we make our hands dirty, it’s necessary to get some basic understanding about the LVM in terms of 3 key things, they are PV (physical volumes), VG (volume groups) and LV (logical volumes). LVM integrates multiple partitions or disks into a big independent partition (VG), then formats it to create multiple logical volumes (LV) to be able to mount filesystem.</p></div><a class="article-more button is-small is-size-7" href="/2021/01/08/Increase-Disk-Space-for-Linux-Virtual-Machine-Created-by-VMware/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/01/01/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-3/"><img class="fill" src="/img/partitionmergeribbon.jpg" alt="SQL Server Table Partitioning in Large Scale Data Warehouse 3"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-01T22:54:55.000Z" title="2021-01-01T22:54:55.000Z">2021-01-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-11-27T20:20:42.338Z" title="2021-11-27T20:20:42.338Z">2021-11-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Database-BI/">Database, BI</a></span><span class="level-item">16 minutes read (About 2474 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/01/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-3/">SQL Server Table Partitioning in Large Scale Data Warehouse 3</a></h1><div class="content"><p>This post is the last part on series of table partitioning, as plan this part is going to focus on some advanced topics like partition merge, split, conversion and performance optimization in terms of different business demands.</p>
<p>The first thing we will talk about might be interesting, we know one benefit of table partitioning is speed up loading and archiving data, we can easily feel the performance improvement on query against large data after table partitioning, but data archiving is not that apparent and it’s on the lower file system level to helps you mange data more on the backend efficiently by file group and database file. After all, we are intent to manage data through partitions and additionally, manage partitions through database files, but that is not that straight forward.</p></div><a class="article-more button is-small is-size-7" href="/2021/01/01/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-3/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/"><img class="fill" src="/img/partitionswitchribbon.webp" alt="SQL Server Table Partitioning in Large Scale Data Warehouse 2"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-26T17:06:21.000Z" title="2020-12-26T17:06:21.000Z">2020-12-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-11-27T22:41:43.851Z" title="2021-11-27T22:41:43.851Z">2021-11-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Database-BI/">Database, BI</a></span><span class="level-item">39 minutes read (About 5843 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/">SQL Server Table Partitioning in Large Scale Data Warehouse 2</a></h1><div class="content"><p>This post is part 2, we are focusing on design and create data process to extract, transform and load  (ETL) big amount data into partitioned tables to be able to integrate to SSIS package then operate ETL process by scheduled job automatically. </p>
<p>In this case, we suppose transaction data with 100 columns and 100 million records need to be loaded into data warehouse from staging table. Technically, we can do partition switching from staging table (non partitioned table) to data warehouse table (partitioned table), but under the business reality, we can’t just simply do it like that, because</p>
<ol>
<li>Source table and target table usually are in different database, it’s not able to switch data directly because of violating same file group condition by partition switching basic in part 1. </li>
<li>Staging table would be empty after partition switching, but the most of data transformations are applied to staging table then load final results into target table in data warehouse, so in business point of view, we can’t do partition switching from staging to target neither because big impact for entire daily, weekly or monthly data process.</li>
</ol>
<p>There might be another question: why don’t we bulk load data from source to target or what benefits do we get from partition switching? Technically, yes, we can do bulk insert, however, for such big volume of data  movement, the lead time is going to be hours (2-3 hours), if the process ran on business hours, it would cause big impact and it’s hard to tolerant by business users for critical data like transaction so from efficiency and performance perspective,  we have to leverage partition switching to deliver transaction data in short period of time without interrupt BI reports, dashboard refresh and business analysis.</p>
<h2 id="What-is-the-main-idea-for-production-operation"><a href="#What-is-the-main-idea-for-production-operation" class="headerlink" title="What is the main idea for production operation?"></a>What is the main idea for production operation?</h2><p>In order to promise transaction data is always available, we need to create a middle table to hold transaction data as source table for partition switching, we call that middle table as switch table. To satisfy all the requirements for partition switching, the switch table has to be created in as the same file group as target transaction table, identical columns, indexes, use the same partition column. We do the bulk insert from staging table to switch table then partition switch to target transaction table in data warehouse, as part 1 mentioned, this step will finish in flash as long as there is no blocking. At last, we drop switch table so the entire data process completes. Now let’s dig litter deeper on details for each steps.</p></div><a class="article-more button is-small is-size-7" href="/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/12/25/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-1/"><img class="fill" src="/img/tablepartitionribbon1.jpg" alt="SQL Server Table Partitioning in Large Scale Data Warehouse 1"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-26T02:20:12.000Z" title="2020-12-26T02:20:12.000Z">2020-12-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-26T03:27:02.537Z" title="2020-12-26T03:27:02.537Z">2020-12-25</time></span><span class="level-item"><a class="link-muted" href="/categories/Database-BI/">Database, BI</a></span><span class="level-item">36 minutes read (About 5428 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/25/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-1/">SQL Server Table Partitioning in Large Scale Data Warehouse 1</a></h1><div class="content"><p>This is a series articles to demonstrate table partitioning technology and how to apply it in a large scale data warehouse to improve database performance and even benefit for maintenance operation based on Microsoft SQL Server. This series is composed by three parts</p>
<ol>
<li>The first one is a fundamental introduction on basis knowledge</li>
<li>The second part is showcase the entire production workflow to apply table partitioning to large tables</li>
<li>Finally, it’s going to be advanced topic like partition merge, split, conversion and performance optimization in terms of different business demands</li>
</ol>
<h2 id="Table-Partitioning-The-Basics"><a href="#Table-Partitioning-The-Basics" class="headerlink" title="Table Partitioning The Basics"></a><em>Table Partitioning The Basics</em></h2><p>For this part, I will reference Cathrine Wilhelmsen’s work, she is  Microsoft Data Platform MVP, BimlHero Certified Expert, international speaker, author, blogger, and chronic volunteer. She loves data and coding, as well as teaching and sharing knowledge - oh, and sci-fi, chocolate, coffee, and cats :)</p>
<p>I learnt a lot and got many ideas from her blogs especially for table partitioning technology, below is her blog address, hope it can help you as well:blush:</p>
<p><strong><a target="_blank" rel="noopener" href="https://www.cathrinewilhelmsen.net/">Cathrine blog</a></strong></p></div><a class="article-more button is-small is-size-7" href="/2020/12/25/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-1/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/"><img class="fill" src="/img/hadoopsqlribbon.png" alt="Hadoop Data Side Load from SQL Server"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-23T01:41:30.000Z" title="2020-12-23T01:41:30.000Z">2020-12-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-11-27T23:26:25.153Z" title="2021-11-27T23:26:25.153Z">2021-11-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Data-BI/">Data, BI</a></span><span class="level-item">13 minutes read (About 1951 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/">Hadoop Data Side Load from SQL Server</a></h1><div class="content"><p>Agile development and DevOps bring flexibilities and quick solutions to support business intelligent in timely manners. A technical platform and its associated applications and tools are able to turnaround very quick so that business analysts and data scientists would be able to leverage them to do the data modeling or machine learning, but in the other side, unlike functions buildup, data sync across different platforms is not that easy and quick especially for large organizations.</p>
<h2 id="Background-and-the-gap-of-data-modeling-for-Hadoop-early-adopter"><a href="#Background-and-the-gap-of-data-modeling-for-Hadoop-early-adopter" class="headerlink" title="Background and the gap of data modeling for Hadoop early adopter"></a>Background and the gap of data modeling for Hadoop early adopter</h2><p>These years, <code>big data</code> and <code>Hadoop</code> are kind of trend for next generation data technic. Many companies adopt that as major data platform, but the most of data is still allocated in RDBMS data warehouse, business intention is to leverage high quality data in SQL database to build their analytical work in Hadoop, the data consistency is the first consideration from data perspective, but it is not a easy task because data is going to migrate to different platform with different operating system (from <code>Windows</code> to <code>Linux</code>). Technically, the best solution for the project is the build the direct connection from <code>SQL Server</code> and <code>SSIS</code> to <code>Hive</code> by using <code>Apache Sqoop</code> or utilize the JVM to build JDBC connection by JAVA, but for large organization, applying a new tool on production needs a quite lot approve work; developing JDBC connection facility also needs multiple level testing, those are taking a long time.</p>
<p>Therefore the solution is back to the foundation of the Hadoop - file system. Because SSIS cannot write to Hive directly using ODBC (before 2015 version). The alternative is to create a file with the appropriate file format and copy it directly to the Hadoop file system then use Hive command to write metadata to <code>Hive metastore</code>, the data will show up in the Hive table and also available in <code>Cloudera Impala</code>. </p></div><a class="article-more button is-small is-size-7" href="/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/#more">Read more</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Previous</a></div><div class="pagination-next"><a href="/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar1.png" alt="Herman"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Herman</p><p class="is-size-6 is-block">Data Analyst&amp;Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Toronto, Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">20</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/HermanTeng19" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/HermanTeng19"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/2022/06/10/Github-Pages-Hosts-A-Website-with-A-Custom-Domain/"><img src="/img/filesoncloud.jpg" alt="Github Pages Hosts A Website with A Custom Domain"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-06-10T21:26:45.000Z">2022-06-10</time></p><p class="title"><a href="/2022/06/10/Github-Pages-Hosts-A-Website-with-A-Custom-Domain/">Github Pages Hosts A Website with A Custom Domain</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2022/06/02/A-Little-Thoughts-on-DP-203-MS-Azure-Data-Engineering-Associate-Exam/"><img src="/img/microsoft-certified-azure-data-engineer-associate.png" alt="A Little Thoughts on DP-203: MS Azure Data Engineering Associate Exam"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-06-03T02:11:57.000Z">2022-06-02</time></p><p class="title"><a href="/2022/06/02/A-Little-Thoughts-on-DP-203-MS-Azure-Data-Engineering-Associate-Exam/">A Little Thoughts on DP-203: MS Azure Data Engineering Associate Exam</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2022/05/30/Online-Document-Resume-Deployment-on-Github-Pges-with-Docsify/"><img src="/img/filesoncloud.jpg" alt="Online Document/Resume Deployment on Github Pges with Docsify"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-05-31T02:42:02.000Z">2022-05-30</time></p><p class="title"><a href="/2022/05/30/Online-Document-Resume-Deployment-on-Github-Pges-with-Docsify/">Online Document/Resume Deployment on Github Pges with Docsify</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/"><img src="/img/sparkicon.png" alt="Setup SPARK Environment Locally for Big Data Development"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-02-27T21:51:47.000Z">2021-02-27</time></p><p class="title"><a href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/">Setup SPARK Environment Locally for Big Data Development</a></p><p class="categories"><a href="/categories/IT/">IT</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/"><img src="/img/pyvirtualenvicon.png" alt="Python Development Bootstrap on Managing Environment and Packages"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-01-10T13:20:54.000Z">2021-01-10</time></p><p class="title"><a href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/">Python Development Bootstrap on Managing Environment and Packages</a></p><p class="categories"><a href="/categories/IT-BI/">IT, BI</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Data/"><span class="level-start"><span class="level-item">Data</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Data-BI/"><span class="level-start"><span class="level-item">Data, BI</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Database-BI/"><span class="level-start"><span class="level-item">Database, BI</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/IT/"><span class="level-start"><span class="level-item">IT</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/IT-BI/"><span class="level-start"><span class="level-item">IT, BI</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/data-BI/"><span class="level-start"><span class="level-item">data, BI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Github-Page-web-hosting-domain-DNS/"><span class="tag">Github Page, web hosting, domain, DNS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hadoop-Hive-Impala-HDFS-SQL-Server-SSIS/"><span class="tag">Hadoop, Hive, Impala, HDFS, SQL Server, SSIS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-Centos-network-config/"><span class="tag">Linux, Centos, network, config</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-LVM-VMware-Disk-management/"><span class="tag">Linux, LVM, VMware, Disk management</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-MySQL-Config-Database/"><span class="tag">Linux, MySQL, Config, Database</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-centos-ubuntu-network/"><span class="tag">Linux, centos, ubuntu, network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Microsoft-Azure-Cloud-Certificated-Data-engineering/"><span class="tag">Microsoft Azure, Cloud, Certificated, Data engineering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python-Pandas-SQL-ETL/"><span class="tag">Python, Pandas, SQL, ETL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python-virtual-environment/"><span class="tag">Python, virtual environment</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL-Server-database-monitoring-system-optimization/"><span class="tag">SQL Server, database, monitoring, system optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL-Server-partitioning/"><span class="tag">SQL Server, partitioning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL-query-SQL-Server-optimization/"><span class="tag">SQL, query, SQL Server, optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docsify-online-document-github-pages/"><span class="tag">docsify, online document, github pages</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/file-system-csv/"><span class="tag">file system, csv</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo-blog-online-image-config-Node-js-router/"><span class="tag">hexo, blog, online image, config, Node.js, router</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/oracle-SSIS-SAS-config/"><span class="tag">oracle, SSIS, SAS, config</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python-automation-ETL-Excel/"><span class="tag">python, automation, ETL, Excel</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python-sql-db2-CLI-SSIS-config/"><span class="tag">python, sql, db2, CLI, SSIS, config</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sandbox-SQL-SQL-Server-database/"><span class="tag">sandbox, SQL, SQL Server, database</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/spark-pyspark-environment-linux/"><span class="tag">spark, pyspark, environment, linux</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/avatar1.png" alt="Herman-blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Herman Teng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>