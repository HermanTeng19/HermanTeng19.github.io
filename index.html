<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Herman-blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Icaurs - Hexo Theme"><meta name="msapplication-TileImage" content="/img/favicon1.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Icaurs - Hexo Theme"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Herman-blog"><meta property="og:url" content="https://hermanteng19.github.io/"><meta property="og:site_name" content="Herman-blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://hermanteng19.github.io/img/og_image.png"><meta property="article:author" content="Herman Teng"><meta property="article:tag" content="Data, Analysis, JavaScript, Python, SQL, NoSQL, Web, Finance, Banking"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hermanteng19.github.io"},"headline":"Herman-blog","image":["https://hermanteng19.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Herman Teng"},"description":""}</script><link rel="icon" href="/img/favicon1.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/avatar1.png" alt="Herman-blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/"><img class="fill" src="/img/sparkpythonribbon.jpg" alt="Setup SPARK Environment Locally for Big Data Development"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-02-27T21:51:47.000Z" title="2021-02-27T21:51:47.000Z">2021-02-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-10T03:16:00.769Z" title="2021-06-10T03:16:00.769Z">2021-06-09</time></span><span class="level-item"><a class="link-muted" href="/categories/IT/">IT</a></span><span class="level-item">8 minutes read (About 1231 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/">Setup SPARK Environment Locally for Big Data Development</a></h1><div class="content"><p>Spark has been reported to be one of the most valuable tech skills to learn by data professionals and demand for Spark and Big Data skill has exploded in recent years. As one of the latest technologies in the big data space, Spark is quickly becoming one of the most powerful Big Data tools for data processing and machine learning, its ability to run programs up to 100x faster than Hadoop MapReduce in memory. </p>
<p>Unlike single program installation, Spark environment setup is not that straight forward but needs a series of installations and configurations with sequence order requirement and dependencies from operating system to software. The main purpose of Spark is dealing with Big Data which means data is too big to allocate into a single server but multiple servers to comprise cluster. Because it’s cluster environment so that Spark is naturally installed in Linux server, therefore, from the operating system perspective, Linux is the only option, which decide the entire process is going to heavy rely on CLI instead of GUI so that the basic Linux CLI command is required before rolling up your sleeves and get your hand dirty.</p>
<p>But if you don’t want to setup environment by yourself, there is also a good solution provided by Databrick. Databrick is a company started by the creator of Spark that provides clusters that run on the top of AWS and adds a convience of having a notebook system already set up and the ability to quickly add files either from storage like Amazon S3 or from your local computer. I has a free community version that supports a 6 GB cluster. Because it is a web-based service so that you can easily follow the wizard on Databrick web portal to finish setup. We don’t show that here because it’s out of our scope.</p>
<p>Before we begin, something is very necessary to emphasis here. Spark is written by Scala, Scala is written by Java, so we have to follow the sequence to make sure Java installed then followed by Scala. Now let’s start it.</p></div><a class="article-more button is-small is-size-7" href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/"><img class="fill" src="/img/pyvirtualenvribbon1.jpg" alt="Python Development Bootstrap on Managing Environment and Packages"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-10T13:20:54.000Z" title="2021-01-10T13:20:54.000Z">2021-01-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-10T03:27:34.284Z" title="2021-06-10T03:27:34.284Z">2021-06-09</time></span><span class="level-item"><a class="link-muted" href="/categories/IT-BI/">IT, BI</a></span><span class="level-item">29 minutes read (About 4276 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/">Python Development Bootstrap on Managing Environment and Packages</a></h1><div class="content"><p>If you have basic python experience and knowledge and willing to develop a real python application or product, I think that post is right for you. Let’s recap how do we start to use <code>Python</code>, we either go to official site to download python, install it on system wise then jump start to write “Hello, world!” or install <code>Anaconda</code> to get a long list of pre-installed libraries and then do the same thing. I would say that is totally ok by using python globally or system wise at the beginning stage, but as we accumulate enough python scripts and  do the real python project, we may found something really painful during our development</p>
<ul>
<li>Version confusion: different python versions reside together as well as pip versions</li>
<li>Package redundancy: too many libraries in one place, some of them you use only for some practice and never use them again</li>
<li>Package version conflict: that is the most severe one and headache, imagine<ul>
<li>your early development based on python2, you move to python3 afterwards, but you found your early applications break after you upgrade package version on which your python2 applications dependent, because those libraries are not backwards compatible</li>
<li>even all your development based on python3, you still stuck on the situation that  one library of your new development needs high version sub-dependency package but your early development rely on the same sub-dependency package but with lower version</li>
<li>when you work with team, you need to pass your work to other teammate to test, your application breaks due to there no unique environment and packages between your machines</li>
</ul>
</li>
</ul>
<p>Therefore, for real python production development, the first step should set up a proper virtual environment to be able to compatibility and collaboration before jump to coding. Thanks to community contributors, we have multiple choices to meet our different requirements and needs, there are 3 ways to do that, they are </p>
<ol>
<li>Python early official solution: venv (virtualenv) and pip</li>
<li>Python latest official solution: pipenv</li>
<li>Conda environment and package management tool</li>
</ol>
<p>now let’s see how do we do the configuration.</p></div><a class="article-more button is-small is-size-7" href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/01/08/Increase-Disk-Space-for-Linux-Virtual-Machine-Created-by-VMware/"><img class="fill" src="/img/linuxlvmribbon.jpg" alt="Increase Disk Space for Linux Virtual Machine Created by VMware"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-08T22:31:16.000Z" title="2021-01-08T22:31:16.000Z">2021-01-08</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-11-27T20:17:41.829Z" title="2021-11-27T20:17:41.829Z">2021-11-27</time></span><span class="level-item"><a class="link-muted" href="/categories/IT-BI/">IT, BI</a></span><span class="level-item">7 minutes read (About 1029 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/08/Increase-Disk-Space-for-Linux-Virtual-Machine-Created-by-VMware/">Increase Disk Space for Linux Virtual Machine Created by VMware</a></h1><div class="content"><p>Virtual machine is a major way to setup <code>Linus</code> development environment in <code>Windows PC</code>. One of the great things about newly Linux distro such as <code>RHEL, Ubuntu, Centos</code> etc is that most of them adopt to a LVM (Logical Volume Manager) filesystem which is a natural fit for Linux virtualized system like <code>VMware</code> or <code>VitualBox</code>.</p>
<p>The main characteristic for LVM is that it is able to adjust filesystem volume dynamically by integrating multiple partitions which feel like one partition on one disk after LVM adjustment, moreover, adding and remove partitions to increase and shrink disk space also become very easy than before and this feature applies virtualized hard drive and makes it very easy to grow the disk space within few steps setup. You might ask what is the point to do that, I would say if you make virtual machine as your main development environment, the disk space is going to run out quickly as time goes by when more and more tools and libraries are installed. I have my VM hard disk initial 20GB by default setting but after I setup all my environment items my root directory only has 300MB space left. </p>
<p>I will grow disk space with my Linux virtual machine to 40GB:</p>
<p>Before we make our hands dirty, it’s necessary to get some basic understanding about the LVM in terms of 3 key things, they are PV (physical volumes), VG (volume groups) and LV (logical volumes). LVM integrates multiple partitions or disks into a big independent partition (VG), then formats it to create multiple logical volumes (LV) to be able to mount filesystem.</p></div><a class="article-more button is-small is-size-7" href="/2021/01/08/Increase-Disk-Space-for-Linux-Virtual-Machine-Created-by-VMware/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/01/01/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-3/"><img class="fill" src="/img/partitionmergeribbon.jpg" alt="SQL Server Table Partitioning in Large Scale Data Warehouse 3"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-01T22:54:55.000Z" title="2021-01-01T22:54:55.000Z">2021-01-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-11-27T20:20:42.338Z" title="2021-11-27T20:20:42.338Z">2021-11-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Database-BI/">Database, BI</a></span><span class="level-item">16 minutes read (About 2474 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/01/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-3/">SQL Server Table Partitioning in Large Scale Data Warehouse 3</a></h1><div class="content"><p>This post is the last part on series of table partitioning, as plan this part is going to focus on some advanced topics like partition merge, split, conversion and performance optimization in terms of different business demands.</p>
<p>The first thing we will talk about might be interesting, we know one benefit of table partitioning is speed up loading and archiving data, we can easily feel the performance improvement on query against large data after table partitioning, but data archiving is not that apparent and it’s on the lower file system level to helps you mange data more on the backend efficiently by file group and database file. After all, we are intent to manage data through partitions and additionally, manage partitions through database files, but that is not that straight forward.</p></div><a class="article-more button is-small is-size-7" href="/2021/01/01/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-3/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/"><img class="fill" src="/img/partitionswitchribbon.webp" alt="SQL Server Table Partitioning in Large Scale Data Warehouse 2"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-26T17:06:21.000Z" title="2020-12-26T17:06:21.000Z">2020-12-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-11-27T22:41:43.851Z" title="2021-11-27T22:41:43.851Z">2021-11-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Database-BI/">Database, BI</a></span><span class="level-item">39 minutes read (About 5843 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/">SQL Server Table Partitioning in Large Scale Data Warehouse 2</a></h1><div class="content"><p>This post is part 2, we are focusing on design and create data process to extract, transform and load  (ETL) big amount data into partitioned tables to be able to integrate to SSIS package then operate ETL process by scheduled job automatically. </p>
<p>In this case, we suppose transaction data with 100 columns and 100 million records need to be loaded into data warehouse from staging table. Technically, we can do partition switching from staging table (non partitioned table) to data warehouse table (partitioned table), but under the business reality, we can’t just simply do it like that, because</p>
<ol>
<li>Source table and target table usually are in different database, it’s not able to switch data directly because of violating same file group condition by partition switching basic in part 1. </li>
<li>Staging table would be empty after partition switching, but the most of data transformations are applied to staging table then load final results into target table in data warehouse, so in business point of view, we can’t do partition switching from staging to target neither because big impact for entire daily, weekly or monthly data process.</li>
</ol>
<p>There might be another question: why don’t we bulk load data from source to target or what benefits do we get from partition switching? Technically, yes, we can do bulk insert, however, for such big volume of data  movement, the lead time is going to be hours (2-3 hours), if the process ran on business hours, it would cause big impact and it’s hard to tolerant by business users for critical data like transaction so from efficiency and performance perspective,  we have to leverage partition switching to deliver transaction data in short period of time without interrupt BI reports, dashboard refresh and business analysis.</p>
<h2 id="What-is-the-main-idea-for-production-operation"><a href="#What-is-the-main-idea-for-production-operation" class="headerlink" title="What is the main idea for production operation?"></a>What is the main idea for production operation?</h2><p>In order to promise transaction data is always available, we need to create a middle table to hold transaction data as source table for partition switching, we call that middle table as switch table. To satisfy all the requirements for partition switching, the switch table has to be created in as the same file group as target transaction table, identical columns, indexes, use the same partition column. We do the bulk insert from staging table to switch table then partition switch to target transaction table in data warehouse, as part 1 mentioned, this step will finish in flash as long as there is no blocking. At last, we drop switch table so the entire data process completes. Now let’s dig litter deeper on details for each steps.</p></div><a class="article-more button is-small is-size-7" href="/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/12/25/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-1/"><img class="fill" src="/img/tablepartitionribbon1.jpg" alt="SQL Server Table Partitioning in Large Scale Data Warehouse 1"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-26T02:20:12.000Z" title="2020-12-26T02:20:12.000Z">2020-12-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-26T03:27:02.537Z" title="2020-12-26T03:27:02.537Z">2020-12-25</time></span><span class="level-item"><a class="link-muted" href="/categories/Database-BI/">Database, BI</a></span><span class="level-item">36 minutes read (About 5428 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/25/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-1/">SQL Server Table Partitioning in Large Scale Data Warehouse 1</a></h1><div class="content"><p>This is a series articles to demonstrate table partitioning technology and how to apply it in a large scale data warehouse to improve database performance and even benefit for maintenance operation based on Microsoft SQL Server. This series is composed by three parts</p>
<ol>
<li>The first one is a fundamental introduction on basis knowledge</li>
<li>The second part is showcase the entire production workflow to apply table partitioning to large tables</li>
<li>Finally, it’s going to be advanced topic like partition merge, split, conversion and performance optimization in terms of different business demands</li>
</ol>
<h2 id="Table-Partitioning-The-Basics"><a href="#Table-Partitioning-The-Basics" class="headerlink" title="Table Partitioning The Basics"></a><em>Table Partitioning The Basics</em></h2><p>For this part, I will reference Cathrine Wilhelmsen’s work, she is  Microsoft Data Platform MVP, BimlHero Certified Expert, international speaker, author, blogger, and chronic volunteer. She loves data and coding, as well as teaching and sharing knowledge - oh, and sci-fi, chocolate, coffee, and cats :)</p>
<p>I learnt a lot and got many ideas from her blogs especially for table partitioning technology, below is her blog address, hope it can help you as well:blush:</p>
<p><strong><a target="_blank" rel="noopener" href="https://www.cathrinewilhelmsen.net/">Cathrine blog</a></strong></p></div><a class="article-more button is-small is-size-7" href="/2020/12/25/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-1/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/"><img class="fill" src="/img/hadoopsqlribbon.png" alt="Hadoop Data Side Load from SQL Server"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-23T01:41:30.000Z" title="2020-12-23T01:41:30.000Z">2020-12-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-23T19:35:32.788Z" title="2020-12-23T19:35:32.788Z">2020-12-23</time></span><span class="level-item"><a class="link-muted" href="/categories/Data-BI/">Data, BI</a></span><span class="level-item">13 minutes read (About 1958 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/">Hadoop Data Side Load from SQL Server</a></h1><div class="content"><p>Agile development and DevOps bring flexibilities and quick solutions to support business intelligent in timely manners. A technical platform and its associated applications and tools are able to turnaround very quick so that business analysts and data scientists would be able to leverage them to do the data modeling or machine learning, but in the other side, unlike functions buildup, data sync across different platforms is not that easy and quick especially for large organizations.</p>
<h2 id="Background-and-the-gap-of-data-modeling-for-Hadoop-early-adopter"><a href="#Background-and-the-gap-of-data-modeling-for-Hadoop-early-adopter" class="headerlink" title="Background and the gap of data modeling for Hadoop early adopter"></a>Background and the gap of data modeling for Hadoop early adopter</h2><p>These years, <code>big data</code> and <code>Hadoop</code> are kind of trend for next generation data technic. Many companies adopt that as major data platform, but the most of data is still allocated in RDBMS data warehouse, business intention is to leverage high quality data in SQL database to build their analytical work in Hadoop, the data consistency is the first consideration from data perspective, but it is not a easy task because data is going to migrate to different platform with different operating system (from <code>Windows</code> to <code>Linux</code>). Technically, the best solution for the project is the build the direct connection from <code>SQL Server</code> and <code>SSIS</code> to <code>Hive</code> by using <code>Apache Sqoop</code> or utilize the JVM to build JDBC connection by JAVA, but for large organization, applying a new tool on production needs a quite lot approve work; developing JDBC connection facility also needs multiple level testing, those are taking a long time.</p>
<p>Therefore the solution is back to the foundation of the Hadoop - file system. Because SSIS cannot write to Hive directly using ODBC (before 2015 version). The alternative is to create a file with the appropriate file format and copy it directly to the Hadoop file system then use Hive command to write metadata to <code>Hive metastore</code>, the data will show up in the Hive table and also available in <code>Cloudera Impala</code>. </p></div><a class="article-more button is-small is-size-7" href="/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/12/20/Data-Manipulation-and-ETL-with-Pandas/"><img class="fill" src="/img/pandassqlribbon.jpg" alt="Data Manipulation and ETL with Pandas"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-21T02:18:08.000Z" title="2020-12-21T02:18:08.000Z">2020-12-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-21T16:15:41.864Z" title="2020-12-21T16:15:41.864Z">2020-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/data-BI/">data, BI</a></span><span class="level-item">7 minutes read (About 984 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/20/Data-Manipulation-and-ETL-with-Pandas/">Data Manipulation and ETL with Pandas</a></h1><div class="content"><p>Pandas is the most used library in Python to manipulate data and deal with data transformation, by leveraging <code>Pandas</code> in memory data frame and abundant build-in functions in terms of data frame, it almost can handle all kinds of ETL task. We are going to talk about a data process to read input data from <code>Excel Spreadsheet</code>, make some data transformations by business requirement then load reporting data into <code>SQL Server</code> database.</p></div><a class="article-more button is-small is-size-7" href="/2020/12/20/Data-Manipulation-and-ETL-with-Pandas/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/12/17/Blocking-Process-Monitoring-and-Auto-Email-Notification-in-SQL-Server/"><img class="fill" src="/img/blockingribbon.webp" alt="Blocking Process Monitoring and Auto Email Notification in SQL Server"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-18T02:54:30.000Z" title="2020-12-18T02:54:30.000Z">2020-12-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-21T03:59:58.468Z" title="2020-12-21T03:59:58.468Z">2020-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/IT-BI/">IT, BI</a></span><span class="level-item">11 minutes read (About 1699 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/17/Blocking-Process-Monitoring-and-Auto-Email-Notification-in-SQL-Server/">Blocking Process Monitoring and Auto Email Notification in SQL Server</a></h1><div class="content"><p>From function perspective, to maintain a large scale business data warehouse is for making database system stable, robust and fast, which is a essential part to boost business team productivity and performance. However, for business users, the fundamental thing is <strong>data</strong>, so data can be delivered in high frequency and in time is the cornerstone for all business analysis and BI reporting. Obviously, the primary mandate for data management team is highly monitor ETL jobs to promise data process running well and smooth and never being blocked or corrupt by using process.</p>
<p>In this article, I am going to talk about how to build up a ETL job monitoring system to watch user query automatically in designed frequency. To accomplish this task, we need </p>
<ol>
<li>Create a view to collect user query in real time</li>
<li>Create a stored procedure to detect blocking in different situations</li>
<li>Create another stored procedure to handle the notification email sending</li>
<li>Create a console script to overall control the workflow</li>
</ol></div><a class="article-more button is-small is-size-7" href="/2020/12/17/Blocking-Process-Monitoring-and-Auto-Email-Notification-in-SQL-Server/#more">Read more</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/12/15/Automation-Process-for-Email-Attachment-Excel-in-Python/"><img class="fill" src="/img/pythonexcelribbon.webp" alt="Automation Process for Email Attachment Excel in Python"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-16T03:28:10.000Z" title="2020-12-16T03:28:10.000Z">2020-12-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-20T15:54:34.432Z" title="2020-12-20T15:54:34.432Z">2020-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/Data-BI/">Data, BI</a></span><span class="level-item">11 minutes read (About 1691 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Automation-Process-for-Email-Attachment-Excel-in-Python/">Automation Process for Email Attachment Excel in Python</a></h1><div class="content"><p>Working with business data, <strong>Excel spreadsheet</strong> is the most common file type you might deal with in daily basis, because Excel is a dominated application in the business world. What is the most used way to transfer those Excel files for business operation team, obviously, it’s <strong>Outlook</strong>, because email attachment is the easiest way for business team to share data and reports. Following this business common logic and convention, you may get quite a lot Excel files from email attachment when you involved into a business initiative or project to design a data solution for BI reporting and business analysis.</p>
<p>The pain points is too much manual work dragging down efficiency of data availability and also increasing the possibility of human error. Imagine, every day get data from email attachment, you need to check your inbox every once for a while, then download those files from attachment, open Excel to edit data or rename file to meet data process requirement such as remove the protected password, after those preparation works all done, push data to NAS drive, finally, launch the job to proceed the data. It’s not surprise that how easily you might make mistake because any single step contains error would cause the whole process failed.</p>
<p>It’s very necessary to automate the whole data process if business project turns to BAU (Business As Usual) program and you have to proceed data in regular ongoing basis. <code>Python</code> and <code>Windows Task Scheduler</code> provides a simple and fast way to solve this problem, now let’s take a look.</p>
<p>Overall speaking, this task can be broken down by a couple of steps:</p>
<ol>
<li>Access Outlook to read email and download the attachment</li>
<li>Remove Excel file protected password (it’s common in business file to protect data privacy)</li>
<li>Manipulate and edit Excel file</li>
<li>Copy file to NAS drive</li>
<li>Setup the Windows Task Scheduler to run python script automatically.</li>
</ol>
<p><img src="https://p.130014.xyz/2020/12/17/emailexcelmindset.png" alt="emailexcelmindset.png"></p></div><a class="article-more button is-small is-size-7" href="/2020/12/15/Automation-Process-for-Email-Attachment-Excel-in-Python/#more">Read more</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Previous</a></div><div class="pagination-next"><a href="/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar1.png" alt="Herman"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Herman</p><p class="is-size-6 is-block">Data Analyst&amp;Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Toronto, Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">17</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/HermanTeng19" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/HermanTeng19"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/"><img src="/img/sparkicon.png" alt="Setup SPARK Environment Locally for Big Data Development"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-02-27T21:51:47.000Z">2021-02-27</time></p><p class="title"><a href="/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/">Setup SPARK Environment Locally for Big Data Development</a></p><p class="categories"><a href="/categories/IT/">IT</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/"><img src="/img/pyvirtualenvicon.png" alt="Python Development Bootstrap on Managing Environment and Packages"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-01-10T13:20:54.000Z">2021-01-10</time></p><p class="title"><a href="/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/">Python Development Bootstrap on Managing Environment and Packages</a></p><p class="categories"><a href="/categories/IT-BI/">IT, BI</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/01/08/Increase-Disk-Space-for-Linux-Virtual-Machine-Created-by-VMware/"><img src="/img/linuxlvmicon.jpg" alt="Increase Disk Space for Linux Virtual Machine Created by VMware"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-01-08T22:31:16.000Z">2021-01-08</time></p><p class="title"><a href="/2021/01/08/Increase-Disk-Space-for-Linux-Virtual-Machine-Created-by-VMware/">Increase Disk Space for Linux Virtual Machine Created by VMware</a></p><p class="categories"><a href="/categories/IT-BI/">IT, BI</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/01/01/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-3/"><img src="/img/partitionmergeribbon.png" alt="SQL Server Table Partitioning in Large Scale Data Warehouse 3"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-01-01T22:54:55.000Z">2021-01-01</time></p><p class="title"><a href="/2021/01/01/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-3/">SQL Server Table Partitioning in Large Scale Data Warehouse 3</a></p><p class="categories"><a href="/categories/Database-BI/">Database, BI</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/"><img src="/img/partitionswitchicon.png" alt="SQL Server Table Partitioning in Large Scale Data Warehouse 2"></a></figure><div class="media-content"><p class="date"><time dateTime="2020-12-26T17:06:21.000Z">2020-12-26</time></p><p class="title"><a href="/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/">SQL Server Table Partitioning in Large Scale Data Warehouse 2</a></p><p class="categories"><a href="/categories/Database-BI/">Database, BI</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Data/"><span class="level-start"><span class="level-item">Data</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Data-BI/"><span class="level-start"><span class="level-item">Data, BI</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Database-BI/"><span class="level-start"><span class="level-item">Database, BI</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/IT/"><span class="level-start"><span class="level-item">IT</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/IT-BI/"><span class="level-start"><span class="level-item">IT, BI</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/data-BI/"><span class="level-start"><span class="level-item">data, BI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Hadoop-Hive-Impala-HDFS-SQL-Server-SSIS/"><span class="tag">Hadoop, Hive, Impala, HDFS, SQL Server, SSIS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-Centos-network-config/"><span class="tag">Linux, Centos, network, config</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-LVM-VMware-Disk-management/"><span class="tag">Linux, LVM, VMware, Disk management</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-MySQL-Config-Database/"><span class="tag">Linux, MySQL, Config, Database</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-centos-ubuntu-network/"><span class="tag">Linux, centos, ubuntu, network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python-Pandas-SQL-ETL/"><span class="tag">Python, Pandas, SQL, ETL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python-virtual-environment/"><span class="tag">Python, virtual environment</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL-Server-database-monitoring-system-optimization/"><span class="tag">SQL Server, database, monitoring, system optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL-Server-partitioning/"><span class="tag">SQL Server, partitioning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL-query-SQL-Server-optimization/"><span class="tag">SQL, query, SQL Server, optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/file-system-csv/"><span class="tag">file system, csv</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo-blog-online-image-config-Node-js-router/"><span class="tag">hexo, blog, online image, config, Node.js, router</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/oracle-SSIS-SAS-config/"><span class="tag">oracle, SSIS, SAS, config</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python-automation-ETL-Excel/"><span class="tag">python, automation, ETL, Excel</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python-sql-db2-CLI-SSIS-config/"><span class="tag">python, sql, db2, CLI, SSIS, config</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sandbox-SQL-SQL-Server-database/"><span class="tag">sandbox, SQL, SQL Server, database</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/spark-pyspark-environment-linux/"><span class="tag">spark, pyspark, environment, linux</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/avatar1.png" alt="Herman-blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Herman Teng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>