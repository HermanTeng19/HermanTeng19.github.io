<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Hadoop Data Side Load from SQL Server - Herman-blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Icaurs - Hexo Theme"><meta name="msapplication-TileImage" content="/img/favicon1.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Icaurs - Hexo Theme"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="Agile development and DevOps bring flexibilities and quick solutions to support business intelligent in timely manners. A technical platform and its associated applications and tools are able to turna"><meta property="og:type" content="blog"><meta property="og:title" content="Hadoop Data Side Load from SQL Server"><meta property="og:url" content="https://hermanteng19.github.io/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/"><meta property="og:site_name" content="Herman-blog"><meta property="og:description" content="Agile development and DevOps bring flexibilities and quick solutions to support business intelligent in timely manners. A technical platform and its associated applications and tools are able to turna"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://hermanteng19.github.io/img/hadoopsqlribbon.png"><meta property="article:published_time" content="2020-12-23T01:41:30.000Z"><meta property="article:modified_time" content="2021-11-27T23:26:25.153Z"><meta property="article:author" content="Herman Teng"><meta property="article:tag" content="Hadoop, Hive, Impala, HDFS, SQL Server, SSIS"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/hadoopsqlribbon.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hermanteng19.github.io/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/"},"headline":"Herman-blog","image":["https://hermanteng19.github.io/img/hadoopsqlribbon.png"],"datePublished":"2020-12-23T01:41:30.000Z","dateModified":"2021-11-27T23:26:25.153Z","author":{"@type":"Person","name":"Herman Teng"},"description":"Agile development and DevOps bring flexibilities and quick solutions to support business intelligent in timely manners. A technical platform and its associated applications and tools are able to turna"}</script><link rel="canonical" href="https://hermanteng19.github.io/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/"><link rel="icon" href="/img/favicon1.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/avatar1.png" alt="Herman-blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/hadoopsqlribbon.png" alt="Hadoop Data Side Load from SQL Server"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-23T01:41:30.000Z" title="2020-12-23T01:41:30.000Z">2020-12-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-11-27T23:26:25.153Z" title="2021-11-27T23:26:25.153Z">2021-11-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Data-BI/">Data, BI</a></span><span class="level-item">13 minutes read (About 1951 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Hadoop Data Side Load from SQL Server</h1><div class="content"><p>Agile development and DevOps bring flexibilities and quick solutions to support business intelligent in timely manners. A technical platform and its associated applications and tools are able to turnaround very quick so that business analysts and data scientists would be able to leverage them to do the data modeling or machine learning, but in the other side, unlike functions buildup, data sync across different platforms is not that easy and quick especially for large organizations.</p>
<h2 id="Background-and-the-gap-of-data-modeling-for-Hadoop-early-adopter"><a href="#Background-and-the-gap-of-data-modeling-for-Hadoop-early-adopter" class="headerlink" title="Background and the gap of data modeling for Hadoop early adopter"></a>Background and the gap of data modeling for Hadoop early adopter</h2><p>These years, <code>big data</code> and <code>Hadoop</code> are kind of trend for next generation data technic. Many companies adopt that as major data platform, but the most of data is still allocated in RDBMS data warehouse, business intention is to leverage high quality data in SQL database to build their analytical work in Hadoop, the data consistency is the first consideration from data perspective, but it is not a easy task because data is going to migrate to different platform with different operating system (from <code>Windows</code> to <code>Linux</code>). Technically, the best solution for the project is the build the direct connection from <code>SQL Server</code> and <code>SSIS</code> to <code>Hive</code> by using <code>Apache Sqoop</code> or utilize the JVM to build JDBC connection by JAVA, but for large organization, applying a new tool on production needs a quite lot approve work; developing JDBC connection facility also needs multiple level testing, those are taking a long time.</p>
<p>Therefore the solution is back to the foundation of the Hadoop - file system. Because SSIS cannot write to Hive directly using ODBC (before 2015 version). The alternative is to create a file with the appropriate file format and copy it directly to the Hadoop file system then use Hive command to write metadata to <code>Hive metastore</code>, the data will show up in the Hive table and also available in <code>Cloudera Impala</code>. </p>
<a id="more"></a>

<h2 id="File-operation-for-data-migration-from-SQL-to-Hadoop"><a href="#File-operation-for-data-migration-from-SQL-to-Hadoop" class="headerlink" title="File operation for data migration from SQL to Hadoop"></a>File operation for data migration from SQL to Hadoop</h2><p>In our case, moving files can be a litter more complex because of crossing different operating system and platform as noted earlier. In this case, we need t a way to execute the <code>dfs -put</code> command on the remote server. Server tools enable us to execute the remote processes. Hadoop is build upon Linux, so <code>bash</code> shell script to execute the remote process. But that is quite challenged in real operation in production environment.</p>
<ul>
<li>What if the number of files is huge and cannot done by manually issuing command but have to do the batch operation automatically.</li>
<li>What if files being moved crossing multiple platform from Windows to Linux local file system to Hadoop file system and if the data would be able to keep the original value and format in Hive or Impala table.</li>
</ul>
<h3 id="Generate-source-data-by-CSV-file-from-Windows-side"><a href="#Generate-source-data-by-CSV-file-from-Windows-side" class="headerlink" title="Generate source data by CSV file from Windows side"></a>Generate source data by CSV file from Windows side</h3><p>For multi-processing, SSIS is the sounds solution and tool set on Windows side which enables us to run SSH commands on the remote server from an <code>Execute Process Task</code>. Setting up a package to implement this process is relatively straight forward. Just set up a data flow as normal with a source component retrieving data from SQL Server data warehouse. Any transformation actions that need to be applied to the data can be performed. Ad the last step of the data flow, the data needs to be written to a file. The format of the file is determined by what the Hive system expects. The easiest format to work with from SSIS is a delimited format, with carriage return/line feeds delimiting rows, and a column delimiter like a comma(,) or pipe (|) separating column values. The SSIS flat file destination is designed to write these types of files.</p>
<p><img src="/img/screenshots/dataflow.png" alt="dataflow.png"></p>
<p><img src="/img/screenshots/dataflow2.png" alt="dataflow2.png"></p>
<h3 id="Automate-file-operation-from-Windows-to-Linux-and-HDFS"><a href="#Automate-file-operation-from-Windows-to-Linux-and-HDFS" class="headerlink" title="Automate file operation from Windows to Linux and HDFS"></a>Automate file operation from Windows to Linux and HDFS</h3><p>Once the file is produced, then use a file system task to copy it to a network location that is accessible to both SSIS server and Hadoop cluster. The next step is to call the process to copy the file into the HDFS. This is done through an <code>Execute Process Task</code>. It can be configured to use expressions to make this process more dynamic. In addition, if you are moving multiple files, it can be used inside a <code>For loop</code> in SSIS to repeat the process a specified number of times.</p>
<p>To FTP data file from Windows to Linux, we can use either <code>sftp</code> or <code>scp</code> command but first you should make sure you have Hadoop access. But the problem is when we copy data to Hadoop, we need to provide the password to access the system, so the process is manual. As mentioned before, for larger scale data migration, we have to figure out the way of automation so that silent mode is needed to avoid manually provide user password then we can use script to schedule job to automate data process.</p>
<h4 id="Check-OpenSSH-Client-installed-and-enable-in-your-machine"><a href="#Check-OpenSSH-Client-installed-and-enable-in-your-machine" class="headerlink" title="Check OpenSSH Client installed and enable in your machine"></a>Check <code>OpenSSH Client</code> installed and enable in your machine</h4><p>Open Windows <code>setting</code> -&gt; <code>App</code> -&gt; <code>Apps &amp; features</code> -&gt; <code>Optional feature</code>, check if <code>OpenSSH Client</code> is on the list, if not you need to install that client tool</p>
<p><img src="/img/screenshots/openssh.png" alt="openssh.png"></p>
<h4 id="On-source-side-to-create-a-public-access-key"><a href="#On-source-side-to-create-a-public-access-key" class="headerlink" title="On source side to create a public access key"></a>On source side to create a public access key</h4><p>This step we need use <code>cmd</code> prompt or <code>git bash</code>, open <code>git bash</code>, change directory to home folder by issuing command <code>cd $HOME</code>, you will find the hidden folder <code>.ssh</code>, you can display all folder by command <code>ls -la</code> </p>
<p><img src="/img/screenshots/sshfolder.png" alt="sshfolder.png"></p>
<p>Issue below command to generate the public access key in order to enable the silent mode to Hadoop</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -b 2048 -t rsa</span><br></pre></td></tr></table></figure>

<p>tap 3 time enter key to generate the key, the value for argument <code>-b</code> is key size and has to be the number at least 2048, value of <code>-t</code> is algorithm. You will find the 2 new files are generated, <code>id_rsa</code> stores your identification information and <code>id_rsa.pub</code> is your public key which we need to copy to Linux server.</p>
<p><img src="/img/screenshots/publickey.png" alt="publickey.png"></p>
<h4 id="On-target-side-FTP-public-key-file-to-it"><a href="#On-target-side-FTP-public-key-file-to-it" class="headerlink" title="On target side FTP public key file to it"></a>On target side FTP public key file to it</h4><p>We can use <code>scp</code> command to upload public key to Linux server. </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -p 22 /home/yourUserName/.ssh/id_rsa.pub yourUserName@LinuxServerName:/home/yourUserName/.ssh/authorized_key</span><br></pre></td></tr></table></figure>

<p>You need to give password when issue above command to pass the authentication, but later this step will  be bypass because of the public access key.</p>
<h4 id="Load-file-from-Linux-file-system-to-HDFS"><a href="#Load-file-from-Linux-file-system-to-HDFS" class="headerlink" title="Load file from Linux file system to HDFS"></a>Load file from Linux file system to HDFS</h4><p>Even Hadoop is installed on Linux, it has independent  file operating system called HDFS, we need to issue command to transfer file between Linux and Hadoop. <code>hdfs dfs</code> command to manipulate Hadoop files, Hadoop is kind of remote server so we use <code>-put</code> argument to push file from Linux local to Hadoop; use <code>-get</code> to push file from Hadoop to Linux local, is this case we use put.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put /yourLinuxFilePath/fileName.csv /HadoopFilePath/targetHiveDatabase.db</span><br></pre></td></tr></table></figure>

<p>Now, data file is in HDFS so that we can issue Hive command to write the table metadata into Hive metastore</p>
<h3 id="Configure-Hive-table-metadata-to-match-up-with-SQL-Server"><a href="#Configure-Hive-table-metadata-to-match-up-with-SQL-Server" class="headerlink" title="Configure Hive table metadata to match up with SQL Server"></a>Configure Hive table metadata to match up with SQL Server</h3><p>In Hadoop side, we need to define Hive tables to store the data from SQL Serer. For keeping data consistency, we define the Hive table schema in terms of SQL table data type. We don’t use Ctrl-A(0x001) which is the default Hive column delimiter for flat file but use pipe bar(|) as field delimiter, because that isn’t supported well for use from SSIS in Window platform.</p>
<p>Now let’s dig a litter deeper on Hive data type which is the critical element and the most important consideration to keep data consistent. Hive provides a layer on top of Hadoop data that resembles a RDBMS. In particular, Hive is designed to support the common operations for data warehousing scenarios. Thanks to Hive to build the bridge between Hadoop MapReduce and RDBMS so that many of these data types have equivalent values in SQL Server, but only a few are unique to Hive.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Examples</th>
<th>SQL Server Equivalent</th>
</tr>
</thead>
<tbody><tr>
<td>Float</td>
<td>4-byte single-precision floating point</td>
<td>25.189164</td>
<td>real</td>
</tr>
<tr>
<td>Double</td>
<td>8-byte double-precision floating point</td>
<td>25.1897645126</td>
<td>float(53)</td>
</tr>
<tr>
<td>Decimal</td>
<td>A 38-digit precision number</td>
<td>25.1897654</td>
<td>decimal, numeric</td>
</tr>
<tr>
<td>Boolean</td>
<td>Boolean true or false</td>
<td>TRUE FALSE</td>
<td>bit</td>
</tr>
<tr>
<td>Timestamp</td>
<td>JDBC-compliant timestamp format YYYY-MM-DD HH:MM:SS:ffffffffff</td>
<td></td>
<td>datetime, datetime2</td>
</tr>
</tbody></table>
<p>define and create a Hive table is simple, the syntax just like SQL, but something different after all, it’s file system.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> yourTableName(</span><br><span class="line">col1 <span class="built_in">bigint</span>,</span><br><span class="line">    col2 <span class="built_in">int</span>,</span><br><span class="line">    col3 <span class="built_in">varchar</span>(<span class="number">5</span>),</span><br><span class="line">    col4 <span class="built_in">decimal</span>(<span class="number">5</span>,<span class="number">2</span>),</span><br><span class="line">    col5 <span class="built_in">smallint</span>,</span><br><span class="line">    col6 <span class="built_in">tinyint</span>,</span><br><span class="line">    col7 <span class="built_in">timestamp</span>,</span><br><span class="line">    col8 <span class="built_in">float</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">&#x27;|&#x27;</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">tblproperties (<span class="string">&quot;skip.header.line.count&quot;</span>=<span class="string">&quot;1&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>If data files come with header then you need to tell Hive skip the first row by <code>tblproperties</code>. We save above script as <code>createTbl1.hql</code> then use Hive <code>beeline</code> command to generate table</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline -f ./yourPath/createTbl1.hql &gt; /yourPath/hql.ot</span><br></pre></td></tr></table></figure>

<h3 id="Load-data-file-to-Hive-and-Impala-table"><a href="#Load-data-file-to-Hive-and-Impala-table" class="headerlink" title="Load data file to Hive and Impala table"></a>Load data file to Hive and Impala table</h3><p>As long as we push data file to Hadoop and Hive directory, we can easily to write data to Hive table by issuing <code>beeline -e</code> command like below</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline -e <span class="string">&quot;LOAD DATA INPATH &#x27;/HadoopFilePath/targetHiveDatabase.db/fileName.csv&#x27; INTO TABLE yourTableName&quot;</span></span><br></pre></td></tr></table></figure>

<p>the metadata will be in the Hive metastore.</p>
<p><code>Cloudera Impala</code> metastore is not sync with Hive automatically, so if you want to manipulate data in Impala, we need to issue impala command</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">impala -q <span class="string">&quot;invalidate metadata yourTableName&quot;</span></span><br><span class="line">impala -q <span class="string">&quot;refresh yourTableName&quot;</span></span><br></pre></td></tr></table></figure>

<p>after that data is on both Hive and Impala table.</p>
<h2 id="Automate-the-whole-process"><a href="#Automate-the-whole-process" class="headerlink" title="Automate the whole process"></a>Automate the whole process</h2><p>To get a better understanding of how the process looks like, below workflow illustrates how data file read from SQL Server side and load into Hive table in HDFS cluster.</p>
<p><img src="/img/screenshots/win2hadoopwf.png" alt="win2hadoopwf.png"></p>
<p>now let’s create a script to bulk copy data file to Linux and save it as <code>bulkcopy.sh</code> </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -p 22 -r /yourDataFileFolder/ user@host:/home/user/dataFileFolder</span><br></pre></td></tr></table></figure>

<p>then for the task in Linux server side, we also create a script to push data from Linux local file system to HDFS then write data to Hive table and save it as <code>bulkload.sh</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put /yourLinuxFilePath/fileName.csv /HadoopFilePath/targetHiveDatabase.db/ &amp;&amp;</span><br><span class="line">beeline -e <span class="string">&quot;LOAD DATA INPATH &#x27;/HadoopFilePath/targetHiveDatabase.db/fileName.csv&#x27; INTO TABLE yourTableName&quot;</span></span><br><span class="line">impala -q <span class="string">&quot;invalidate metadata yourTableName&quot;</span></span><br><span class="line">impala -q <span class="string">&quot;refresh yourTableName&quot;</span></span><br></pre></td></tr></table></figure>

<p>so far, we have csv data file, <code>bulkcopy.sh</code> and <code>bulkload.sh</code>, now we will schedule a job to run those scripts automatically by using SSIS package</p>
<p><img src="/img/screenshots/ssiscontrolflow.png" alt="ssiscontrolflow.png"></p>
<p>last, configure the SSIS package to write log data either file or database for debugging.</p>
<p><strong>Both Hive table and Impala table will be good after run command invalidate metadata command in Impala shell for both HUE (GUI tool) and Linux server. Invalidate metadata command takes effect across both Impala shell and HUE web interface, but refresh command only tales effect on the environment the command run against.</strong></p>
<h4 id="Data-restatement"><a href="#Data-restatement" class="headerlink" title="Data restatement"></a>Data restatement</h4><p>It’s quite common practice in production to reload historical data with reconcile with new or changed logic or even correct some mistake so remove partial data and then reload is also important. Than task is easy for RDBMS but what if the same situation take place in Hadoop, there is no update statement in Hive or Impala, so we need to do multiple steps:</p>
<ol>
<li>Start state: number of data, e.g. 3 files need to restate in HDFS</li>
<li>Issue command <code>hdfs dfs -rm</code> to delete the 3 files with month_key (3 months)</li>
<li>Run query from HUE, make sure 3 month data and won’t impact other month</li>
<li>Run above scripts and SSIS package add new 3 data files back again, then the data would be back to the Hive and Impala table</li>
</ol>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Hadoop-Hive-Impala-HDFS-SQL-Server-SSIS/">Hadoop, Hive, Impala, HDFS, SQL Server, SSIS</a></div><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a><a class="a2a_button_facebook"></a><a class="a2a_button_twitter"></a><a class="a2a_button_telegram"></a><a class="a2a_button_whatsapp"></a><a class="a2a_button_reddit"></a></div><script src="https://static.addtoany.com/menu/page.js" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/12/25/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">SQL Server Table Partitioning in Large Scale Data Warehouse 1</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/12/20/Data-Manipulation-and-ETL-with-Pandas/"><span class="level-item">Data Manipulation and ETL with Pandas</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hermanteng19.github.io/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/';
            this.page.identifier = '2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'herman-hexo-blog' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar1.png" alt="Herman"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Herman</p><p class="is-size-6 is-block">Data Analyst&amp;Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Toronto, Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">20</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/HermanTeng19" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/HermanTeng19"><i class="fab fa-github"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Background-and-the-gap-of-data-modeling-for-Hadoop-early-adopter"><span class="level-left"><span class="level-item">1</span><span class="level-item">Background and the gap of data modeling for Hadoop early adopter</span></span></a></li><li><a class="level is-mobile" href="#File-operation-for-data-migration-from-SQL-to-Hadoop"><span class="level-left"><span class="level-item">2</span><span class="level-item">File operation for data migration from SQL to Hadoop</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Generate-source-data-by-CSV-file-from-Windows-side"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Generate source data by CSV file from Windows side</span></span></a></li><li><a class="level is-mobile" href="#Automate-file-operation-from-Windows-to-Linux-and-HDFS"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Automate file operation from Windows to Linux and HDFS</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Check-OpenSSH-Client-installed-and-enable-in-your-machine"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">Check OpenSSH Client installed and enable in your machine</span></span></a></li><li><a class="level is-mobile" href="#On-source-side-to-create-a-public-access-key"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">On source side to create a public access key</span></span></a></li><li><a class="level is-mobile" href="#On-target-side-FTP-public-key-file-to-it"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">On target side FTP public key file to it</span></span></a></li><li><a class="level is-mobile" href="#Load-file-from-Linux-file-system-to-HDFS"><span class="level-left"><span class="level-item">2.2.4</span><span class="level-item">Load file from Linux file system to HDFS</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Configure-Hive-table-metadata-to-match-up-with-SQL-Server"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">Configure Hive table metadata to match up with SQL Server</span></span></a></li><li><a class="level is-mobile" href="#Load-data-file-to-Hive-and-Impala-table"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">Load data file to Hive and Impala table</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Automate-the-whole-process"><span class="level-left"><span class="level-item">3</span><span class="level-item">Automate the whole process</span></span></a><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#Data-restatement"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">Data restatement</span></span></a></li></ul></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="column-right-shadow is-hidden-widescreen"></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/avatar1.png" alt="Herman-blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Herman Teng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>