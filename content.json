{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"Generate non comma delimiter CSV file","text":"This is some tip but sometime makes you life easier when you work with business team and deal with data from business input. Excel spreadsheet is kind of standard file format to communicate with business team, but in data manipulation side, it’s not a ideal input data format, technically, we usually convert spreadsheet to csv file. But default comma delimiter might cause some trouble because business data might contains quite a lot of , in attributes such as comments, suggestions, reasons etc. In order to better identify the column, non-comma delimiter should be used like | pipe. How do we generate the | delimiter csv file. There are two ways Change format setting system-wiseIn windows, open the control panel, find the Region setting, on the Formats tab click Additional setting, a pop-up window will show up, on that window, find the option list separator then type whatever delimiter you want to setup like |. ) save the setting, when you Save as csv file in excel, it will generate | separated csv file afterwards. Use database client tool to save query result to csv file with customized delimiterSome sql database client applications provide the functionality to save query result to csv file, like WinSQL. Before running your query, select execute to save to a text file, then select | from drop-down list","link":"/2020/12/07/Generate-non-comma-delimiter-CSV-file/"},{"title":"Linux network auto boot and restart","text":"Centos 7 network is disabled by default after installation and initialization, which causes network connection can not be made until you manually turn on it That is such annoying when you usually use SSH tool remote connect to centos workstation or server, so it’s quite necessary to turn on the network automatically every time reboot machine or VM. To accomplish that, we need to modify the network configuration file. Auto Boot NetworkVIM open config file: /etc/sysconfig/network-scripts/ifcfg-ens331vim /etc/sysconfig/network-scripts/ifcfg-ens33 Revise the ONBOOT value to be “yes” Save and quit config file by :wq vim commandRestart NetworkTo be able to make that change taking effect, network needs to be restarted by below command 1systemctl restart network.service something trick here is some blog articles mention that part is only systemctl restart, which won’t work if there is no network.service, I confuse and spend many time to figure out that, thanks to CodeSheep whose video shed the light on it and help me to solve that problem.","link":"/2020/11/25/Linux-network-auto-boot-and-restart/"},{"title":"A little thought on SQL query performance optimization 1","text":"Working with data and database, writing query is daily routine, query running time might be big different for different queries but serve the same purpose, which shows query performance is not 100 percent determined by database system configuration, server hard ware, table index and statistics etc. but how well do you use SQL to construct your query. To satisfy user strong demand, we built sandbox database on production for business partners to server their needs of BI reporting and business analytical, in the meanwhile, we are also in charge of database maintenance and monitoring so that we got chance to collect all kinds of user queries. Some of them even crash the system or drag the database performance down apparently, here I want to share my thinking of query optimization on large amount of data. Before writing a queryJust like a project, a query is able to achieve the things with backend business logics even if it is small, so make a plan before creation is very necessary to be able to let the query return result in flash and only consume minimum system resources. What level detail of data do you want to query from? Business data is built on certain level, e.g. in common scenario, there are customer level, account level and transaction level data. It’s better clarify target data is on which level or mix different level. What the time scope? The data amount volume would be big if you want to query multiple years even months, so if you evaluate the data is big, it’s better use loop or paging technic instead of returning outcome in one single page. What are query tables look like? A table contains a lot of information, not only business attributes or columns but data type, keys, index, constraints, triggers. Familiar with table structure is going to give additional benefits when you write your queries against those tables. SQL query optimization tips Correlated and uncorrelated subquery Business often asks about the highest transaction information in terms of product, product group, merchant category etc. in certain month or quarter. For this kind of request is not straight forward to be solved by single select from query but it needs subquery. Now let’s see what I got from a business partner 12345678select tran_date, Product_cd, Merchant_Name, Trans_Amtfrom trans awhere tran_date between '2020-10-01' and '2020-10-31' and trans_Amt=( select Max(b.Trans_Amt) from trans b where a.product_cd=b.product_cd and tran_date between '2020-10-01' and '2020-10-31')order by Product_cd subquery is correlated with main query so trans data is loaded into memory again and make a calculation to return data to main query, that query execution time is 32s based on 90mm data. What if change correlated to uncorrelated subquery like below 12345678910select tran_date, Product_cd, Merchant_Name, Trans_Amtfrom trans ajoin (select Product_cd, Max(Trans_Amt) as Trans_Amt, tran_date from trans where tran_date between '2020-10-01' and '2020-10-31' group by Producct_cd, tran_date) as bon a.Product_cd=b.Product_cd and a.Trans_Amt=b.Trans_Amt and a.tran_date=b.tran_dateorder by a.Product_cd the query execution time reduce to 26s. Exists clause and table join The most active accounts and their corresponding spending amount is another important KPI for product manage from account management perspective, additionally, if some condition applied such as account opened on certain month, we got the query from one business partner using exists statement down below 12345678select acct_id, trans_dt,count(*) as num_trans, sum(trans_amt) as tot_trans_amtfrom trans awhere date_key=202010 and exists(select 1 from acct b where a.date_key=b.date_key and a.acct_id=b.acct_id and year(acct_open_dt)*100+month(acct_open_dt)&gt;=202009)group by trans_dt, acct_idhaving count(acct_id)&gt;10order by num_trans desc from the query structure, we might be able to tell exists statement was put in where clause to get the account open date, its execution time is 13s, but it’s obviously not thinking about the whole business logic before, if the query changed to get target accounts for account open date first then did the calculation, the execution time will reduce to 3s like below 123456select a.acct_id, a.trans_dt, count(*) as num_trans, sum(trans_amt) as tot_trans_amtfrom trans a join acct b on a.acct_id=b.acct_id and a.date_key=b.date_keywhere year(b.acct_open_dt)*100+month(b.acct_open_dt)&gt;=202009 and a.date_key=202010group by a.trans_dt, a.acct_idhaving count(a.acct_id)&gt;10order by num_trans desc Reduce the data scope by subquery It’s very necessary to think about your query data scope first when you do a bunch of left outer join, especially, data volume is quite big on your main left table . Below query does a series left join by using all full tables data, but based on business requirement, only partial data is required on the major left table, so it cause somehow resources wasted in reflect on the execution time of 7s 123456select a.*from wfs_trans aleft join credit_decision b on a.tran_id=b.tran_idleft join finance_request c on a.req_id=c.req_idwhere cast(a.creation_date as date)&gt;='2019-01-01'order by creation_date desc after revised above query on left table, the execution time reduced to 5s (2000ms) 123456789select a.*from (select * from wfs_trans where cast(a.creation_date as date)&gt;='2019-01-01') as aleft join credit_decision b on a.tran_id=b.tran_idleft join finance_request c on a.req_id=c.req_idorder by a.creation_date desc Paging browse data Business users sometime complaint the SSRS or Power BI report is slow when they browse data by skipping pages, that is not the programming problem because on the backend the query to support the BI report like below 12345select prod_cd, post_dt, tran_dt, tran_amtfrom transwhere date_key=202010 and prod_cd='ABCD'order by post_dtoffset 1000000 rows fetch next 100 only; that will take 22s to return results based on 80mm data in single month. Thing thing is even the data is ordered by index column post_dt, database engine doesn’t know where is 1000000 row, it needs to recalculate again, so to answer that business concern, we recommend to apply some conditions parameter then start to browse data like below 12345select prod_cd, post_dt, tran_dt, tran_amtfrom transwhere date_key=202010 and prod_cd='ABCD' and post_dt='2020-10-15'order by post_dtoffset 1000 rows fetch next 100 only; by this way, the report will be presented by less than 1s.","link":"/2020/11/30/A-little-thought-on-SQL-query-performance-optimization-1/"},{"title":"How to let local images display on your hexo blog website","text":"Hexo blog framework friendly supports markdown which is the most popular syntax for technical blog writing. When you try to illustrate some ideas, it’s common to use screenshot photos in your articles to get reader better understanding so that you have to insert images into your blog, unlike typing text content that is straight forward, inserting images and deploying them to public blog site might be some tricks there, let’s see some common scenarios on images display issue and how to resolved them. Use absolute path to insert imageThe intuitive way for junior blogger is referencing absolute path + file name, I would say there is no any problem if the blog is only for local review or your local machine is a web server to host your blog website, otherwise, your images won’t be displayed on your public blog site after deployment if without any configuration. Like below shows when you use that way inert on your local blog, images can be displayed as expected but it will cause issue on display when you deploy your article to public blog website like below shown Use relative path based on _config.yml fileActually, hexo blog is powered by Node.js in the backend, its general configuration file _config.yml shows the way to properly allocate your resources including image, css, font and js etc. By default, your images should be put into blog/themes/your theme name/source/images , when you reference it, use below syntax no matter you use Windows or Linux, 1![your image's name](/images/your_image_file_name.png) now, you can push your blog to public blog site and your images can be presented there well but the problem is you lose your sight on local but only are able to see on blog website after you deployment, that is not you really want like below shown Use online image management toolTo be able to solve image display problem on both local and public site, the best way is utilize online image management website to upload your images then it will generate corresponding URLs for those photos, you can replace path name of your local images by URL. Now your image is going to be rendered by both your local markdown editor and web browser. Another advantage of managing your photos by online image tool is your images are in the cloud with permanent URLs so that you won’t lose them even something bad happened on your local machine. I use wailian.work to manage my images, it’s free and provide markdown link for each of your uploaded picture. A small thinking about Node.js top route design and file renderThe root cause of confusion here might be the Node.js route design, unlike Apache and Nginx web server which come with web container so that the route can be your local folder path name and render your local html file in the folder, in this case, you do tell the file local path from URL address, but for Node.js, it doesn’t have web container, therefore, we have to design route for local web files, in other words, you are not able to tell the local file path by URL address for Node.js web server. Let’s see an example 123456789101112131415161718const fs = require('fs');const path = require('path');const url = require('url');exports.static = function (request, response, staticPath) { let pathname = url.parse(request.url).pathname; pathname = pathname == '/' ? '/index.html' : pathname; let extname = path.extname(pathname); if (pathname != '/favicon.ico') { fs.readFile('./' + staticPath + pathname, async (err, data) =&gt; { if (!err) { let mime = await getFileMime(extname.split('.')[1]); response.writeHead(200, { 'Content-Type': `${mime}; charset=&quot;utf-8&quot;` }); response.end(data); } }) }} Assume user request images, the URL is probably like https://yourdomainname/images, but in the backend, in fact the Node.js read file from path './' + staticPath + pathname such as /root/web_project1/static/images. URL address doesn’t reflect file path, they are relatively independent in Node.js, by knowing that, we are going to dig a little deeper to take a look at _config.yml file and come up with idea why browser is able to render out to image file but text editor can’t. Open _config.yml file 1vim ./blog/_config.yml 12345# URL## If your site is put in a subdirectory, set url as 'http://example.com/child' and root as '/child'url: https://hermanteng19.github.ioroot: /permalink: :year/:month/:day/:title/ here list the route on your blog site which should be https://hermanteng19.github.io/ as home page and corresponding html file should be put into “/“ directory, what about images file? It is going to be “/images”, so far, we know the directory in below actually is route or web request URL, that is why text editor is not able to find the real file path. 1![mysql-connection](/images/mysql_conn_established.png) But, you might still confuse about why browser can render the image file, now let’s continue to see what’s going on web server. Enter your local hexo blog path 1ls -l ./blog you can find there is public/ folder which is your blog website “/root” folder, all web files such as html, css, js, image, font are in there, let’s enter it and take a look images/ folder is on the list, continue to drill down, mysql_conn_established.png file is on the list. So now you may be clear by Node.js route design, when web browser visits images on your blog website, it actually read the files from “/public/images/“, not from your local file absolute path like “themes/your_theme_name/source/images”. It does cause some confusion by this special type of route design, but in the other way, I would say it’s quite smart, you can design out a very nice, neat and beautiful URL for your website, which would bring very good user experience and impress website visitors. Think about no matter how ugly your local file path is like /root/jfdioaw/_jfidosjo_jfi123/oneMoreFolder/13u8030/product_1.html, but your website URL always looks like http://yourdomainname/business/product/product_1.html. At last, I want to thank CodeSheep for help me with the idea of leveraging online photo mangement tool to solve this problem!","link":"/2020/11/27/How-to-let-local-images-display-on-your-hexo-blog-website/"},{"title":"Oracle client side configuration for 12C","text":"The client side need to do some configurations after Oracle 11g upgrade to 12C on Server in order to make database server is connectable. Before starting to configurate your clients, you have to get the below new server information from DBA Host name Port number Service name Your user name(usually it won’t be changed and replicated from old version) Password(initial password for test connection then you need to update it) and then you need to make a new connection strings to add it into ORA file(*.ora) Oracle SQL Developer configurationThe simplest way to connect to oracle 12C is by using Oracle sql client tool SQL Developer, it uses build-in JDBC driver to make connections and GUI connection wizard guide you fill in the server information without updating ora file. It’s better use version 19.1 and above. fill in all info in connection window, then click Test button to test connection Other client tool connection by ODBC driver such as SSIS and WinSqlThe first step, you need to install the ODBC driver, there is another client tool comes with ODBC driver called Oracle Database Client12cSQLDev 12.1.0.2 R02, install that in your local machine, then update your environment variable to make sure Oracle home directory being added into it. Update environment variableIf you installed both Oracle SQL Developer and Client12cSQLDev 12.1.0.2 R02, there is going to be 2 Oracle folders, one is client_32 which is 32 bit, the other one is client_1 which is 64 bit application, you need to add both of them into you environment variable. The path is usually C:\\app\\product\\12.1.0\\client_* Configurate ora file after setup oracle environment variableThe tricky thing is the ODBC driver only works well for 32 bit version not 64 bit so we can only edit the ora file for client_32. You can find out the ora file on below directory C:\\app\\product\\12.1.0\\client_32\\network\\admin\\tnsnames.ora append following string to the ora file save andclose the ora file, then go to windows ODBC data source center to finalize ODBC driver configuration. Configurate ODBC driver and DSN to test connection Data Source Name is customizable; TNS Service Name is the connection string name in ora file CAP12CPRD_32bit in this case; fill in user ID then click test connection button, a prompt window will pop out to let you input password. SSIS package connection manager configurationOpen SSIS client SSDT to configure connection manager for Oracle data source. SSDT -&gt; open a project -&gt; right click data source on right side panel -&gt; follow the wizard both preinstalled .net provider and native ole db provider for Oracle are working well server name is as the same as connection string name in ora file in this case is CAP12CPRD_32bit. Give a name for oracle new data source then connection manager can be created to Oracle database. SAS connection to Oracle 12C prerequisite: SAS grid server local DSN need to be created before testing; PC SAS login server is necessary SAS grid local DSN for Oracle 12C (ask for system admin create local DSN and return the name to you and it is going to be the value for path when you create new library for Oracle 12C) SAS EG connection to 12C: open SAS EG 7.1 and make sure it connects to grid server open a new code window and create a new library by running below statement 1libname ora12C oracle user=&quot;yourUserName&quot; password=&quot;xxxx&quot; path=sas_grid_local_DSN schema=oracle_schema make sure the program run against SASApp rather than localhost new Oracle library would be created under the SASApp dataset list","link":"/2020/12/11/Oracle-client-side-configuration-for-12C/"},{"title":"Python Environment Setup for Implementation","text":"Python is a good scripting language to boost your productivity on data analysis and BI reporting. As open source language, you can easily get the binary installation file from python official site for windows and source code on vary versions for Linux, in production, it’s better choose installation approach by source code. We also need to setup python environment after installation so that we can not only use python interpreter to develop but also make it executable by CLI and even some ETL tool such as Microsoft SSIS. Python environment variable configuration and local folder set up for your file, package and libraryIf python is installed in system-wise, then you need to create some new folders to store you python file, package and library, e.g. python install path is “D:\\Python36&quot;, then you need to add python executable interpreter to be a part of the PATH variable. Next create python environment variable PYTHONPATH with the following paths and create empty file __init__.py file in each of these folders: create a new folder under D drive “D:\\pyLib” and set that directory as value of PYTHONPATH and create __init__.py file in “D:\\pyLib” you can also create subfolder to assign different permissions for different user group create a subfolder “D:\\pyLib\\AD-group1” and create the __init__.py file in it. create a subfolder “D:\\pyLib\\AD-group2” and create the __init__.py file in it. For Linux, if you install python3 by source code and directory is /usr/local/python3, then edit ~/.bash_profile file, append the python directory into PATH 12# Python3export PATH=/usr/local/python3/bin/:$PATH then run source ~/.bash_profile let setting take effect if your system pre-installed python2 then it’s necessary to make a soft link 12ln -s /user/local/python3/bin/python3 /user/bin/python3ln -s /user/local/python3/bin/pip3 /user/bim/pip3 setup name space and package python scripts for development project to be able to importable create or edit environment variable and add your python files folder into your system directory enter your python file folder to create an empty file __init__.py file open terminal prompt type python to active python interactive console import sys execute sys.path to make sure your python file folder is recognizable by python Python readiness test in localhost for SQL database connection (Anaconda virtual environment)First check python and Ipython version by issue command python --version and ipython --version. Anaconda almost pre-installs all python prevailing and popular libraries in its virtual environment, to check library list by using command pip list Python and SQL database connection facility with supported driversDepends on what python library do you install for database connectivity, it usually comes with function to show you available drivers to connect python to your database, e.g: pyodbc, use drivers() function to list the odbc drivers Python SQL Server database connection and data taskSQL Server database can be connected both by DB API (pyodbc) and ORM (sqlalchemy), create a py script and run sql query from user input 123456789101112131415161718192021222324252627282930313233343536373839import pyodbcimport sqlalchemyfrom sqlalchemy import create_engineimport pandas as pddef pyquery(conn): cnxn = pyodbc.connect(conn) cur = cnxn.cursor() sql_cmd = input(&quot;Input your sql command with database name: &quot;) result = cur.execute(sql_cmd) for row in result: print(row) cur.close() cnxn.close() def py2mssql(): way2conn = input(&quot;Do you want to connect to db by [ORM] or [DBAPI]: &quot;) if way2conn.upper()==&quot;DPAPI&quot;: dsn_str = input(&quot;Do you want to connect to db by [connection string] or [DSN]: &quot;) if dsn_str.upper()==&quot;DSN&quot;: dsn_name=input(&quot;Please enter your DSN name: &quot;) conn_dsn='DSN{};Trusted_connection=yes'.format(dsn_name) conn=conn_dsn pyquery(conn) else: conn_str='DRIVER={SQL Server Native Client 11.0}; SERVER=YOURSERVERNAME;DATABASE=YOURDB;Trusted_connection=yes' conn=conn_str pyquery(conn) else: ser_name=input('Please enter your server name: ') db_name=input('Please enter your database name: ') tbl_name=input('Enter the table name: ') orm_dict={'servername':'{}'.format(ser_name), 'database':'{}'.format(db_name), 'driver':'driver=SQL Server Native Client 11.0'} engine=create_engine('mssql+pyodbc://'+orm_dict['servername']+'/'+orm_dict['database']+'?'+orm_dict['driver']) df=pd.read_sql_table(tbl_name, engine, index_col=0, schema='dbo') print(df.head(10)) if __name__==&quot;__main__&quot;: py2mssql() script both can be ran directly or imported (recommend) after setup PYTHONPATH variable in your account and copy that script over to the path of environment variable. Python DB2 database connection and data taskWe can only connect to DB2 by DBAPI (pyodbc), connection string doesn’t work but only DSN (PRD1 was setup as system DSN in local and server) plus user id and password. Use below script to try to connect 123456789101112131415161718192021222324252627import pyodbcimport getpassimport pandas as pddef py2edw(): uid=getpass.getuser() print(&quot;Your user id is '{}'&quot;.format(uid)) pwd=getpass.getpass(&quot;Please enter your db2 password: &quot;) dsn=input(&quot;Please enter your db2 dsn name: &quot;) conn_dsn='DSN={0}; UID={1}; PWD={2}'.format(dsn, uid, pwd) conn=pyodbc.connect(conn_dsn) cur=conn.cursor() sql_cmd= ''' select * from table ''' result=cur.execute(sql_cmd) for row in result: print(row) cur.close() df=pd.read_sql_query(sql_cmd, conn) print(df.info()) print(df) conn.close() if __name__==&quot;__main__&quot;: py2edw() one thing need to be aware is the script better runs on the shell than python interactive console because pyQT doesn’t support password masking Productionize Python script by passing in parametersIn this section we will demonstrate how you can parameterize your code in python or pyspark so that you can use these techniques before deployed your script into production for automation. It’s best practice to parameterize database names, tables names, and dates so that you can pass these values as inputs to your script. This is beneficial when writing code for values that are dynamic in nature, which can change depending on the environment and/or use case. The key module is from python standard library: sys. Assign variables through sys.argv[...] 1234567891011121314151617import pandas as pdfrom sqlalchemy import create_engineimport sysdef py2mssql(): ser_name=sys.argv[1] # sys.argv[0] is assigned to python script itself, all other parameters start from 1 db_name=sys.argv[2] tbl_name=sys.argv[3] orm_dict={'servername':'{}'.format(ser_name),'database':'{}'.format(db_name), 'driver':'driver=SQL Server Native Client 11.0'} engine=create_engine('mssql+pyodbc://'+orm_dict['servername']+'/'+ orm_dict['database']+'?'+orm_dict['driver']) df=pd.read_sql_table(tbl_name, engine, index_col=0, schema='dbo') df.to_sql('pandas_sql_test', engine, schema='dbo', if_exists='replace', index=False) if __name__==&quot;__main__&quot;: py2mssql() run python script in CLI (command line interface) by following parameter values 1ipython py2mssql_argvdf.py yourSvrNm yourDBNm yourTblNm Encapsulate into SSIS to minimize change in production deployment – python interpreterAn alternative way to apply python in production is leverage current SSIS package and embed python script in process task you can hard code the configuration or use expression (VB) through variables in package Encapsulate into SSIS to minimize change in production deployment – batch processuse batch process by .bat file also can achieve that task Call user defined module or function in python scriptIt’s very efficient to create bunch of generic module packages to contain functions to be used widely by other python scripts for specific tasks. 1. Firstly, setup python environment variable to include directories which are recognized by python 2. Create __init__.py file (can be empty) in these folders 3. Create python programs and save script should end up with if __name__==”__main__“: main() 4. Ready to import user modules and functions 1234567import pandas as pdimport py2mssql_module as dbimport systbl_name=sys.argv[1]df=pd.read_sql_table(tbl_name, db.py2mssql('serverName','databaseName'), index_col=0, schema='dbo')print(df.head(10))","link":"/2020/12/08/Python-Environment-Setup-for-Implementation/"},{"title":"comment test","text":"This is a gitalk comment function test.","link":"/2020/12/13/comment-test/"},{"title":"Remote Connect MySQL Server from Client Machine Setup","text":"You can only connect to MySQL Server from localhost after MySQL installation by default, but in production, all MySQL clients remotely connect to server, for simulating real production environment in your home network, some configurations need to be made to be able to let you connect MySQL from client machine other than localhost. Revise or create MySQL configuration file (RHEL or Centos 7)Modify or create /etc/my.cnf file1vim /etc/my.cnf add a configuration item bind-address and let it value to be your MySQL server host ip address (eg. 192.168.1.114)1bind-address=192.168.1.114 save and exit1:wq Restart MySQL service1systemctl restart mysql.service Open TCP port 3306 using iptablesSetup /sbin/iptables and let firewall opens port on 3306 for any remote machine1/sbin/iptables -A INPUT -i eth0 -p tcp --destination-port 3306 -j ACCEPT To specific client host machine to access port 3306, you can explicitly assign ip address (eg. 192.168.1.134)1/sbin/iptables -A INPUT -i eth0 -s 192.168.1.134/24 -p tcp --destination-port 3306 -j ACCEPT Finally save IPv4 firewall rules1/sbin/iptables-save &gt; /etc/sysconfig/iptables If it doesn’t work, for testing and develop environment, you can turn off firewall12systemctl stop firewalld.servicesystemctl disable firewalld.service Grant remote access to new MySQL databaseCreate a new database1create database foo; Grant remote user access to a specific database on user host machine (192.168.1.134)1grant all privileges on foo.* to herman@'192.168.1.134' identified by 'youOwnPasswd'; Grant remote access to existing MySQL database for user (herman) on its host machine (eg. 192.168.1.134)Require a set of two commands12update db set Host='192.168.1.134' where Db='mysql';update user set Host='192.168.1.134' where user='herman'; Open MySQL client tool on workstation with address 192.168.1.134 like MySQL workbenchCreate new connection using username and passwordusername: hermanpassword: youOwnPasswdport: 3306 Connection is created and foo database is on the list under user herman","link":"/2020/11/25/Remote-Connect-MySQL-Server-from-Client-Machine-Setup/"},{"title":"Sandbox solution for BI reporting and business analytics","text":"At beginning, I’d like to share a story from my client and business partner. One day, my team worked on a big marketing project, data from all kinds of source like spreadsheet, csv, mainframe and SQL Server, we had to do cross reference all those data to generate analysis reports but the headache thing was they were isolated and no way to put them into a single unique environment to run one time query then return the results so we could only open multiple windows to compare them by eyeballs. During the project, we often composed some complex queries then ran for a long time to return the result dataset, those datasets were quite important for future further analysis and share with the whole team, but the another panic thing was we could not save those dataset into database due to insufficient access so what we did was copy and paste everything in excel spreadsheet, after for a while, we found number of excel file explode and hard to find the report among those huge files, we feed up with the tedious work and decided created a bunch of views in database but that was also not controlled by us but infrastructure team, all we could do was submit the request then followed infrastructure team’s schedule and waited for month end deployment, no matter how urgent those reports would be. That is the story, I think if you had ever experienced that, that solution might be right for you. You might get the common points from above story, it’s inefficient and even painful if you can only leverage data from data warehouse tables, that is reason why the sandbox database comes up which is a brand new data play zone on production with ability of pipeline to bring multiple sources of data based on major data warehouse you are using. In a brief mark, sandbox is aiming to build a homogeneous solution for heterogenous environment. How to build up and what is the foundation of sandbox database, now let’s take a look into it. The Major data source is so called big db, but it’s data warehouse so that user was only assigned read access which means you can do nothing but only select and query data. Now we carved out two new databases - sandbox and control db. Sandbox database brief introductionsandbox is the new data loading zone for slicing and dicing data in production, like you own backyard playground, you can do almost anything you want to do with it, so user will be granted both read and write permission. control_db that is control console station to provide access gateway for sandbox and also monitor and logging the users’ behaviors, just like a guardian to promise the safety and healthy for sandbox. Let’s take a closer look at these two databases. Sandbox, in general, user can perform 3 kinds of actions, DDL, DML as well as batch job execution based on stored procedure. With DDL command which is database define language, user can create table/view/stored procedure, alter them, even drop them. With DML command which is database manipulate language, user can insert/update/delete data from existing table and also import data from other source. The batch operation is fit for user with programming background and write multiple queries with logical sequence into it like conditional and looping function by using T-SQL scripting language in terms of stored procedure. What things should be considered beforeA very critical point for sandbox database is object ownership, which means you can only create and deal with your own database objects plus read others’ object. Image you data or analytical work were deleted by other accidently, how would you feel at the moment, so the restriction must be setup along with the sandbox database creation to promise the user data safety. Another important thing need to be considered before is user data volume control. Unlike data warehouse, data volume increasing is followed trend and stable for each year, but sandbox database size is unpredictable and totally depends on user personal behavior, in order to prevent non-critical user data dominants your server hard disk, it’s very necessary to set quota for each users. When reach to the limit then User won’t be allowed to create table User also won’t be allowed insert new data into existing table until manually clean data and release space. Creating sandbox database walk throughDefine and create a new database12345use master;goif DB_ID('sandbox') is not nulldrop database sandboxgo in real production, you might need to create a new file group and bunch of file under it for better management. 12345678910create database sandboxon primary(name=N'sandbox', filename=N'G:\\sqldata\\sandbox.mdf', size=1048kB, maxsize=5124KB, filegrowth=512KB),filegroup [yourTeam_sandbox](name=N'yourTeam_sandbox_FG1',filename=N'G:\\sqldata\\yourTeam_sandbox1.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB),(name=N'yourTeam_sandbox_FG2',filename=N'G:\\sqldata\\yourTeam_sandbox2.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB),(name=N'yourTeam_sandbox_FG3',filename=N'G:\\sqldata\\yourTeam_sandbox3.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB)log on(name=N'yourTeam_sandbox_log',filename=N'H:\\sqllog\\yourTeam_sandbox_log.ldf',size=1048KB, maxsize=5124KB, filegrowth=512KB)go make sure your sandbox and control console database have the same database ownership user 12345678use master;goalter authorization on database::sandboxto sagoalter authorization on database::control_dbto sago make sure your new created file group to be the default file group so that coming data is going to be allocated there 12345use sandbox;goif not exists (select name from sys.filegroups where is_default=1 and name=N'sandbox')alter database sandbox modify filegroup [yourTeam_sandbox] defaultgo Grant user permissionsFirst, you need to provide the basic read access to your business user 1234567use sandbox;goif exists (select * from sys.database_principals where name=N'yourDomain\\yourUserGroup')drop user [yourDomain\\yourUserGroup]elsecreate user [yourDomain\\yourUserGroup] for login [yourDomain\\yourUserGroup] with default_schema=dboexec sp_addrolemember 'db_datareader','yourDomain\\yourUserGroup'; then assign to them DDL and DML permissions 1234567891011121314151617181920use sandbox;gogrant alter any schema to [yourDomain\\yourUserGroup];gogrant create procedure to [yourDomain\\yourUserGroup];gogrant create table to [yourDomain\\yourUserGroup];gogrant create view to [yourDomain\\yourUserGroup];gogrant delete to [yourDomain\\yourUserGroup];gogrant insert to [yourDomain\\yourUserGroup];gogrant select to [yourDomain\\yourUserGroup];gogrant update to [yourDomain\\yourUserGroup];gogrant execute to [yourDomain\\yourUserGroup];go Create database level trigger on DDL query control for rules and restrictions encourage give table name starting with id number which is unique identifier for employee in your company like nameInitial2_tableName must be eligible user, identity check by control database no special characters are allowed in table name like !@#$%^&amp;- no over quota allowed not allowed user drop or delete others’ object and data let’s connect above constraints with workflow chart to be able to see the big picture First of all, we create the database trigger for enforcing object creation rules 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158create trigger ddltrg_create_tableon databasewith execute as 'dbo'for create_tableasbegin try set noncount on; declare @eventdata xml declare @orig_object_name varchar(100) declare @orig_user_name varchar(25) declare @alt_object_name varchar(100) declare @alt_user_name varchar(25) declare @valid_chars varchar(50) declare @create_status char(1) declare @create_err_msg varchar(250) declare @rename_required char(1) declare @trigger_sql varchar(max) declare @schema_name varchar(25) declare @trigger_insert varchar(max) declare @guid varchar(32) declare @overQuota_user table (userId varchar(25) null) set @guid='250' set @valid_chars = '%[^a-zA-Z0-9_]%' set @eventdata = EVENTDATA() set @orig_object_namne = @eventdata.value('(/EVENT_INSTANCE/objectName)[1]','nvarchar(100)') set @orig_user_name = upper(@eventdata.value('(/EVENT/INSTANCE/LoginName)[1]','nvarchar(25)')) set @create_status='Y' set @create_err_msg='' set @alt_user_name = right(@orig_user_name,len(@orig_user_name)-charindex('\\\\',@orig_user_name)) select @schema_name = SCHEMA_NAME(SCHEMA_ID) from sys.tables where name = @orig_object_name if (@alt_user_name = 'yourServiceAccout') return if substring(@orig_object_name,1,len(@alt_user_name)) = @orig_object_name begin set @alt_object_name = @orig_object_name set @rename_required = 'N' end else begin set @alt_object_name = @orig_object_name set @rename_required = 'Y' end /*check the user eligibility*/ if not exists (select * from control_db.dbo.Audit_Sandbox_User where User_ID = @alt_user_name) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' does not have privilege to use the sandbox database' end insert into @overQuota_user select user_id from control_db.dbo.Audit_Sandbox_User where sandbox_limit - current_usage &lt; 0 /*user control for over limit quota*/ if @alt_user_name in (select userId from @overQuota_user) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' exceeded designed space quota, ' + @alt_user_name + ' is not able to ' + 'create new table. Please delete data not needed to free up space so usage is less than quota 1024MB and rerun query again' end /*check object name eligibility*/ if (patindex(@valid_chars,@orig_object_name) &gt; 0) begin set @create_status = 'N' set @create_err_msg = 'Invalid characters in table name' end /*check object existency*/ if exists(select * from sys.objects where object_id=OBJECT_ID(@alt_object_name) and type in (N'U')) begin if @rename_required = 'Y' begin set @create_status = 'N' set @create_err_msg = 'Table: ' + @alt_object_name + ' already exists' end end /*check schema name eligibility*/ if @schema_name &lt;&gt; 'dbo' begin set @create_status = 'N' set @create_err_msg = 'Table: ' + @orig_object_name + ' not created. Schema is not specified: dbo.&lt;table name&gt;!' end if (@create_status = 'N') begin rollback print @create_err_msg end /*log sandbox event into tracking table in control_db*/ insert control_db.dbo.Audit_Sandbox_Event_Tracking (Event_Type, Event_Time, Event_User, Event_Database, Event_Object_Name ,Event_Object_Type, Event_SQK, Audit_Created_Status, Audit_Error_Message) value (@eventdata.value('(/EVENT_INSTANCE/EventType)[1]','nvarchar(50)'), @eventdata.value('(/EVENT_INSTANCE/PostTime)[1]','datetime'), @Orig_user_name, @eventdata.value('(EVENT_INSTANCE/DatabaseName)[1]','nvarchar(25)'), @orig_object_name, @eventdata.value('(/EVENT_INSTANCE/ObjectType)[1]','nvarchar(25)'), @eventdata.value('(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]','nvarchar(max)'), case when @create_status = 'Y' then 'success' else 'failed' end, @create_err_msg ) if (@create_status = 'N') return /*rename object to add user id prefix*/ begin exec sp_rename @orig_object_name, @alt_object_name print 'Table name is renamed with prefix of your user id' end /*creaet DML trigger on table level*/ set @trigger_sql = 'create trigger DML trig_'+@alt_object_name+ 'ON ' + @alt_object_name + '' set @trigger_sql = @trigger_sql + 'for delete, update, insert' set @trigger_sql = @trigger_sql + 'as' set @trigger_sql = @trigger_sql + 'begin ' set @trigger_sql = @trigger_sql + ' set nocount on;' set @trigger_sql = @trigger_sql + ' declare @user varchar(25)' set @trigger_sql = @trigger_sql + ' select @user = right(SUSER_NAME(),LEN(SUSER_NAME()) - CHARINDEX(''\\\\'',SUSER_NAME()))' set @trigger_sql = @trigger_sql + ' IF @user &lt;&gt; '''+@alt_user_name+''' ' set @trigger_sql = @trigger_sql + ' begin' set @trigger_sql = @trigger_sql + ' rollback' set @trigger_sql = @trigger_sql + ' print ''User: ''+@user+'' does not have the privilege to perform a DML operation on table' +@alt_object_name + ''' ' set @trigger_sql = @trigger_sql + ' end' set @trigger_sql = @trigger_sql + ' end' exec (@trigger_sql) ; /*create over quota DML trigger on table level*/ create table #overQuota (userId varchar(25) Null) ; set @trigger_sql = 'create trigger DML trig_'+@alt_object_name+ '_insert ON ' + @alt_object_name + '' set @trigger_sql = @trigger_sql + 'after insert' set @trigger_sql = @trigger_sql + 'as' set @trigger_sql = @trigger_sql + 'begin ' set @trigger_sql = @trigger_sql + ' set nocount on;' set @trigger_sql = @trigger_sql + ' declare @user varchar(25)' set @trigger_sql = @trigger_sql + ' select @user = right(SUSER_NAME(),LEN(SUSER_NAME()) - CHARINDEX(''\\\\'',SUSER_NAME()))' set @trigger_sql = @trigger_sql + ' insert into #overQuota' set @trigger_sql = @trigger_sql + ' select user_id from control_db.dbo.Audit_Sandbox_User where sandbox_limit-current_usage&lt;0;' set @trigger_sql = @trigger_sql + ' IF @user in (select userId from #overQuota)' set @trigger_sql = @trigger_sql + ' begin' set @trigger_sql = @trigger_sql + ' rollback' set @trigger_sql = @trigger_sql + ' print ''User: ''+@user+'' is over quota on usage so it can not perform a INSERT operation on table' +@alt_object_name + ', please delete data that is not needed to free up space then rerun the query'' ' set @trigger_sql = @trigger_sql + ' end' set @trigger_sql = @trigger_sql + ' end' exec (@trigger_sql) ; drop table #overQuota ;end trybegin catch declare @err_msg varchar(900), @err_num int, @err_line int, @syserr varchar(900) select @err_msg = ERROR_MESSAGE(), @err_num = ERROR_NUMBER(), @err_line = ERROR_LINE() set @syserr = 'Ended in DDLTRIG_CREATE_TABLE with errors: Line= ' + convert(varchar(10), @err_line) + ', Error Num = ' + convert(varchar(10), @err_num) + ', Error Msg= ' + @err_msg /*save to log file or control_db table*/end catchgoenable trigger [ddltrg_create_table] on databasego One more run need to be applied is prevent user drop object which is not belong to that user. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051create trigger ddltrig_drop_tableon databasewith execute as 'dbo' for drop_tableas set nocount on; declare @eventdata xml declare @orig_object_name varchar(100) declare @orig_user_name varchar(25) declare @alt_user_name varchar(25) declare @create_status char(1) declare @create_err_msg varchar(250) set @eventdata = EVENTDATA() set @orig_object_name = @eventdata.value('(/EVENT_INSTANCE/ObjectName)[1]','nvarchar(100)') set @orig_user_name = upper(@eventdata.value('(/EVENT_INSTANCE/LoginName)[1]','nvarchar(25)')) set @creaet_status = 'Y' set @create_err_msg = '' set @alt_user_name = right(@orig_user_name, len(@orig_user_name) - charindex('\\\\', @orig_user_name)) if @alt_user_name &lt;&gt; 'domainName/yourServiceAccount' begin if not exists (select * from control_db.dbo.Audit_Sandbox_User where User_ID = @alt_user_name) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' does not have privilege to use the sandbox database' end if substring(@orig_object_name, 1, len(@alt_user_name)) &lt;&gt; @alt_user_name begin set @create_status = 'N' set @create_err_msg = 'user: ' + @alt_user_name + ' does not have privilege to drop table: ' + @orig_object_name endendif (@create_Status = 'N')begin rollback print @create_err_msgendinsert control_db.dbo.Audit_Sandbox_Event_Tracking (Event_Type, Event_Time, Event_User, Event_Database, Event_Object_Name ,Event_Object_Type, Event_SQK, Audit_Created_Status, Audit_Error_Message) value (@eventdata.value('(/EVENT_INSTANCE/EventType)[1]','nvarchar(50)'), @eventdata.value('(/EVENT_INSTANCE/PostTime)[1]','datetime'), @Orig_user_name, @eventdata.value('(EVENT_INSTANCE/DatabaseName)[1]','nvarchar(25)'), @orig_object_name, @eventdata.value('(/EVENT_INSTANCE/ObjectType)[1]','nvarchar(25)'), @eventdata.value('(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]','nvarchar(max)'), case when @create_status = 'Y' then 'success' else 'failed' end, @create_err_msg )go At last, one very important setting need to be in placed in case cause issue when [sa] user cross database reference data 1alter database sandbox set TRUSTWORTHY ON; about trustworthy for detail, you can see Microsoft official document on below link Trustworthy database property","link":"/2020/12/02/Sandbox-solution-for-BI-reporting-and-business-analytics/"}],"tags":[{"name":"file system, csv","slug":"file-system-csv","link":"/tags/file-system-csv/"},{"name":"Linux, Centos, network, config","slug":"Linux-Centos-network-config","link":"/tags/Linux-Centos-network-config/"},{"name":"SQL, query, SQL Server, optimization","slug":"SQL-query-SQL-Server-optimization","link":"/tags/SQL-query-SQL-Server-optimization/"},{"name":"hexo, blog, online image, config, Node.js, router","slug":"hexo-blog-online-image-config-Node-js-router","link":"/tags/hexo-blog-online-image-config-Node-js-router/"},{"name":"oracle, SSIS, SAS, config","slug":"oracle-SSIS-SAS-config","link":"/tags/oracle-SSIS-SAS-config/"},{"name":"python, sql, db2, CLI, SSIS, config","slug":"python-sql-db2-CLI-SSIS-config","link":"/tags/python-sql-db2-CLI-SSIS-config/"},{"name":"test, comment, gitalk","slug":"test-comment-gitalk","link":"/tags/test-comment-gitalk/"},{"name":"Linux, MySQL, Config, Database","slug":"Linux-MySQL-Config-Database","link":"/tags/Linux-MySQL-Config-Database/"},{"name":"sandbox, SQL, SQL Server, database","slug":"sandbox-SQL-SQL-Server-database","link":"/tags/sandbox-SQL-SQL-Server-database/"}],"categories":[{"name":"IT","slug":"IT","link":"/categories/IT/"},{"name":"Data","slug":"Data","link":"/categories/Data/"},{"name":"Database, BI","slug":"Database-BI","link":"/categories/Database-BI/"},{"name":"IT, BI","slug":"IT-BI","link":"/categories/IT-BI/"},{"name":"it","slug":"it","link":"/categories/it/"}]}