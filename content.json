{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"A little thought on SQL query performance optimization 1","text":"Working with data and database, writing query is daily routine, query running time might be big different for different queries but serve the same purpose, which shows query performance is not 100 percent determined by database system configuration, server hard ware, table index and statistics etc. but how well do you use SQL to construct your query. To satisfy user strong demand, we built sandbox database on production for business partners to server their needs of BI reporting and business analytical, in the meanwhile, we are also in charge of database maintenance and monitoring so that we got chance to collect all kinds of user queries. Some of them even crash the system or drag the database performance down apparently, here I want to share my thinking of query optimization on large amount of data. Before writing a queryJust like a project, a query is able to achieve the things with backend business logics even if it is small, so make a plan before creation is very necessary to be able to let the query return result in flash and only consume minimum system resources. What level detail of data do you want to query from? Business data is built on certain level, e.g. in common scenario, there are customer level, account level and transaction level data. It’s better clarify target data is on which level or mix different level. What the time scope? The data amount volume would be big if you want to query multiple years even months, so if you evaluate the data is big, it’s better use loop or paging technic instead of returning outcome in one single page. What are query tables look like? A table contains a lot of information, not only business attributes or columns but data type, keys, index, constraints, triggers. Familiar with table structure is going to give additional benefits when you write your queries against those tables. SQL query optimization tips Correlated and uncorrelated subquery Business often asks about the highest transaction information in terms of product, product group, merchant category etc. in certain month or quarter. For this kind of request is not straight forward to be solved by single select from query but it needs subquery. Now let’s see what I got from a business partner 12345678select tran_date, Product_cd, Merchant_Name, Trans_Amtfrom trans awhere tran_date between '2020-10-01' and '2020-10-31' and trans_Amt=( select Max(b.Trans_Amt) from trans b where a.product_cd=b.product_cd and tran_date between '2020-10-01' and '2020-10-31')order by Product_cd subquery is correlated with main query so trans data is loaded into memory again and make a calculation to return data to main query, that query execution time is 32s based on 90mm data. What if change correlated to uncorrelated subquery like below 12345678910select tran_date, Product_cd, Merchant_Name, Trans_Amtfrom trans ajoin (select Product_cd, Max(Trans_Amt) as Trans_Amt, tran_date from trans where tran_date between '2020-10-01' and '2020-10-31' group by Producct_cd, tran_date) as bon a.Product_cd=b.Product_cd and a.Trans_Amt=b.Trans_Amt and a.tran_date=b.tran_dateorder by a.Product_cd the query execution time reduce to 26s. Exists clause and table join The most active accounts and their corresponding spending amount is another important KPI for product manage from account management perspective, additionally, if some condition applied such as account opened on certain month, we got the query from one business partner using exists statement down below 12345678select acct_id, trans_dt,count(*) as num_trans, sum(trans_amt) as tot_trans_amtfrom trans awhere date_key=202010 and exists(select 1 from acct b where a.date_key=b.date_key and a.acct_id=b.acct_id and year(acct_open_dt)*100+month(acct_open_dt)&gt;=202009)group by trans_dt, acct_idhaving count(acct_id)&gt;10order by num_trans desc from the query structure, we might be able to tell exists statement was put in where clause to get the account open date, its execution time is 13s, but it’s obviously not thinking about the whole business logic before, if the query changed to get target accounts for account open date first then did the calculation, the execution time will reduce to 3s like below 123456select a.acct_id, a.trans_dt, count(*) as num_trans, sum(trans_amt) as tot_trans_amtfrom trans a join acct b on a.acct_id=b.acct_id and a.date_key=b.date_keywhere year(b.acct_open_dt)*100+month(b.acct_open_dt)&gt;=202009 and a.date_key=202010group by a.trans_dt, a.acct_idhaving count(a.acct_id)&gt;10order by num_trans desc Reduce the data scope by subquery It’s very necessary to think about your query data scope first when you do a bunch of left outer join, especially, data volume is quite big on your main left table . Below query does a series left join by using all full tables data, but based on business requirement, only partial data is required on the major left table, so it cause somehow resources wasted in reflect on the execution time of 7s 123456select a.*from wfs_trans aleft join credit_decision b on a.tran_id=b.tran_idleft join finance_request c on a.req_id=c.req_idwhere cast(a.creation_date as date)&gt;='2019-01-01'order by creation_date desc after revised above query on left table, the execution time reduced to 5s (2000ms) 123456789select a.*from (select * from wfs_trans where cast(a.creation_date as date)&gt;='2019-01-01') as aleft join credit_decision b on a.tran_id=b.tran_idleft join finance_request c on a.req_id=c.req_idorder by a.creation_date desc Paging browse data Business users sometime complaint the SSRS or Power BI report is slow when they browse data by skipping pages, that is not the programming problem because on the backend the query to support the BI report like below 12345select prod_cd, post_dt, tran_dt, tran_amtfrom transwhere date_key=202010 and prod_cd='ABCD'order by post_dtoffset 1000000 rows fetch next 100 only; that will take 22s to return results based on 80mm data in single month. Thing thing is even the data is ordered by index column post_dt, database engine doesn’t know where is 1000000 row, it needs to recalculate again, so to answer that business concern, we recommend to apply some conditions parameter then start to browse data like below 12345select prod_cd, post_dt, tran_dt, tran_amtfrom transwhere date_key=202010 and prod_cd='ABCD' and post_dt='2020-10-15'order by post_dtoffset 1000 rows fetch next 100 only; by this way, the report will be presented by less than 1s.","link":"/2020/11/30/A-little-thought-on-SQL-query-performance-optimization-1/"},{"title":"Generate non comma delimiter CSV file","text":"This is some tip but sometime makes you life easier when you work with business team and deal with data from business input. Excel spreadsheet is kind of standard file format to communicate with business team, but in data manipulation side, it’s not a ideal input data format, technically, we usually convert spreadsheet to csv file. But default comma delimiter might cause some trouble because business data might contains quite a lot of , in attributes such as comments, suggestions, reasons etc. In order to better identify the column, non-comma delimiter should be used like | pipe. How do we generate the | delimiter csv file. There are two ways Change format setting system-wiseIn windows, open the control panel, find the Region setting, on the Formats tab click Additional setting, a pop-up window will show up, on that window, find the option list separator then type whatever delimiter you want to setup like |. ) save the setting, when you Save as csv file in excel, it will generate | separated csv file afterwards. Use database client tool to save query result to csv file with customized delimiterSome sql database client applications provide the functionality to save query result to csv file, like WinSQL. Before running your query, select execute to save to a text file, then select | from drop-down list","link":"/2020/12/07/Generate-non-comma-delimiter-CSV-file/"},{"title":"Blocking Process Monitoring and Auto Email Notification in SQL Server","text":"From function perspective, to maintain a large scale business data warehouse is for making database system stable, robust and fast, which is a essential part to boost business team productivity and performance. However, for business users, the fundamental thing is data, so data can be delivered in high frequency and in time is the cornerstone for all business analysis and BI reporting. Obviously, the primary mandate for data management team is highly monitor ETL jobs to promise data process running well and smooth and never being blocked or corrupt by using process. In this article, I am going to talk about how to build up a ETL job monitoring system to watch user query automatically in designed frequency. To accomplish this task, we need Create a view to collect user query in real time Create a stored procedure to detect blocking in different situations Create another stored procedure to handle the notification email sending Create a console script to overall control the workflow to make clear with those step by a process workflow chart, we can easily to see the logic on behand the scene Firstly, SQL agent job is going to check blocking transaction every 30 minutes, if there are some user queries blocked ETL job execution service account, then check back database to determine if those transactions are new, if they are new then write into database Blocking_Transaction table and send email notification to data management team to raise awareness; but if those blocking are not new, update last_time and count column in table then calculate if count number reach to 4 if yes, then send email to user and data management team. Create a view to collect user query in real time The first and most important thing for us is get the all transaction records against database so that we are able to know which user query transactions would block service account conduct ETL job. We need to utilize sys schema tables or dmv (dynamic management views) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071use controlDB;gocreate view dbo.usrQueryMonioringaswith t1 as(select b.spid, c.DBName as [DB_Name] ,a.total_scheduled_time/1000 as TotalScheTime_SS ,a.total_scheduled_time/1000/60 as TotalScheTime_MM ,a.total_elapsed_time/1000 as TotalElapTime_SS ,a.total_elapsed_time/1000/60 as TotalElapTime_MM ,a.last_request_start_time ,a.last_request_end_time ,a.login_time ,a.login_name ,a.host_name ,a.program_name ,a.nt_domain ,a.nt_user_name ,a.status ,a.is_user_process ,b.blocked --add blocking query information to quick locate the blocking transactions ,b.cmd, ,b.memusage*8 as Memusage_KB ,b.memusage*8/1024 as Memusage_MB ,c.Query from master.sys.dm_exec_sessions a join master.sys.sysprocesses b on a.session_id=b.spid and a.login_time=b.login_time cross apply ( select from master.sys.dm_exec_sql_text(b.sql_handle) ) c where a.is_user_process=1 --consider spids for user only, no system spids and a.session_id!=@@SPID --don't include request from current spid),t2 as(select spid ,DB_Name ,TotalScheTime_SS ,TotalScheTime_MM ,TotalElapTime_SS ,TotalElapTime_MM ,blocked as BlockedBy ,last_request_start_tiem ,last_request_end_time ,login_time ,login_name ,case when host_name like 'your server name%' then 'Server' else 'Client' end as 'Host Name' ,case when program_name like 'Microsoft SQL Server Mangement Studio%' then 'SQL Query' when program_name like 'Python' then 'Python Access' else 'Server general job or data provider' end as 'Program Name' ,nt_domain ,nt_user_name ,status ,case is_user_process when 0 then 'SA System' when 1 then 'User Initiate' end as 'Is_User_Process' ,cmd ,Memusage_KB ,Query from t1)select *from t2;go Before moving forward, we need a middle table to store all service account blocking transaction records 1234567891011121314151617use controlDB;gocreate table dbo.blocking_transaction(TraceID int identity(1,1) not null ,spid smallint not null ,blokedBy ,login_name nvarchar(128) not null ,nt_user_name nvarchar(128) null ,TotalElapTime_MM int not null ,ps_time datetime not null ,fr_Time datetime not null --first record time ,lr_Time datetime not null --last record time ,counter smallint not null ,cmd nchar(16) not null ,query varchar(max) null) Create a stored procedure to detect blocking in different situationsNext we will apply the main logics to detect service account blocking transactions in terms of different scenarios when we have the query transaction records data. In this time, we need to create a stored procedure to be executed by SQL job. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586create proc dbo.usp_blocking_trans (@counter smallint output, @rowcnt int output)asif OBJECT_ID(N'tempdb..#blked') is not nulldrop table #blked;declare @row_cnt intdeclare @sSQL nvarchar(max)declare @sParam nvarchar(4)begin trywith t1 as(select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from controlDB.dbo.usrQueryMonioring where BlockedBy!=0 and login_name='your service account'),t2 as(select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from t1 except select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from t1 a join t1 b on a.spid=b.BlockedBy --remove duplicates),t3 as(select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from controlDB.dbo.usrQueryMonioring where spid in (select BlockedBy from t2) -- in operator to hold potential multiple transactions),t4 as(select * from t2 union all select * from t3)select *into #blkedfrom t4/*return 1 if no record in temp table*/if not exists(select * from #blked)beginprint 'no record in temp table'return 1end/*check for incremental load*/elseselect @row_cnt=count(a.spid)from #blked a left join controlDB.dbo.blocking_transaction bon a.spid=b.spid and a.last_request_start_time=b.ps_timewhere b.spid is null and b.ps_time is null;if @row_cnt&gt;=1beginset @sSQL=N'insert into dbo.blocking_transactionselect a.spid,a.BlockedBy,a.login_name,a.nt_user_name,a.TotalElapTime_MM,a.last_request_start_time,getdate(),getdate(),1,a.cmd,a.queryfrom #blked a'exec sp_executesql @sSQLselect @rowcnt=@@ROWCOUNT --output number of row insertedreturn 2end/*update the old records on last_record_time and count if counter&gt;=3*/elseselect top(1) @counter = counter from controlDB.dbo.blocking_transaction order by lr_time desc;update a set lr_time=getdate(),counter=@counter+1from blocking_transaction a inner join #blked bon a.spid=b.spid and a.ps_time=b.last_reqeust_start_time;if (select top(1) counter from blocking _transaction order by lr_time desc)%4 = 0return 3elsereturn 4drop table #blkedbegin catchdeclare @err_msg nvarchar(1000),@err_num int,@err_line int,@syserr nvarchar(1000)select @err_msg=ERROR_MESSAGE(),@err_num=ERROR_NUMBER(),@err_line=ERROR_LINE()SET @syserr=N'usp_blocking_trans ended with error: Line='+convert(nvarchar(3),@err_line)+', Error_Msg='+@err_msg+''return -1end catchend Create another stored procedure to handle the notification email sendingSending email to raise awareness is another important task for the entire monitoring system, in this case there are two server levels, if user query blocking time less than 4 (4*30=120 mins) only email to data management team, otherwise, email to both data management team and user to cancel the transaction. If user ignores then data management team would do something to clear the lock by policy. we use parameters output from stored procedure usp_blocking_trans as input to be the key conditions 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106use controlDB;gocreate procedure dbo.usp_blocking_Email( @case smallint, @times smallint, @rownum int --handle multiple blocking case)asbegin trydeclare @result intdeclare @runtime nvarchar(20)declare @subject nvarchar(500)declare @sSQL nvarchar(200)declare @sParam nvarchar(100)declare @spid nvarchar(4)declare @body nvarchar(max)declare @counter intset @runtime=convert(nvarchar,GETDATE())set @sSQL=N'select top 1 @bID=blockedBy from controlDB.dbo.blocking_transaction where nt_user_name=''your service account'' and blockedBy!=0 by tranceID desc'set @sParam=N'@bID nvarchar(4) output'exec sp_executesql @sSQL, @sParam, @bId=spid outputset @subject=N'Warning: ETL job being blocked on '+@runtime+N'by SPID'+@spid+N', check it out!'if @case=2set @body=N'&lt;p style=&quot;color:#000000;font-family: Georgia; font-size: 18px; line-height: 18px&quot;&gt;Hi Team,&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;color:#000000;font-family: Georgia; font-size: 18px; line-height: 18px&quot;&gt;A job is now being blocked by a user process with &lt;font color=&quot;#FF5733&quot;&gt;&lt;b&gt;SPIS='+@spid+'&lt;/b&gt;&lt;/font&gt;&lt;br /&gt;&lt;br /&gt;Please check [controlDB].dbo.[blocking_transaction] table.&lt;br /&gt;&lt;br /&gt;&lt;/i&gt;&lt;font size=&quot;4&quot;&gt;More details information shown on below&lt;/font&gt;&lt;br /&gt;&lt;br /&gt;Thanks and Best Regards,&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;'+N'&lt;style&gt; th { background: #33FFBD height: 30px; } table,th,td { border: 2px solid green; } table { width: 80%; color: black; text-align: center; }&lt;/style&gt;'+ N'&lt;h2&gt;&lt;center&gt;&lt;font color=&quot;red&quot;&gt;ETL Job Info for Blocking&lt;/font&gt;&lt;/center&gt;&lt;/h2&gt;'+ N'&lt;table align=&quot;center&quot;&gt;' + N'&lt;tr&gt;'+ N'&lt;th&gt;SPID&lt;/th&gt;&lt;th&gt;BLkdBY&lt;/th&gt;' + N'&lt;th&gt;User Name&lt;/th&gt;' + N'&lt;th&gt;Elapse Time in MM&lt;/th&gt;&lt;th&gt;Process Run Time&lt;/th&gt;' + N'&lt;th&gt;First Record Time&lt;/th&gt;&lt;th&gt;Last Record Time&lt;/th&gt;' + N'&lt;th&gt;Counter&lt;/th&gt;&lt;th&gt;CMD&lt;/th&gt;' + N'&lt;/tr&gt;' + cast(( select top (@rownum) td=[spid],'' ,td=[blockedBy],'' ,td=[nt_user_name],'' ,td=[TotalElapTime_in_MM],'' ,td=[ps_time],'' ,td=[fr_time],'' ,td=[lr_time],'' ,td=[counter],'' ,td=[cmd],'' from controlDB.dbo.blocking_transaction order by TranceID desc FOR XML PATH('tr'), TYPE ) as nvarchar(max)) +N'&lt;/table&gt;' +N'&lt;p&gt;&lt;br /&gt;&lt;/p&gt;' +N'your team name';elseif @case=3set @body=N'&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;color:#000000;font-family: Georgia; font-size: 18px; line-height: 18px&quot;&gt;Hi Team,&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;A ETL job is now still being blocked by a user process with spid='+@spdi+' for more than '+@times+' checks adn '+@times*30+' minutescheck previous email or controlDB.dbo.blocking_transaction table for more details!&lt;br /&gt;&lt;br /&gt;your team name'execute @result=msdb.dbo.sp_send_dbmail@profile_name=N'your SMTP profile name',@recipients = N'email1@yourcompany.com;email2@yourcompany.com',@copy_recipients = N'email3@yourcompany.com',@subject = @subject,@body = @body,@body_format='HTML';return 0end tryend Create a console script to overall control the workflowat the last step, we need a control script put all above stored procedure together to be able to schedule SQL Agent job. 1234567891011121314151617181920212223use controlDB;godeclare @result_status smallintdeclare @counter smallintdeclare @rowcnt intexec @result_status=usp_blocking_trans @counter=@counter output, @rowcnt=@rowcnt outputif @result_status=1beginprint('no blocking process')returnendelseif @result_status=4beginprint('no new blocking process')returnendelsebeginset @counter=@counter+1exec usp_blocked_Email @case=@result_status, @times=@counter, @rownum=rowcnt","link":"/2020/12/17/Blocking-Process-Monitoring-and-Auto-Email-Notification-in-SQL-Server/"},{"title":"Hadoop Data Side Load from SQL Server","text":"Agile development and DevOps bring flexibilities and quick solutions to support business intelligent in timely manners. A technical platform and its associated applications and tools are able to turnaround very quick so that business analysts and data scientists would be able to leverage them to do the data modeling or machine learning, but in the other side, unlike functions buildup, data sync across different platforms is not that easy and quick especially for large organizations. Background and the gap of data modeling for Hadoop early adopterThese years, big data and Hadoop are kind of trend for next generation data technic. Many companies adopt that as major data platform, but the most of data is still allocated in RDBMS data warehouse, business intention is to leverage high quality data in SQL database to build their analytical work in Hadoop, the data consistency is the first consideration from data perspective, but it is not a easy task because data is going to migrate to different platform with different operating system (from Windows to Linux). Technically, the best solution for the project is the build the direct connection from SQL Server and SSIS to Hive by using Apache Sqoop or utilize the JVM to build JDBC connection by JAVA, but for large organization, applying a new tool on production needs a quite lot approve work; developing JDBC connection facility also needs multiple level testing, those are taking a long time. Therefore the solution is back to the foundation of the Hadoop - file system. Because SSIS cannot write to Hive directly using ODBC (before 2015 version). The alternative is to create a file with the appropriate file format and copy it directly to the Hadoop file system then use Hive command to write metadata to Hive metastore, the data will show up in the Hive table and also available in Cloudera Impala. File operation for data migration from SQL to HadoopIn our case, moving files can be a litter more complex because of crossing different operating system and platform as noted earlier. In this case, we need t a way to execute the dfs -put command on the remote server. Server tools enable us to execute the remote processes. Hadoop is build upon Linux, so bash shell script to execute the remote process. But that is quite challenged in real operation in production environment. What if the number of files is huge and cannot done by manually issuing command but have to do the batch operation automatically. What if files being moved crossing multiple platform from Windows to Linux local file system to Hadoop file system and if the data would be able to keep the original value and format in Hive or Impala table. Generate source data by CSV file from Windows sideFor multi-processing, SSIS is the sounds solution and tool set on Windows side which enables us to run SSH commands on the remote server from an Execute Process Task. Setting up a package to implement this process is relatively straight forward. Just set up a data flow as normal with a source component retrieving data from SQL Server data warehouse. Any transformation actions that need to be applied to the data can be performed. Ad the last step of the data flow, the data needs to be written to a file. The format of the file is determined by what the Hive system expects. The easiest format to work with from SSIS is a delimited format, with carriage return/line feeds delimiting rows, and a column delimiter like a comma(,) or pipe (|) separating column values. The SSIS flat file destination is designed to write these types of files. Automate file operation from Windows to Linux and HDFSOnce the file is produced, then use a file system task to copy it to a network location that is accessible to both SSIS server and Hadoop cluster. The next step is to call the process to copy the file into the HDFS. This is done through an Execute Process Task. It can be configured to use expressions to make this process more dynamic. In addition, if you are moving multiple files, it can be used inside a For loop in SSIS to repeat the process a specified number of times. To FTP data file from Windows to Linux, we can use either sftp or scp command but first you should make sure you have Hadoop access. But the problem is when we copy data to Hadoop, we need to provide the password to access the system, so the process is manual. As mentioned before, for larger scale data migration, we have to figure out the way of automation so that silent mode is needed to avoid manually provide user password then we can use script to schedule job to automate data process. Check OpenSSH Client installed and enable in your machineOpen Windows setting -&gt; App -&gt; Apps &amp; features -&gt; Optional feature, check if OpenSSH Client is on the list, if not you need to install that client tool On source side to create a public access keyThis step we need use cmd prompt or git bash, open git bash, change directory to home folder by issuing command cd $HOME, you will find the hidden folder .ssh, you can display all folder by command ls -la Issue below command to generate the public access key in order to enable the silent mode to Hadoop 1ssh-keygen -b 2048 -t rsa tap 3 time enter key to generate the key, the value for argument -b is key size and has to be the number at least 2048, value of -t is algorithm. You will find the 2 new files are generated, id_rsa stores your identification information and id_rsa.pub is your public key which we need to copy to Linux server. On target side FTP public key file to itWe can use scp command to upload public key to Linux server. 1scp -p 22 /home/yourUserName/.ssh/id_rsa.pub yourUserName@LinuxServerName:/home/yourUserName/.ssh/authorized_key You need to give password when issue above command to pass the authentication, but later this step will be bypass because of the public access key. Load file from Linux file system to HDFSEven Hadoop is installed on Linux, it has independent file operating system called HDFS, we need to issue command to transfer file between Linux and Hadoop. hdfs dfs command to manipulate Hadoop files, Hadoop is kind of remote server so we use -put argument to push file from Linux local to Hadoop; use -get to push file from Hadoop to Linux local, is this case we use put. 1hdfs dfs -put /yourLinuxFilePath/fileName.csv /HadoopFilePath/targetHiveDatabase.db Now, data file is in HDFS so that we can issue Hive command to write the table metadata into Hive metastore Configure Hive table metadata to match up with SQL ServerIn Hadoop side, we need to define Hive tables to store the data from SQL Serer. For keeping data consistency, we define the Hive table schema in terms of SQL table data type. We don’t use Ctrl-A(0x001) which is the default Hive column delimiter for flat file but use pipe bar(|) as field delimiter, because that isn’t supported well for use from SSIS in Window platform. Now let’s dig a litter deeper on Hive data type which is the critical element and the most important consideration to keep data consistent. Hive provides a layer on top of Hadoop data that resembles a RDBMS. In particular, Hive is designed to support the common operations for data warehousing scenarios. Thanks to Hive to build the bridge between Hadoop MapReduce and RDBMS so that many of these data types have equivalent values in SQL Server, but only a few are unique to Hive. Type Description Examples SQL Server Equivalent Float 4-byte single-precision floating point 25.189164 real Double 8-byte double-precision floating point 25.1897645126 float(53) Decimal A 38-digit precision number 25.1897654 decimal, numeric Boolean Boolean true or false TRUE FALSE bit Timestamp JDBC-compliant timestamp format YYYY-MM-DD HH:MM:SS:ffffffffff datetime, datetime2 define and create a Hive table is simple, the syntax just like SQL, but something different after all, it’s file system. 1234567891011121314create table yourTableName(col1 bigint, col2 int, col3 varchar(5), col4 decimal(5,2), col5 smallint, col6 tinyint, col7 timestamp, col8 float)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '|'STORED AS TEXTFILEtblproperties (&quot;skip.header.line.count&quot;=&quot;1&quot;) If data files come with header then you need to tell Hive skip the first row by tblproperties. We save above script as createTbl1.hql then use Hive beeline command to generate table 1beeline -f ./yourPath/createTbl1.hql &gt; /yourPath/hql.ot Load data file to Hive and Impala tableAs long as we push data file to Hadoop and Hive directory, we can easily to write data to Hive table by issuing beeline -e command like below 1beeline -e &quot;LOAD DATA INPATH '/HadoopFilePath/targetHiveDatabase.db/fileName.csv' INTO TABLE yourTableName&quot; the metadata will be in the Hive metastore. Cloudera Impala metastore is not sync with Hive automatically, so if you want to manipulate data in Impala, we need to issue impala command 12impala -q &quot;invalidate metadata yourTableName&quot;impala -q &quot;refresh yourTableName&quot; after that data is on both Hive and Impala table. Automate the whole processTo get a better understanding of how the process looks like, below workflow illustrates how data file read from SQL Server side and load into Hive table in HDFS cluster. now let’s create a script to bulk copy data file to Linux and save it as bulkcopy.sh 1scp -p 22 -r /yourDataFileFolder/ user@host:/home/user/dataFileFolder then for the task in Linux server side, we also create a script to push data from Linux local file system to HDFS then write data to Hive table and save it as bulkload.sh 1234hdfs dfs -put /yourLinuxFilePath/fileName.csv /HadoopFilePath/targetHiveDatabase.db/ &amp;&amp;beeline -e &quot;LOAD DATA INPATH '/HadoopFilePath/targetHiveDatabase.db/fileName.csv' INTO TABLE yourTableName&quot;impala -q &quot;invalidate metadata yourTableName&quot;impala -q &quot;refresh yourTableName&quot; so far, we have csv data file, bulkcopy.sh and bulkload.sh, now we will schedule a job to run those scripts automatically by using SSIS package last, configure the SSIS package to write log data either file or database for debugging. Both Hive table and Impala table will be good after run command invalidate metadata command in Impala shell for both HUE (GUI tool) and Linux server. Invalidate metadata command takes effect across both Impala shell and HUE web interface, but refresh command only tales effect on the environment the command run against. Data restatementIt’s quite common practice in production to reload historical data with reconcile with new or changed logic or even correct some mistake so remove partial data and then reload is also important. Than task is easy for RDBMS but what if the same situation take place in Hadoop, there is no update statement in Hive or Impala, so we need to do multiple steps: Start state: number of data, e.g. 3 files need to restate in HDFS Issue command hdfs dfs -rm to delete the 3 files with month_key (3 months) Run query from HUE, make sure 3 month data and won’t impact other month Run above scripts and SSIS package add new 3 data files back again, then the data would be back to the Hive and Impala table","link":"/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/"},{"title":"Automation Process for Email Attachment Excel in Python","text":"Working with business data, Excel spreadsheet is the most common file type you might deal with in daily basis, because Excel is a dominated application in the business world. What is the most used way to transfer those Excel files for business operation team, obviously, it’s Outlook, because email attachment is the easiest way for business team to share data and reports. Following this business common logic and convention, you may get quite a lot Excel files from email attachment when you involved into a business initiative or project to design a data solution for BI reporting and business analysis. The pain points is too much manual work dragging down efficiency of data availability and also increasing the possibility of human error. Imagine, every day get data from email attachment, you need to check your inbox every once for a while, then download those files from attachment, open Excel to edit data or rename file to meet data process requirement such as remove the protected password, after those preparation works all done, push data to NAS drive, finally, launch the job to proceed the data. It’s not surprise that how easily you might make mistake because any single step contains error would cause the whole process failed. It’s very necessary to automate the whole data process if business project turns to BAU (Business As Usual) program and you have to proceed data in regular ongoing basis. Python and Windows Task Scheduler provides a simple and fast way to solve this problem, now let’s take a look. Overall speaking, this task can be broken down by a couple of steps: Access Outlook to read email and download the attachment Remove Excel file protected password (it’s common in business file to protect data privacy) Manipulate and edit Excel file Copy file to NAS drive Setup the Windows Task Scheduler to run python script automatically. Interact Outlook and Excel with PythonIn this case, we are focusing on local operation because server setting is vary for different production environment. Python standard library can’t meet our need on this request, we have to leverage its third-party libraries to access outlook and interact with Excel so we are going to import win32com, openpyxl and shutil. We firstly work out workflow to guide us to compose the python scripts From the workflow chart, we can tell there are four functions in the streamline. The first one is read email and download attachment readmail(), this step is critical and the most import business logic build-in it. we are going to apply two business logics, one is the latest email, the other one is if the email is latest (current date) then check if the current date -1 equals to data date in the subject line. we will process file when all the requirements are satisfied. For Excel manipulation and spreadsheet decryption are pretty straight forward, no additional business logic so just apply the single function. We create 3 python module files then encapsulate them into the main readmail() function script. fileTransfer_module.py1234567891011121314151617import win32com.client as win32import osimport os.pathimport openpyxl as xlfrom datetime import datetime, timedeltaimport sysimport shutildef cpfile(src_file, tgt_file): if os.path.exists(tgt_file): os.remove(tag_file) shutil.copy(src_file, tgt_file) else: shutil.copy(src_file, tgt_file)if __name__ == &quot;__main__&quot;: cpfile(src_file, tgt_file) excelOp_mudule.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546import openpyxl as xlfrom file_Transfer_module import cpfiledef xltemp(file, file_ind): prd_dir = r&quot;\\\\your NAS drive UNC address&quot; if file_ind == 0: ## for dealing with multiple Excel files fn = &quot;yourExcelFile_1.xlsx&quot; desc_fn = prd_dir + os.sep + fn cpfile(file, desc_fn) elif file_ind == 2: fn = &quot;yourExcelFile_2.xlsx&quot; desc_fn = prd_dir + os.sep + fn cpfile(file, desc_fn) elif file_ind == 1: ## Excel file need to be manipulated wb_raw = xl.load_workbook(file) fn_tgt = &quot;yourExcelFile_3.xlsx&quot; desc_file = prd_dir + os.sep + fn_tgt wb_tgt = xl.Workbook() ws_tgt = wb_tgt.active ws_tgt.title = &quot;Sheet1&quot; ## define work sheet name ws_tgt['A1'] = &quot;Date&quot; ## define the first column name ws_tgt['B1'] = &quot;ID&quot; ws_tgt['C1'] = &quot;Type&quot; ws_tgt['D1'] = &quot;Amt&quot; ws_tgt['E1'] = &quot;Info&quot; cur_sheet = str((datetime.today().day) - 1) ws_src = wb_raw.get_sheet_by_name(cur_sheet) 1 = 1 row_lt = [] for each in ws_src.rows: if each[0].value is not None: row_lt.append(i) i += 1 else: break num_max_row_raw = row_lt.pop() num_max_col = ws_tgt.max_column for i in range(2, num_max_row_raw+1): for j in range(1, num_max_col+1): c = ws_src.cell(row = i, column = j) ws_tgt.cell(row = i, column = j).value = c.value wb_tgt.save(desc_file) wb_tgt.close() wb_raw.close()if __name__ == &quot;__main__&quot;: xltemp(file, file_ind) pwdDecry_module.py123456789101112131415161718192021222324252627282930313233343536import win32com.client as win32import osimport os.pathfrom datetime import datetimeimport sysdef xlpwd(): ''' function xlpwd() is aiming to peel off attachment excel file password to be able to be ready by program arg: opt1_opt2 value: &quot;opt1&quot; to deal with one excel file; &quot;opt2&quot; to deal with another excel file ''' opt1_opt2 = sys.argv[1] excel = win32.Dispatch('Excel.Application') mon = '0' + str(datetime.today().month) ## suppose password is letters and 2 digits month combination if opt1_opt2 = &quot;opt1&quot;: pwd = &quot;randomletters&quot; + mon fn = &quot;yourExcelFile_1.xlsx&quot; sn = 2 ## sheet number st = &quot;tab name 1&quot; elif opt1_opt2 = &quot;opt2&quot;: pwd = &quot;otherrandomletters&quot; + mon fn = &quot;yourExcelFile_2.xlsx&quot; sn = 1 st = &quot;other tab name&quot; prd_dir = r&quot;\\\\your NAS UNC address&quot; file = prd_dir + os.sep + fn wb_tgt = excel.Workbooks.open(file,0,False,5,pwd) ## open encrypted Excel file wb_tgt.Password = &quot;&quot; ## remove password wb_tgt.Worksheets(sn).Name = st ## define the tagart worksheet order and name wb_tgt.Save() wb_tgt.Close() excel.Quit()if __name__ == &quot;__main__&quot;: xlpwd() readMail.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import os, os.pathimport sysfrom datetime import datetime, timedeltaimport win32com.client as win32from excelOp_mudule import xltempdef readMail(): ''' function readMail is aiming to read outlook email attachment and download them arg: lob, app, c_type value: lob == 'you line of business' and app == 'application name generate the source file' and c_type == 'business category' ''' lob = sys.argv[1] app = sys.argv[2] c_type = sys.argv[3] outlook = win32.Dispatch(&quot;Outlook.Application&quot;).GetNameSpace(&quot;MAPI&quot;) type1 = outlook.Folders[&quot;your email address&quot;].Folders[&quot;your inbox subfolder1&quot;] type2 = outlook.Folders[&quot;your email address&quot;].Folders[&quot;your inbox subfolder2&quot;] type3 = outlook.Folders[&quot;your email address&quot;].Folders[&quot;your inbox subfolder3&quot;] ## if your inbox subfolder name has convention you can use while loop inbox = outlook.GetDefaultFolder(6) ## Microsoft Outlook API number for inbox is 6 type1_msgs = type1.Items type2_msgs = type2.Items type3_msgs = type3.Items inbox_msgs = inbox.Items file_ind = 0 folder = &quot;&quot; if lob == &quot;finance&quot; and app == &quot;app1&quot; and c_type == &quot;consumer&quot;: mail_items, file_ind, folder = type1_msgs, 0, &quot;finance_files&quot; elif lob == &quot;marketing&quot; and app == &quot;app2&quot; and c_type == &quot;small business&quot;: mail_items, file_ind, folder = type2_msgs, 1, &quot;small business files&quot; elif lob == &quot;inventory&quot; and app == &quot;app3&quot; and c_type == &quot;cooperate&quot;: mail_items, file_ind, folder = type3_msgs, 2, &quot;cooperate files&quot; else: mail_items = None path = r&quot;\\\\your file staing folder directory&quot; + os.sep + folder num_mails = len(mail_items) lst_mails = list(reversed(range(num_mails))) id_mail = lst_mail[0] email = mail_items[id_mail] subject = email.Subject ## Outlook API Subject line object if file_ind in (0, 2): num = -13 ## depends on your own situation date_mail_str = subject[num:] if date_mail_str[0] != ' ': date_mail_dt = datetime.strptime(date_mail_str, &quot;%b. %d, %Y&quot;) else: date_mail_dt = datetime.strptime(date_mail_str, &quot; %b. %d, %Y&quot;) elif file_ind == 1 and datetime.today().strftime(&quot;%a&quot;) != &quot;Mon&quot; and datetime.today().day &lt;= 10: date_mail_str = subject[-8:-1] + '2020' date_mail_dt = datetime.strptime(date_mail_str, &quot; %B %d %Y&quot;) elif file_ind == 1 and datetime.today().strftime(&quot;%a&quot;) != &quot;Mon&quot; and datetime.today().day &lt;= 10: date_mail_str = subject[-8:-1] + '2020' date_mail_dt = datetime.strptime(date_mail_str, &quot;%B %d %Y&quot;) received_time = email.ReceivedTime ## Outlook API receive time object today = datetime.today() ## check availiability of the latest file if today.year == received_time.year and today.month == received_time.month and today.day == received_time.day: avail_ind = 1 else: raise AttributeError(&quot;the latest file is not available! check with business team&quot;) ## check if the file is right copy if avail_ind == 1 and (file_ind == 0 or file_ind == 1): val_dt = received_time - timedelta(days = 1) if date_mail_dt.day == val_dt.day and date_mail_dt.month == val_dt.month and date_mail_dt.year == val_dt.year: valid_copy_ind = 1 ## usually business file for current date is yesterday's data if data is daily basis else: raise AttributeError(&quot;file copy is not right! check with business team&quot;) elif avail_ind == 1 and file_ind == 2: val_dt = received_time if date_mail_dt.day == val_dt.day and date_mail_dt.month == val_dt.month and date_mail_dt.year == val_dt.year: valid_copy_ind = 1 ## sometime current date file is current date data depends on business process else: raise AttributeError(&quot;file copy is not right! check with business team&quot;) if valid_copy_ind == 1: attachment = email.Attachment.item(1) report_name = attachment.FileName os.chdir(path) input_file = os.getcwd() + os.sep + date_mail_str + report_name if not os.path.exists(input_file): attachment.SaveAsFile(input_file) xltemp(input_file, file_ind) if __name__ == &quot;__main__&quot;: readMail() Schedule jobs to auto check your inbox and execute python scriptsIf you use Linux as your local machine then there are so many scheduling tools such as crontab, for Windows users, you can use GUI tool Task Scheduler to to the automation scheduling task. In this case, we use Task Scheduler. simply follow the wizard to create local jobs to implement above readMail.py and pwdDery_module.py scripts. After decrypted Excel files push to your server then trigger server jobs so that make the whole data process automated, no more manual work.","link":"/2020/12/15/Automation-Process-for-Email-Attachment-Excel-in-Python/"},{"title":"Increase Disk Space for Linux Virtual Machine Created by VMware","text":"Virtual machine is a major way to setup Linus development environment in Windows PC. One of the great things about newly Linux distro such as RHEL, Ubuntu, Centos etc is that most of them adopt to a LVM (Logical Volume Manager) filesystem which is a natural fit for Linux virtualized system like VMware or VitualBox. The main characteristic for LVM is that it is able to adjust filesystem volume dynamically by integrating multiple partitions which feel like one partition on one disk after LVM adjustment, moreover, adding and remove partitions to increase and shrink disk space also become very easy than before and this feature applies virtualized hard drive and makes it very easy to grow the disk space within few steps setup. You might ask what is the point to do that, I would say if you make virtual machine as your main development environment, the disk space is going to run out quickly as time goes by when more and more tools and libraries are installed. I have my VM hard disk initial 20GB by default setting but after I setup all my environment items my root directory only has 300MB space left. I will grow disk space with my Linux virtual machine to 40GB: Before we make our hands dirty, it’s necessary to get some basic understanding about the LVM in terms of 3 key things, they are PV (physical volumes), VG (volume groups) and LV (logical volumes). LVM integrates multiple partitions or disks into a big independent partition (VG), then formats it to create multiple logical volumes (LV) to be able to mount filesystem. Assign more space or partition on virtual machine settingThe first step is that assign space you want on virtual machine console, bear in mind, the host must be shutdown before doing anything change. In this case, we will use WMware as example, the VIrtualBox is similar. The fast and alternative way is use command line: 1vmware-vdiskmanager.exe -x 40GB &quot;your virtual disk name.vmdk&quot; Things may be tricky here if you were new to virtual machine as above highlight “Expand increases only the size of a virtual disk. Sizes of partitions and file systems are not affected“, it looks like no that easy to expand the disk space only by VMWare setup, because now it only adds more unpartitioned space to the virtual hard disk. Convert the unpartitioned space into usable filesystem so it can be included within the LVM filesystem. LVM configuration to extend new partition to mount filesystemBoot Linux VM and switch to root account. We first check out our disk partition status in advanced so that we can see clearly the change before and after, there are many commands in Linux e.g df -h, mount, here we use lsblk which is abbreviation of list block device 1lsblk -ip /dev/sda from return data, we know our total disk space is 40G, new unpartitioned disk /dev/sda3 with 20G is new added space, now let’s see how to create a new partition that takes up 20G space. Create a new partition for unallocated spaceThere are 2 disk partition tools we can utilize to create a new partition in according to your partition table format, fdisk is for MBR and gdisk is for GPT. We can use gdisk to check our partition format we can see MBR is the partition table for current system so that we will use fdisk to create partition. 1fdisk /dev/sda 123456789n (new)p (primary)3 (partition number, since 1st and 2nd partition already exists)select default first available cylinder to the default last cylinder.t (type)3 (partition number)8e (set type to LVM)p (view the new partitions layout)w (write out the new partitions layout to disk) Reboot the system so the new partition is recognized by the system; if you don’t want to reboot, there is a command to update partition table like source that is partprobe 1partprobe -s We first check our disk ID to make sure “8e“ is presented as Linux file system type ID and under Linux LVM system LVM operation to mount new partition to file systemThe next step is to use LVM to take the newly formed partition and turn it into a new Physical Volume, add it to a Volume Group, and finally assimilate its free space into a Logical Volume. Convert /dev/sda3 partition into a Physical Volume so LVM can make use of it: 1pvcreate /dev/sda3 Add the new Physical Volume to the Volume Group as additional free space: 1vgextend centos /dev/sda3 Note the free space now in the Volume Group which can now be assigned to a Logical Volume 1vgdisplay Have the Logical Volume (within the Volume Group) overtake the remaining free space of the Volume Group: 1lvextend -l +100%FREE /dev/centos/root 1vgdisplay You can see all free space is now allocated. The last step, we use xfs_growfs command to make logical volume live and mount filesystem so the new disk space can be used right away: 1xfs_growfs /dev/centos/root from above, you might feel the useful of LVM but extend is only one feature of it, we summarize all features of LVM by below table for reference: Task PV VG LV Filesystem (XFS/EXT4) Scan pvscan vgscan lvscan lsblk, blkid Create pvcreate vgcreate lvcreate mkfs.xfs/mkfs.ext4 Display pvdisplay vgdisplay lvdisplay df, mount Extend N/A vgextend lvextend (lvresize) xfs_growfs/resize2fs Reduce N/A vgreduce lvreduce (lvresize) N/A/resize2fs Remove pvremove vgremove lvremove umount Resize N/A N/A lvresize xfs_growfs/resize2fs Attribute pvchange vgchange lvchange /etc/fstab, remount","link":"/2021/01/08/Increase-Disk-Space-for-Linux-Virtual-Machine-Created-by-VMware/"},{"title":"How to let local images display on your hexo blog website","text":"Hexo blog framework friendly supports markdown which is the most popular syntax for technical blog writing. When you try to illustrate some ideas, it’s common to use screenshot photos in your articles to get reader better understanding so that you have to insert images into your blog, unlike typing text content that is straight forward, inserting images and deploying them to public blog site might be some tricks there, let’s see some common scenarios on images display issue and how to resolved them. Use absolute path to insert imageThe intuitive way for junior blogger is referencing absolute path + file name, I would say there is no any problem if the blog is only for local review or your local machine is a web server to host your blog website, otherwise, your images won’t be displayed on your public blog site after deployment if without any configuration. Like below shows when you use that way inert on your local blog, images can be displayed as expected but it will cause issue on display when you deploy your article to public blog website like below shown Use relative path based on _config.yml fileActually, hexo blog is powered by Node.js in the backend, its general configuration file _config.yml shows the way to properly allocate your resources including image, css, font and js etc. By default, your images should be put into blog/themes/your theme name/source/images , when you reference it, use below syntax no matter you use Windows or Linux, 1![your image's name](/images/your_image_file_name.png) now, you can push your blog to public blog site and your images can be presented there well but the problem is you lose your sight on local but only are able to see on blog website after you deployment, that is not you really want like below shown Use online image management toolTo be able to solve image display problem on both local and public site, the best way is utilize online image management website to upload your images then it will generate corresponding URLs for those photos, you can replace path name of your local images by URL. Now your image is going to be rendered by both your local markdown editor and web browser. Another advantage of managing your photos by online image tool is your images are in the cloud with permanent URLs so that you won’t lose them even something bad happened on your local machine. I use wailian.work to manage my images, it’s free and provide markdown link for each of your uploaded picture. A small thinking about Node.js top route design and file renderThe root cause of confusion here might be the Node.js route design, unlike Apache and Nginx web server which come with web container so that the route can be your local folder path name and render your local html file in the folder, in this case, you do tell the file local path from URL address, but for Node.js, it doesn’t have web container, therefore, we have to design route for local web files, in other words, you are not able to tell the local file path by URL address for Node.js web server. Let’s see an example 123456789101112131415161718const fs = require('fs');const path = require('path');const url = require('url');exports.static = function (request, response, staticPath) { let pathname = url.parse(request.url).pathname; pathname = pathname == '/' ? '/index.html' : pathname; let extname = path.extname(pathname); if (pathname != '/favicon.ico') { fs.readFile('./' + staticPath + pathname, async (err, data) =&gt; { if (!err) { let mime = await getFileMime(extname.split('.')[1]); response.writeHead(200, { 'Content-Type': `${mime}; charset=&quot;utf-8&quot;` }); response.end(data); } }) }} Assume user request images, the URL is probably like https://yourdomainname/images, but in the backend, in fact the Node.js read file from path './' + staticPath + pathname such as /root/web_project1/static/images. URL address doesn’t reflect file path, they are relatively independent in Node.js, by knowing that, we are going to dig a little deeper to take a look at _config.yml file and come up with idea why browser is able to render out to image file but text editor can’t. Open _config.yml file 1vim ./blog/_config.yml 12345# URL## If your site is put in a subdirectory, set url as 'http://example.com/child' and root as '/child'url: https://hermanteng19.github.ioroot: /permalink: :year/:month/:day/:title/ here list the route on your blog site which should be https://hermanteng19.github.io/ as home page and corresponding html file should be put into “/“ directory, what about images file? It is going to be “/images”, so far, we know the directory in below actually is route or web request URL, that is why text editor is not able to find the real file path. 1![mysql-connection](/images/mysql_conn_established.png) But, you might still confuse about why browser can render the image file, now let’s continue to see what’s going on web server. Enter your local hexo blog path 1ls -l ./blog you can find there is public/ folder which is your blog website “/root” folder, all web files such as html, css, js, image, font are in there, let’s enter it and take a look images/ folder is on the list, continue to drill down, mysql_conn_established.png file is on the list. So now you may be clear by Node.js route design, when web browser visits images on your blog website, it actually read the files from “/public/images/“, not from your local file absolute path like “themes/your_theme_name/source/images”. It does cause some confusion by this special type of route design, but in the other way, I would say it’s quite smart, you can design out a very nice, neat and beautiful URL for your website, which would bring very good user experience and impress website visitors. Think about no matter how ugly your local file path is like /root/jfdioaw/_jfidosjo_jfi123/oneMoreFolder/13u8030/product_1.html, but your website URL always looks like http://yourdomainname/business/product/product_1.html. At last, I want to thank CodeSheep for help me with the idea of leveraging online photo mangement tool to solve this problem!","link":"/2020/11/27/How-to-let-local-images-display-on-your-hexo-blog-website/"},{"title":"Data Manipulation and ETL with Pandas","text":"Pandas is the most used library in Python to manipulate data and deal with data transformation, by leveraging Pandas in memory data frame and abundant build-in functions in terms of data frame, it almost can handle all kinds of ETL task. We are going to talk about a data process to read input data from Excel Spreadsheet, make some data transformations by business requirement then load reporting data into SQL Server database. Pandas functions brief introductionFirst, we use read_excel() function to read in input Excel file, then use slicing function iloc() or loc() to get the data which need to be proceeded. The difference between iloc() and loc() is iloc() use row and column index number to slice data but loc() is use column name instead of index number. If we need to rename the column, we use rename(column={},inplace=True/False) function to do that. We also can use drop() function to drop columns. If we want to re-order the sequence of columns, the reindex() will help us to do that. For the purpose of merging or joining multiple tables, we can use merge() function. we can use to_sql() function to write data into database table after transformations. If we want to create UDF (user defined function) to apply to Pandas data frame then we can use apply(UDF name) function. For SQL windows function, Pandas also has equivalent way on those kind of function such as Top N rows per group which is equivalent to row_number()over() sql window function. Use assign(().groupby().cumcount()).query().sort_values() function Pandas use cases to manipulate Excel data to load into SQL Server database1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import numpy as npimport pandas as pdimport pyodbcfrom datetime import datetimefrom sqlalchemy import create_enginedef py2mssql(): engine = create_engine(&quot;mssql+pyodbc://yourServerName/database?driver=SQL Server Native Client 11.0&quot;) con_str = pyodbc.connect(&quot;DRIVER={SQL Server Native Client 11.0}; SERVER=yourServerName; DATABASE=yourDBName; Trusted_connection=yes&quot;) return engine, con_strdef dfstaging(df, sql_cmd): df.to_sql('your target table',py2mssql()[0],if_exists='replace',schema='dbo',index=False) conn = py2mssql()[1] cur = conn.cursor() conn.commit() cur.close() conn.close()def rpt2mssql(): df_pdr = pd.read_excel(r'C:\\your input file path\\fileName.xlsx',sheet_name='Sheet1') df_pd1 = df_pdr.iloc[:,[0,1,2,3,4,5,6,8,9,10]] ## iloc[row,col in num] df_temp = df_pdr.loc[:,['col1','col2','col3']] ## loc[row,col in name] up_col = &quot;&quot;&quot; sql statement &quot;&quot;&quot; dfstaging(df_temp, up_col) df1=pd.read_sql_table('your target table', py2mssql()[0], schema='dbo', index_col=None) ## rename columns in data frame to match with target table df1.rename(column={&quot;user ID&quot;:&quot;User_ID&quot;, &quot;item Key&quot;:&quot;Item_Key&quot;, &quot;connect ID&quot;:&quot;Connect_ID&quot;},inplace=True) df1[&quot;Cust_Id&quot;] = np.nan up_custid = &quot;&quot;&quot; update sql statement &quot;&quot;&quot; dfstaging(df1, up_custid) df2 = pd.read_sql_table('your target table', py2mssql()[0], schema='dbo', index_col=None) ## drop data frame column df3 = df2.drop(columns=[&quot;Connect_ID&quot;,&quot;Item_Key&quot;]) ## join two data frames df_join = pd.merge(df_pd1, df3, left_index=True, right_index=True) df_join[&quot;Process_dt&quot;] = datetime.today().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) df_final = df_join.drop(columns=[&quot;Connect_Id&quot;,&quot;Item_Key&quot;]) ## loop through all columns need to be converted to datetime datatype cvt_lt = [&quot;Work_Queue_Datetime&quot;,&quot;Complete_Datetime&quot;,&quot;Current_StatementDate&quot;] for col in cvt_lt: df_final[col] = pd.to_datetime(df_final[col]) ## get duplicate record from column value df_final[&quot;Dup_Ind&quot;] = [1 if x == &quot;Dup Submission&quot; else 0 for x in df_final[&quot;Reason_Comment&quot;]] ## re-order the columns new_index = [&quot;col5&quot;,&quot;col3&quot;,&quot;col1&quot;,&quot;col2&quot;,&quot;col4&quot;] ## apply the new order df_final = df_final.reindex(columns=new_index) df_final.to_sql('your target table',py2mssql()[0], if_exists='append', schema='dbo', index=None) up_dup = &quot;&quot;&quot; sql statement &quot;&quot;&quot; dfstaging(df_final,up_dup) if __name__==&quot;__main__&quot;: rpt2mssql() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport pandas as pdimport pyodbcfrom sqlalchemy import create_enginedef split_dt(ef_dt): ef_day=(ef_dt//10000)%100 ## extract day info ef_mo=ef_dt//1000000 ## extract month info ef_yr=ef_dt%10000 return pd.Series({'dd': ef_day, 'mm': ef_mo, 'yy': ef_yr})def py2mssql(): engine = create_engine(&quot;mssql+pyodbc://yourServerName/database?driver=SQL Server Native Client 11.0&quot;) con_str = pyodbc.connect(&quot;DRIVER={SQL Server Native Client 11.0}; SERVER=yourServerName; DATABASE=yourDBName; Trusted_connection=yes&quot;) return engine, con_strdef df2staging(df, sql_cmd): df.to_sql('your target table',py2mssql()[0],if_exists='replace',schema='dbo',index=False) conn = py2mssql()[1] cur = conn.cursor() conn.commit() cur.close() conn.close()def df2db(df_type1,df_type2): engine = py2mssql()[0] df_type1.to_sql('your target table', engine, schema='dbo', if_exists='append', index=False) df_type2.to_sql('your target table', engine, schema='dbo', if_exists='append', index=False)def buz2df(): df = pd.read_excel(r'\\\\your input file path\\filename.xlsx', sheet_name='Sheet1', index_col=None) df1 = df.iloc[:,[0,2,3,4,5,8]] ## Pandas equivalents for some SQL analytic and aggregate function ## Top N rows per group which is equivalent to row_number()over() sql window function df2 = df1.assign(rn=df1.sort_values(['your order by column name'],ascending=True) .groupby(['your group column']).cumcount()+1).query('rn == 1').sort_values(['your group column']) df3 = df2['the date column you want to split'].apply(split_dt) df3 = df3.astype(str) ## convert data frame elements as string df3['the data column you want to split'] = df3['mm'] + '/' + df3['dd'] + '/' + df3['yy'] df3['the data column you want to split'] = pd.to_datetime(df3['the data column you want to split']) df2 = pd.concat([df2,df3['the data column you want to split']], axis=1) df_staging = df2.iloc[:,[0,1,5,7,8]] df_type1 = df_staging.loc[:['col1','col2','col3','col4']] df_type2 = df_staging.iloc[:,[2,3,4,5]] df2staging(df_staging) df2db(df_type1,df_type2)if __name__==&quot;__main__&quot;: buz2df()","link":"/2020/12/20/Data-Manipulation-and-ETL-with-Pandas/"},{"title":"Linux network auto boot and restart","text":"Centos 7 network is disabled by default after installation and initialization, which causes network connection can not be made until you manually turn on it That is such annoying when you usually use SSH tool remote connect to centos workstation or server, so it’s quite necessary to turn on the network automatically every time reboot machine or VM. To accomplish that, we need to modify the network configuration file. Auto Boot NetworkVIM open config file: /etc/sysconfig/network-scripts/ifcfg-ens331vim /etc/sysconfig/network-scripts/ifcfg-ens33 Revise the ONBOOT value to be “yes” Save and quit config file by :wq vim commandRestart NetworkTo be able to make that change taking effect, network needs to be restarted by below command 1systemctl restart network.service something trick here is some blog articles mention that part is only systemctl restart, which won’t work if there is no network.service, I confuse and spend many time to figure out that, thanks to CodeSheep whose video shed the light on it and help me to solve that problem.","link":"/2020/11/25/Linux-network-auto-boot-and-restart/"},{"title":"Oracle client side configuration for 12C","text":"The client side need to do some configurations after Oracle 11g upgrade to 12C on Server in order to make database server is connectable. Before starting to configurate your clients, you have to get the below new server information from DBA Host name Port number Service name Your user name(usually it won’t be changed and replicated from old version) Password(initial password for test connection then you need to update it) and then you need to make a new connection strings to add it into ORA file(*.ora) Oracle SQL Developer configurationThe simplest way to connect to oracle 12C is by using Oracle sql client tool SQL Developer, it uses build-in JDBC driver to make connections and GUI connection wizard guide you fill in the server information without updating ora file. It’s better use version 19.1 and above. fill in all info in connection window, then click Test button to test connection Other client tool connection by ODBC driver such as SSIS and WinSqlThe first step, you need to install the ODBC driver, there is another client tool comes with ODBC driver called Oracle Database Client12cSQLDev 12.1.0.2 R02, install that in your local machine, then update your environment variable to make sure Oracle home directory being added into it. Update environment variableIf you installed both Oracle SQL Developer and Client12cSQLDev 12.1.0.2 R02, there is going to be 2 Oracle folders, one is client_32 which is 32 bit, the other one is client_1 which is 64 bit application, you need to add both of them into you environment variable. The path is usually C:\\app\\product\\12.1.0\\client_* Configurate ora file after setup oracle environment variableThe tricky thing is the ODBC driver only works well for 32 bit version not 64 bit so we can only edit the ora file for client_32. You can find out the ora file on below directory C:\\app\\product\\12.1.0\\client_32\\network\\admin\\tnsnames.ora append following string to the ora file save andclose the ora file, then go to windows ODBC data source center to finalize ODBC driver configuration. Configurate ODBC driver and DSN to test connection Data Source Name is customizable; TNS Service Name is the connection string name in ora file CAP12CPRD_32bit in this case; fill in user ID then click test connection button, a prompt window will pop out to let you input password. SSIS package connection manager configurationOpen SSIS client SSDT to configure connection manager for Oracle data source. SSDT -&gt; open a project -&gt; right click data source on right side panel -&gt; follow the wizard both preinstalled .net provider and native ole db provider for Oracle are working well server name is as the same as connection string name in ora file in this case is CAP12CPRD_32bit. Give a name for oracle new data source then connection manager can be created to Oracle database. SAS connection to Oracle 12C prerequisite: SAS grid server local DSN need to be created before testing; PC SAS login server is necessary SAS grid local DSN for Oracle 12C (ask for system admin create local DSN and return the name to you and it is going to be the value for path when you create new library for Oracle 12C) SAS EG connection to 12C: open SAS EG 7.1 and make sure it connects to grid server open a new code window and create a new library by running below statement 1libname ora12C oracle user=&quot;yourUserName&quot; password=&quot;xxxx&quot; path=sas_grid_local_DSN schema=oracle_schema make sure the program run against SASApp rather than localhost new Oracle library would be created under the SASApp dataset list","link":"/2020/12/11/Oracle-client-side-configuration-for-12C/"},{"title":"Linux Network Management Tool nmcli","text":"There are over 60 Linux networking commands you can utilize to do all the system network configurations, some of them are well known and widely used such as ifconfig, ip addr, traceroute, netstat and ping etc.., one command is very useful but relatively few being used, that is nmcli which is used for controlling your network, just like its name network manager and also can do all the thing to configure your network like displaying network device status, create, edit, activate/deactivate and delete network connection. syntax and optionsnmcli command has 2 arguments, one is option and the other one is object. 1nmcli [options] object {command | help} You can quick check help to get the information nmcli -h OPTIONS -t[erse] terse output -p[retty] pretty output -m[ode] tabular|multiline output mode -c[olors] auto|yes|no whether to use colors in output -f[ields] &lt;field1,field2,…&gt;|all|common specify fields to output -g[et-values] &lt;field1,field2,…&gt;|all|common shortcut for -m tabular -t -f -e[scape] yes|no escape columns separators in values -a[sk] ask for missing parameters -s[how-secrets] allow displaying passwords -w[ait] set timeout waiting for finishing operations -v[ersion] show program version -h[elp] print this help OBJECT g[eneral] NetworkManager’s general status and operations n[etworking] overall networking control r[adio] NetworkManager radio switches c[onnection] NetworkManager’s connections d[evice] devices managed by NetworkManager a[gent] NetworkManager secret agent or polkit agent m[onitor] monitor NetworkManager changes From command syntax, you can tell the options can be multiple but object is only one, because nmcli only return information or do configure for one object a time. the place of options and object can not be switched, must follow option first and object second. ExamplesShow network status1nmcli -p general status nmcli permission1nmcli general permission Enable and disable network1nmcli networking on | off | connectivity Radio wifi transmission control1nmcli radio wifi | wwan | all Show local network connection1nmcli connection show Show network device status1nmcli device status Show overall network and device information1nmcli device show that command will show device, device type, network connection, gateway, route, ip4, ip6, dns Show wifi connection status1nmcli device wifi list Configure wifi connections for UbuntuBy default, Ubuntu wifi connection is disabled, so if you want to use wifi, something need to be done to be able to connect to WAN. Firstly, show your wifi device 1nmcli device status Secondly, turn on the wifi radio transmission 1nmcli radio wifi on Thirdly, show all your wifi networks 1nmcli device wifi list Finally, connect your wifi network by password 1nmcli device wifi connect &lt;your wifi network name&gt; password &lt;your password&gt;","link":"/2020/12/14/Linux-Network-Management-Tool-nmcli/"},{"title":"Python Environment Setup for Implementation","text":"Python is a good scripting language to boost your productivity on data analysis and BI reporting. As open source language, you can easily get the binary installation file from python official site for windows and source code on vary versions for Linux, in production, it’s better choose installation approach by source code. We also need to setup python environment after installation so that we can not only use python interpreter to develop but also make it executable by CLI and even some ETL tool such as Microsoft SSIS. Python environment variable configuration and local folder set up for your file, package and libraryIf python is installed in system-wise, then you need to create some new folders to store you python file, package and library, e.g. python install path is “D:\\Python36&quot;, then you need to add python executable interpreter to be a part of the PATH variable. Next create python environment variable PYTHONPATH with the following paths and create empty file __init__.py file in each of these folders: create a new folder under D drive “D:\\pyLib” and set that directory as value of PYTHONPATH and create __init__.py file in “D:\\pyLib” you can also create subfolder to assign different permissions for different user group create a subfolder “D:\\pyLib\\AD-group1” and create the __init__.py file in it. create a subfolder “D:\\pyLib\\AD-group2” and create the __init__.py file in it. For Linux, if you install python3 by source code and directory is /usr/local/python3, then edit ~/.bash_profile file, append the python directory into PATH 12# Python3export PATH=/usr/local/python3/bin/:$PATH then run source ~/.bash_profile let setting take effect if your system pre-installed python2 then it’s necessary to make a soft link 12ln -s /user/local/python3/bin/python3 /user/bin/python3ln -s /user/local/python3/bin/pip3 /user/bim/pip3 setup name space and package python scripts for development project to be able to importable create or edit environment variable and add your python files folder into your system directory enter your python file folder to create an empty file __init__.py file open terminal prompt type python to active python interactive console import sys execute sys.path to make sure your python file folder is recognizable by python Python readiness test in localhost for SQL database connection (Anaconda virtual environment)First check python and Ipython version by issue command python --version and ipython --version. Anaconda almost pre-installs all python prevailing and popular libraries in its virtual environment, to check library list by using command pip list Python and SQL database connection facility with supported driversDepends on what python library do you install for database connectivity, it usually comes with function to show you available drivers to connect python to your database, e.g: pyodbc, use drivers() function to list the odbc drivers Python SQL Server database connection and data taskSQL Server database can be connected both by DB API (pyodbc) and ORM (sqlalchemy), create a py script and run sql query from user input 123456789101112131415161718192021222324252627282930313233343536373839import pyodbcimport sqlalchemyfrom sqlalchemy import create_engineimport pandas as pddef pyquery(conn): cnxn = pyodbc.connect(conn) cur = cnxn.cursor() sql_cmd = input(&quot;Input your sql command with database name: &quot;) result = cur.execute(sql_cmd) for row in result: print(row) cur.close() cnxn.close() def py2mssql(): way2conn = input(&quot;Do you want to connect to db by [ORM] or [DBAPI]: &quot;) if way2conn.upper()==&quot;DPAPI&quot;: dsn_str = input(&quot;Do you want to connect to db by [connection string] or [DSN]: &quot;) if dsn_str.upper()==&quot;DSN&quot;: dsn_name=input(&quot;Please enter your DSN name: &quot;) conn_dsn='DSN{};Trusted_connection=yes'.format(dsn_name) conn=conn_dsn pyquery(conn) else: conn_str='DRIVER={SQL Server Native Client 11.0}; SERVER=YOURSERVERNAME;DATABASE=YOURDB;Trusted_connection=yes' conn=conn_str pyquery(conn) else: ser_name=input('Please enter your server name: ') db_name=input('Please enter your database name: ') tbl_name=input('Enter the table name: ') orm_dict={'servername':'{}'.format(ser_name), 'database':'{}'.format(db_name), 'driver':'driver=SQL Server Native Client 11.0'} engine=create_engine('mssql+pyodbc://'+orm_dict['servername']+'/'+orm_dict['database']+'?'+orm_dict['driver']) df=pd.read_sql_table(tbl_name, engine, index_col=0, schema='dbo') print(df.head(10)) if __name__==&quot;__main__&quot;: py2mssql() script both can be ran directly or imported (recommend) after setup PYTHONPATH variable in your account and copy that script over to the path of environment variable. Python DB2 database connection and data taskWe can only connect to DB2 by DBAPI (pyodbc), connection string doesn’t work but only DSN (PRD1 was setup as system DSN in local and server) plus user id and password. Use below script to try to connect 123456789101112131415161718192021222324252627import pyodbcimport getpassimport pandas as pddef py2edw(): uid=getpass.getuser() print(&quot;Your user id is '{}'&quot;.format(uid)) pwd=getpass.getpass(&quot;Please enter your db2 password: &quot;) dsn=input(&quot;Please enter your db2 dsn name: &quot;) conn_dsn='DSN={0}; UID={1}; PWD={2}'.format(dsn, uid, pwd) conn=pyodbc.connect(conn_dsn) cur=conn.cursor() sql_cmd= ''' select * from table ''' result=cur.execute(sql_cmd) for row in result: print(row) cur.close() df=pd.read_sql_query(sql_cmd, conn) print(df.info()) print(df) conn.close() if __name__==&quot;__main__&quot;: py2edw() one thing need to be aware is the script better runs on the shell than python interactive console because pyQT doesn’t support password masking Productionize Python script by passing in parametersIn this section we will demonstrate how you can parameterize your code in python or pyspark so that you can use these techniques before deployed your script into production for automation. It’s best practice to parameterize database names, tables names, and dates so that you can pass these values as inputs to your script. This is beneficial when writing code for values that are dynamic in nature, which can change depending on the environment and/or use case. The key module is from python standard library: sys. Assign variables through sys.argv[...] 1234567891011121314151617import pandas as pdfrom sqlalchemy import create_engineimport sysdef py2mssql(): ser_name=sys.argv[1] # sys.argv[0] is assigned to python script itself, all other parameters start from 1 db_name=sys.argv[2] tbl_name=sys.argv[3] orm_dict={'servername':'{}'.format(ser_name),'database':'{}'.format(db_name), 'driver':'driver=SQL Server Native Client 11.0'} engine=create_engine('mssql+pyodbc://'+orm_dict['servername']+'/'+ orm_dict['database']+'?'+orm_dict['driver']) df=pd.read_sql_table(tbl_name, engine, index_col=0, schema='dbo') df.to_sql('pandas_sql_test', engine, schema='dbo', if_exists='replace', index=False) if __name__==&quot;__main__&quot;: py2mssql() run python script in CLI (command line interface) by following parameter values 1ipython py2mssql_argvdf.py yourSvrNm yourDBNm yourTblNm Encapsulate into SSIS to minimize change in production deployment – python interpreterAn alternative way to apply python in production is leverage current SSIS package and embed python script in process task you can hard code the configuration or use expression (VB) through variables in package Encapsulate into SSIS to minimize change in production deployment – batch processuse batch process by .bat file also can achieve that task Call user defined module or function in python scriptIt’s very efficient to create bunch of generic module packages to contain functions to be used widely by other python scripts for specific tasks. 1. Firstly, setup python environment variable to include directories which are recognized by python 2. Create __init__.py file (can be empty) in these folders 3. Create python programs and save script should end up with if __name__==”__main__“: main() 4. Ready to import user modules and functions 1234567import pandas as pdimport py2mssql_module as dbimport systbl_name=sys.argv[1]df=pd.read_sql_table(tbl_name, db.py2mssql('serverName','databaseName'), index_col=0, schema='dbo')print(df.head(10))","link":"/2020/12/08/Python-Environment-Setup-for-Implementation/"},{"title":"Remote Connect MySQL Server from Client Machine Setup","text":"You can only connect to MySQL Server from localhost after MySQL installation by default, but in production, all MySQL clients remotely connect to server, for simulating real production environment in your home network, some configurations need to be made to be able to let you connect MySQL from client machine other than localhost. Revise or create MySQL configuration file (RHEL or Centos 7)Modify or create /etc/my.cnf file1vim /etc/my.cnf add a configuration item bind-address and let it value to be your MySQL server host ip address (eg. 192.168.1.114)1bind-address=192.168.1.114 save and exit1:wq Restart MySQL service1systemctl restart mysql.service Open TCP port 3306 using iptablesSetup /sbin/iptables and let firewall opens port on 3306 for any remote machine1/sbin/iptables -A INPUT -i eth0 -p tcp --destination-port 3306 -j ACCEPT To specific client host machine to access port 3306, you can explicitly assign ip address (eg. 192.168.1.134)1/sbin/iptables -A INPUT -i eth0 -s 192.168.1.134/24 -p tcp --destination-port 3306 -j ACCEPT Finally save IPv4 firewall rules1/sbin/iptables-save &gt; /etc/sysconfig/iptables If it doesn’t work, for testing and develop environment, you can turn off firewall12systemctl stop firewalld.servicesystemctl disable firewalld.service Grant remote access to new MySQL databaseCreate a new database1create database foo; Grant remote user access to a specific database on user host machine (192.168.1.134)1grant all privileges on foo.* to herman@'192.168.1.134' identified by 'youOwnPasswd'; Grant remote access to existing MySQL database for user (herman) on its host machine (eg. 192.168.1.134)Require a set of two commands12update db set Host='192.168.1.134' where Db='mysql';update user set Host='192.168.1.134' where user='herman'; Open MySQL client tool on workstation with address 192.168.1.134 like MySQL workbenchCreate new connection using username and passwordusername: hermanpassword: youOwnPasswdport: 3306 Connection is created and foo database is on the list under user herman","link":"/2020/11/25/Remote-Connect-MySQL-Server-from-Client-Machine-Setup/"},{"title":"SQL Server Table Partitioning in Large Scale Data Warehouse 3","text":"This post is the last part on series of table partitioning, as plan this part is going to focus on some advanced topics like partition merge, split, conversion and performance optimization in terms of different business demands. The first thing we will talk about might be interesting, we know one benefit of table partitioning is speed up loading and archiving data, we can easily feel the performance improvement on query against large data after table partitioning, but data archiving is not that apparent and it’s on the lower file system level to helps you mange data more on the backend efficiently by file group and database file. After all, we are intent to manage data through partitions and additionally, manage partitions through database files, but that is not that straight forward. Efficient way to manage partition using database file group and fileWe might suppose out data would be stored like below if table is partitioned by month: January -&gt; partition number 1 February -&gt; partition number 2 … November -&gt; partition number 11 December -&gt; partition number 12 The similar situation on daily partition: Day 1 -&gt; partition number 1 Day 2 -&gt; partition number 2 … Day 29 -&gt; partition number 29 Day 30 -&gt; partition number 30 Day 31-&gt; partition number 31 Let’s assume we have daily transaction table currently has 31 partitions for each month, which is supposed to store daily data to corresponding partition in terms of partition number (1 to 31) and we also create 31 database files under the same file group (one file group for each month), which is supposed to have connection to their corresponding partitions in terms of file number (1 to 31). but, the question is that is data really saved in partition as above pattern showed? and Is partition number really associated with corresponding database file? The answer is no, actually the data will evenly spread to all partitions and each database file associates with all partitions. But how come we say table partitioning is the good way to mange our data and data file, the solution is file group. Create multiple file groups in stead of single file group Manage monthly partition data, we can create 12 file groups for each month with number 1 to 12 instead of one single file group with suffix of year. E.g. Create file group BUSINESS_DB_FG_TRANS_202001...202012 Don’t create file group ‘BUSINESS_DB_FG_TRANS_2020` Create single database file under its corresponding file group. Manga monthly partition data, we can only create single data file under each file group Create BUSINESS_DB_F_TRANS_202001 under file group BUSINESS_DB_FG_TRANS_202001 Create partition scheme which will carry the partition function to associate with file group In this way, data will be loaded into the right file group and right database file, in another word, we can say January 2020 data is allocated at database file BUSINESS_DB_F_TRANS_202001 under the file group BUSINESS_DB_FG_TRANS_202001 Above all, multiple file groups and single file mode, which is the efficient way to manage data, we can assign initial, auto growth and limited size in terms of daily or monthly data volume. If we keep single file group for daily or monthly partition table, we can create any number of files as long as assign the right initial, auto growth and limited size number, but we lose control from physical data management point of view. Partitions merge, split and use case for dealing with transaction data rollupIn this selection, we talk about partition merge and split, actually, they are technics of data management. In production, we either merge partitions or split them in according to data storage and management needs, let’s assume we have daily transaction table with 365 partitions for each year, now think about do you want to keep that mange partitions or you want to rollup to a monthly partition table so that there are only 12 partitions in each year, obviously, in most of case, we choose rollup transaction data into monthly partition table because that is much easier to maintain partitions such as yearly partition expansion for future year data allocation. In another side, what if we get a big chunk of data such as 5 years sales and order data, do you want to keep this big volume data or you want to split it by year or month into different partition, apparently, it’s more efficient to partition table by year or month. Now, let’s see how do we apply partition merge to daily transaction data rollup. Basic setups before daily partition merge into monthly partitionIn this use case, we need two partition tables, one is daily partition table we named it trans_current, it has below setups file group: BUSINESS_DB_FG_TRANS_2020 partition function: BUSINESS_DB_PF_TRANS_CURRENT 12create partition function BUSINESS_DB_PF_TRANS_CURRENT AS range left values (20200101, 20200102,...,20201230,20201231) go partition scheme: BUSINESS_DB_PS_TRANS_CURRENT 123create partition scheme BUSINESS_DB_PS_TRANS_CURRENT as partition BUSINESS_DB_PF_TRANS_CURRENT to (BUSINESS_DB_FG_TRANS_2020,...,BUSINESS_DB_FG_TRANS_2020) the another table is monthly partition table, its name is trans, it has below setups file group: BUSINESS_DB_FG_TRANS_2020 partition function: BUSINESS_DB_PF_TRANS 12create partition function BUSINESS_DB_PF_TRANSas range left for values (20200131, 20200228,...,20201130,20201231) go We will do the partition switching after merge so that the partition function of monthly table set up like above as the last day of month to be boundary value. partition scheme: BUSINESS_DB_PS_TRANS 123create partition scheme BUSINESS_DB_PS_TRANS as partition BUSINESS_DB_PF_TRANS to (BUSINESS_DB_FG_TRANS_2020,...,BUSINESS_DB_FG_TRANS_2020) Workflow of partition merge and switchingIn order to reduce the down time in production, we don’t merge partition on daily transaction table but we first partition switch to a middle temp table switch table then do the partition merge on that, finally switch partition to monthly transaction table. Partition merge and switchingAs workflow chart shows, we use previous part code to firstly create switch table in terms of trans_currsnt table structure then switch 30 (31) partitions for one month to switch table, now switch table contains the whole month transaction data evenly allocated into daily partitions, it’s time for us to merge daily partition into one (the last day of the month) then switch to transaction monthly table. Before that, we need to bear in mind that we have to disable columnstore index then rebuild it after partition merge if you use SQL Server 2014 or order version, we use SQL Server 2012 so we need to do that. 123use business_dbgoalter index CIS_TRANS_CURRENT ON dbo.TRANS_CURRENT disable 123use business_dbgoalter index CIS_TRANS_CURRENT ON dbo.TRANS_CURRENT enable create a functional stored procedure for partition merging in general cases 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859use business_dbgocreate procedure dbo.usp_mergePartitions( @table_name varchar(100) ,@start_boundary int ,@end_boundary int ,@fisc_month_start int ,@partition_interval ,@dbname varchar(100)='business_db')as begindeclare @sp_msg varchar(100),@rangeBoundary intbegin trybegin trans--check some input variable valuesset @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset @sp_msg='end up with @table_name empty, check input values'goto errorProcenddeclare @sSQL varchar(max),@irange int,@fisc_month_end int,@pf_name varchar(100)set pf_name=@db_name+'_PF_'+@table_nameset @sParam='@rangeOut int output'set @irange=@start_boundaryset @fisc_month_end=(abs(@fisc_month_start-1)+@partition_interval)%@partition_intervalwhile(@irange&lt;@end_boundary)beginset @sSQL='use '+@db_name+'; select @rangeOut=cast(value as int) fromsys.partition_range_values as prv inner join sys.partition_functions as pfson prv.function_id=pfs.function_id where pfs.name='''+@pf_name+''' andboundary_id=$partition.'+@pf_name+'('+convert(char(8),@irange)+')'exec sp_executeSQL @sSQL, @sParam, @rangeOut=@rangeBoundary outputif @irange=@rangeBoundarybeginset @sSQL='use '+@db_name+'; alter partition function '+@pf_name+'()'+' merge range( '+convert(char(8),@irange)+')'exec sp_executeSQL @sSQL--dim_calendar is a common time table, day_key is 8 digits integer YYYYMMDD like 20200101select @irange=a.day_key from dim_calendar as ainner join dim_calendar as b on a.daycounter=b.daycounter+1where b.day_key=@irangeendendcommit transreturn 0end trybegin catchrollback--add some try catch block statementreturn -1end catchend Now, we create an API to pass the data to stored procedure based on our example 1234567891011121314151617181920212223242526272829303132use business_dbgocreate procedure trans_partiton_mergingas begin declare @return_result intbegin tryselect top 1 @date_key=load_dt/100 from staging.dbo.trans_currentif @date_key%100&gt;1set @date_key=@date_key-1elseset @date_key=@date_key-89select @rrange=max(day_key),@lrange=min(day_key)from business_db.dbo.dim_calendar where month_key=@date_keyexec @return_result=business_db.dbo.sp_mergePartitions @table_name='switch_trans',@start_boundary=@lrange,@end_boundary=@rrange,@partition_interval=1,@dbname='business_db'if(@return_result&lt;0)begin--add some log statementreturn -1endreturn 0end trybegin catch--add some try catch block statementreturn -1end catchend There is only one partition left for January 2020 after partition merge whose boundary value is 20200131 so that we can partition switch to monthly table (detail reference previous post). Table type conversion —- a more efficient way to handle transaction data rollupYou might think the partition merge is complicated and what performance benefit you can gain by this technology. Actually, partition merge performance is not ideal, depends on how big data size in your partition, the partition merge may take a quit a long time to conduct, 50 columns and 100mm records data roughly takes 3 hours, so the question is is there any another way to do the same task but with simpler and more efficient approach, the answer is yes, table partitioning technology provides us another potential way to “merge partitions” with more than 10 times increase on performance. The main idea is convert switch table from partition table to non partition table then partition switching from non partition table to target partition table (details for partition switching reference previous post). Use create clustered index clause to rebuild clustered indexIt’s kind of wired, anything of creating clustered index is related with our topic, it looks not, but it does, I found that just by chance, when I rebuild clustered index for partitioned table using with drop_existing, I suddenly realized that the partitioned table become non partitioned table with new clustered index setup. The good thing is that step is really fast, as I mentioned 50 columns and 100mm records partitioned table only takes 15 minutes to become non partitioned table. you can know from below screenshot, switch table from partitioned table become non partition table by table property dialogue box below is the reason why with drop_existing statement is fast from Brian Moran post. Changing your clustered indexes by using the CREATE INDEX statement’s DROP_EXISTING clause is faster. The DROP_EXISTING clause tells SQL Server that the existing clustered index is being dropped but that a new one will be added in its place, letting SQL Server defer updating the nonclustered index until the new clustered index is in place. (Note that you can use DBCC DBREINDEX to rebuild existing indexes because it won’t cause SQL Server to rebuild a nonclustered index. But you can’t use DBCC DBREINDEX to change the columns in an existing index.) With DROP_EXISTING, you save one complete cycle of dropping and recreating nonclustered indexes. Additionally, SQL Server won’t rebuild the nonclustered index at all if the clustered index key doesn’t change and is defined as UNIQUE, which isn’t an obvious performance benefit of defining a clustered index as UNIQUE. Using the DROP_EXISTING clause can be a huge time-saver when you need to change the clustered index on a table that also has nonclustered indexes. Workflow of non partitionalize and partition switchingThe central idea for the process is use partition switch for every data load both on partition switching from daily transaction table to switch table and from switch table to target transaction monthly table, which promises the best performance. Now, let’s see how do we convert switch table to non partition table 12345678910111213141516171819202122232425262728293031use business_dbgocreate procedure dbo.switchTrans_nonpartitionasbeginbegin trydeclare @filegroup varchar(100),@month_key int,@sSQL varchar(max),@sParam varchar(100),@processFY varchar(6),@sp_msg varchar(100)select top 1 @month_key=month_key from switch_transset @sSQL='select @fyOUt=fiscalYear from business_db.dbo.dim_calendar wherewhere month_key='+convert(char(6),@month_key)+''set @sParam='@fyOut varchar(10) output'exec sp_executeSQL @sSQL, @sParam, @fyOut=processFY outputselect @filegroup='BUSINESS_DB_FG_TRANS_'+@processFYset @sSQL='create clustered index PK_SWITCH_TRANS ON business_db.dbo.switch_trans(month_key,trans_id,trans_dt)with drop_existing on'+@filegroupexec sp_executeSQL @sSQLreturn 0end trybegin catch--add some tray catch blog statementreturn -1end next step is quite essential which is set up check constraints for non partitioned switch table because by partition switch syntax the database engine does partition switching from one source partition number to one target partition number, so check constraint just like a simulation of partition number to let database engine treats it as one partition. 123456789101112131415161718192021222324252627282930313233343536373839use business_dbgocreate procedure switch_trans_constraintas beginbegin trydeclare @month_key int,@lrange int,@sSQL varchar(max),@sp_msg varchar(100)select top 1 @month_key=month_key from dbo.switch_transif @month_key%100&gt;1set @lrange=@month_key-1elseset @lrange=@month_key-89if OBJECT_ID('ckMinMonth_key','C') is not nullafter table business_db.dbo.switch_trans drop constraint ckMinMonth_keyif OBJECT_ID('ckMaxMonth_key','C') is not nullafter table business_db.dbo.switch_trans drop constraint ckMaxMonth_keyset @sSQL='after table business_db.dbo.switch_transwith check add constraint ckMaxMonth_keycheck (month_key is not null and month_key&gt;'+convert(varchar(6),''+@lrange+'')+');'+'after table business_db.dbo.switch_transwith check add constraint ckMaxMonth_keycheck (month_key is not null and month_key&lt;='+convert(varchar(6),''+@month_key+'')+');'exec sp_executeSQL @sSQL;return 0end trybegin catch--add some catch block statementreturn -1end catchend next, we need to rebuild columnstore index for switch table in order to keep the index structure as the same as target transaction table, finally switch partition to target table. You can see we convert switch table from partition table to non partition table instead of partition merging, which only takes 15 minutes instead of 3 hours so that process performance is highly improved.","link":"/2021/01/01/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-3/"},{"title":"Setup SPARK Environment Locally for Big Data Development","text":"Spark has been reported to be one of the most valuable tech skills to learn by data professionals and demand for Spark and Big Data skill has exploded in recent years. As one of the latest technologies in the big data space, Spark is quickly becoming one of the most powerful Big Data tools for data processing and machine learning, its ability to run programs up to 100x faster than Hadoop MapReduce in memory. Unlike single program installation, Spark environment setup is not that straight forward but needs a series of installations and configurations with sequence order requirement and dependencies from operating system to software. The main purpose of Spark is dealing with Big Data which means data is too big to allocate into a single server but multiple servers to comprise cluster. Because it’s cluster environment so that Spark is naturally installed in Linux server, therefore, from the operating system perspective, Linux is the only option, which decide the entire process is going to heavy rely on CLI instead of GUI so that the basic Linux CLI command is required before rolling up your sleeves and get your hand dirty. But if you don’t want to setup environment by yourself, there is also a good solution provided by Databrick. Databrick is a company started by the creator of Spark that provides clusters that run on the top of AWS and adds a convience of having a notebook system already set up and the ability to quickly add files either from storage like Amazon S3 or from your local computer. I has a free community version that supports a 6 GB cluster. Because it is a web-based service so that you can easily follow the wizard on Databrick web portal to finish setup. We don’t show that here because it’s out of our scope. Before we begin, something is very necessary to emphasis here. Spark is written by Scala, Scala is written by Java, so we have to follow the sequence to make sure Java installed then followed by Scala. Now let’s start it. JDK (Java) InstallationUse below command check if Java 8 or above is in your system, if yes, then we can go directly install Scala. 1Java --version Openjdk version has to be 1.8.0 or higher. If JDK not installed, we have to install Java first. From Oracle official website download the openJDK source file jdk-8u161-linux-x64.tar.gz to put it in Home directory, create a new directory at /usr/local/java, use tar command to uncompress java files into it. 123cd /usr/localmkdir javacd java 1tar -zxv -f ~/jdk-8u161-linux-x64.tar.gz -C ./ Edit /etc/profile file, add JDK into PATH environmental variable at the end of file 1234JAVA_HOME=/usr/local/java/jdk1.8.0_161CLASSPATH=$JAVA_HOME/lib/PATH=$PATH:$JAVA_HOME/binexport PATH JAVA_HOME CLASSPATH At last, execute below command to make configuration take effect 1source /etc/profile Scala InstallationIf you use Ubuntu then Scala installation is very easy becauseapt repository and its mirror site contains Scala so that you can install it by below command 12sudo apt-get updatesudo apt-get install -y scala In this case, we use Centos7, we need to download rpm package from Scala website to install it which is very similar with JDK. We have 2 options to do, once is local install; another one is online install but it needs you have a stable network connection. Local Install ScalaFirst thing we need to go to https://www.scala-lang.org/download/, download scala-2.13.5.rpm from Archive 1wget https://downloads.lightbend.com/scala/2.13.5/scala-2.13.5.rpm To install the package, use the yum localinstall command followed by the path to the package name: 1sudo yum localinstall scala-2.13.5.rpm or use rpm command 1sudo rpm -ivh scala-2.13.5.rpm The -v option tells rpm to show verbose output and -h to show the hash marked progress bar. If the package depends on other packages that are not installed on the system, rpm will display a list of all missing dependencies. You will have to download and install all dependencies manually. Instead of downloading and the installing the RPM package locally, you can use the URL to RPM package to solve dependency problem easily. Online Install ScalaIssue yum command with Scala RPM package URL to install online 1sudo yum localinstall https://downloads.lightbend.com/scala/2.13.5/scala-2.13.5.rpm or use rpm command 1sudo rpm -ivh https://downloads.lightbend.com/scala/2.13.5/scala-2.13.5.rpm SPARK InstallationThere is tricky here that the Apache Spark now has two major versions, one is 2.4.7 which is compatible with python 3.5 or lower version if you use pyspark to write your applications; another one is 3.0.2 which is compatible with python 3.6 or higher version, so make sure your python version before you download it. In our case, we use python 3.8 so we should choose Spark 3.0.2. First, let’s go to Spark official download page to download Spark 3.0.2 https://spark.apache.org/downloads.html, install file is a tgz package. We also need switch use to root for the sake of permission by command 12su - rootcd ~ 1wget ftp://mirror.csclub.uwaterloo.ca/apache/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz Next, we uncompress file to /usr/local/spark 123cd /usr/localmkdir sparkcd spark 1tar -zxv -f ~/spark-3.0.2-bin-hadoop2.7.tgz -C. Then, we need to install a small package to use pyspark – py4j 1yum install -y py4j Last, we need to edit environment variable to add Spark. use vim to edit /etc/profile 123export SPARK_HOME=/usr/local/sparkexport PATH=$PATH:$SPARK_HOME/binexport PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH Now, let’s launch spark-shell by issue command 1spark-shell Config Spark for userWe still need to do some configurations for user or group other than root because that applies to common situation, first switch back to user 1su - user_name Edit user environment variable to add Spark to it, vim open file ~/.bash_profile 1vim ~/.bash_profile add Spark directory to PAH 123export SPARK_HOME=/usr/local/sparkexport PATH=$PATH:$SPARK_HOME/binexport PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH Now, let’s launch pyspark by command pyspark You might come across some permission issue when you use pyspark shell or in Jupyter notebook, in this case, issue below command to grant permission 12sudo chmod 777 /usr/local/spark/pythonsudo chmod 777 /usr/local/spark/python/pyspark Jupyter Notebook and Jupyter Lab configure for PysparkJupyter notebook provides us a very user-friendly interactive python development environment, there is no exceptional for pyspark, so now let’s do some setup to let pyspark can be run on Jupyter 123export PYSPARK_DRIVER_PYTHON=&quot;jupyter&quot;export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook&quot;export PYSPARK_PYTHON=python3 If you use virtual environment on python3 then value of PYSPARK_PYTHON should be python instead of python3. Now, we can launch Jupyter notebook to start use spark and pyspark 1jupyter notebook or Jupyter Lab 1jupyter-lab","link":"/2021/02/27/Setup-SPARK-Environment-Locally-for-Big-Data-Development/"},{"title":"Python Development Bootstrap on Managing Environment and Packages","text":"If you have basic python experience and knowledge and willing to develop a real python application or product, I think that post is right for you. Let’s recap how do we start to use Python, we either go to official site to download python, install it on system wise then jump start to write “Hello, world!” or install Anaconda to get a long list of pre-installed libraries and then do the same thing. I would say that is totally ok by using python globally or system wise at the beginning stage, but as we accumulate enough python scripts and do the real python project, we may found something really painful during our development Version confusion: different python versions reside together as well as pip versions Package redundancy: too many libraries in one place, some of them you use only for some practice and never use them again Package version conflict: that is the most severe one and headache, imagine your early development based on python2, you move to python3 afterwards, but you found your early applications break after you upgrade package version on which your python2 applications dependent, because those libraries are not backwards compatible even all your development based on python3, you still stuck on the situation that one library of your new development needs high version sub-dependency package but your early development rely on the same sub-dependency package but with lower version when you work with team, you need to pass your work to other teammate to test, your application breaks due to there no unique environment and packages between your machines Therefore, for real python production development, the first step should set up a proper virtual environment to be able to compatibility and collaboration before jump to coding. Thanks to community contributors, we have multiple choices to meet our different requirements and needs, there are 3 ways to do that, they are Python early official solution: venv (virtualenv) and pip Python latest official solution: pipenv Conda environment and package management tool now let’s see how do we do the configuration. Python early official solution: venv (virtualenv) and pipvenv formerly known as virtualenv is official build-in packages for managing virtual environment. It separates a special python environment with user defined python interpreter and packages in a separated folder where user project is located. When virtual environment is started, it will temporarily modify system environment variables, in this way, no matter what modules or libraries you install, they are all in project folder, after existing virtual environment, the system environment variables restored, and the operations you do in project folder won’t affect any python system wise setting and working environment. Install evenYou can use pip command to install venv if you don’t find it in your system. We can use pip list | grep pip command to check availability 1pip install venv Create venv virtual environment for projectFirstly, make a new project folder and enter new folder: 12mkdir /home/user1/tmp/cd /home/user1/tmp/ Secondly, execute venv vetest1 to create virtual environment in current folder 1python3 -m venv vetest1 *if you want to create python2 environment, the command line should be python2 -m venv vetest1 Launch and exist virtual environmentFor launch virtual environment, use source &lt;directory&gt;/activate command to start: 1source /home/user1/tmp/vetest1/bin/activate you can see your prompt head title start with (vestest1) which means your virtual environment is now started. Now let’s check the python version in this environment by issue command python --version alternatively, enter the directory and check python binary file next, let’s pip download some package e.g. matplotlib to check if would be resided in virtual environment folder 12python -m pip install matplotlibls -l /home/user1/tmp/vetest1/lib64/python3.8/site-packages/ *you can directly use python or pip after you are in virtual environment you can see the visualization library matplotlib is installed in virtual environment directory other than python system package folder. Exist virtual environment by issue command deactivate, you will see the command line title never start with virtual environment name but restore normal display. Let your IDE recognizes your new virtual environment – PyCharm and VS CodeWe can’t only rely on interactive console to do development so that let our favorite IDE recognize virtual environment is quite important. Undoubtedly, PyCharm is the best python IDE, VS Code is becoming a popular IDE for all language by ton of plugins. Now let’s see how do we configure PyCharm and VS Code. For PyCharm, launch it by creating new project, on prompt window assign a project directory such as /home/user1/pydev1, then choose Previously configured interpreter radio button, then choose our new created virtual environment bin folder python file as interpreter. now you can see the python interpreter is pointing to our virtual environment with python 3.8. For VS Code, launch it then click left part of status bar, from prompt window to find our new created virtual environment bin folder python file as interpreter you can see now python interpreter is our virtual environment vetest1. Create python2 virtual environmentFor senior python user, you might still need python2 environment, very similar with python3 but change command from python3 venv -m vetest2 to python -m even vetest2 1python -m virtualenv vetest2 Python latest official solution: pipenvEvery new technology emerging and booming are driven by reality demand, that still applies to python, such as why pipenv comes up with new features after pip and virtualenv. “What is rational is actual and what is actual is rational” —- Hegel，Georg Wilhelm Friedrich pipenv has a lot of advantages versus its parent generation product virtualenv or venv, simply say, it combine environment and package management together, but actually much more than that, for that part, I will reference Alexander VanTol post in Real Python. Alexander is an avid Pythonista who spends his time on various creative projects involving programming, music, and creative writing. Problems that Pipenv solveso understand the benefits of Pipenv, it’s important to walk through the current methods for packaging and dependency management in Python. Let’s start with a typical situation of handling third-party packages. We’ll then build our way towards deploying a complete Python application. Dependency Management with requirements.txtImagine you’re working on a Python project that uses a third-party package like flask. You’ll need to specify that requirement so that other developers and automated systems can run your application. So you decide to include the flask dependency in a requirements.txt file: 1flask Great, everything works fine locally, and after hacking away on your app for a while, you decide to move it to production. Here’s where things get a little scary… The above requirements.txt file doesn’t specify which version of flask to use. In this case, pip install -r requirements.txt will install the latest version by default. This is okay unless there are interface or behavior changes in the newest version that break our application. For the sake of this example, let’s say that a new version of flask got released. However, it isn’t backward compatible with the version you used during development. Now, let’s say you deploy your application to production and do a pip install -r requirements.txt. Pip gets the latest, not-backward-compatible version of flask, and just like that, your application breaks… in production. “But hey, it worked on my machine!”—I’ve been there myself, and it’s not a great feeling. At this point, you know that the version of flask you used during development worked fine. So, to fix things, you try to be a little more specific in your requirements.txt. You add a version specifier to the flask dependency. This is also called pinning a dependency: 1flask==0.12.1 Pinning the flask dependency to a specific version ensures that a pip install -r requirements.txt sets up the exact version of flask you used during development. But does it really? Keep in mind that flask itself has dependencies as well (which pip installs automatically). However, flask itself doesn’t specify exact versions for its dependencies. For example, it allows any version of Werkzeug&gt;=0.14. Again, for the sake of this example, let’s say a new version of Werkzeug got released, but it introduces a show-stopper bug to your application. When you do pip install -r requirements.txt in production this time, you will get flask==0.12.1 since you’ve pinned that requirement. However, unfortunately, you’ll get the latest, buggy version of Werkzeug. Again, the product breaks in production. The real issue here is that the build isn’t deterministic. What I mean by that is that, given the same input (the requirements.txt file), pip doesn’t always produce the same environment. At the moment, you can’t easily replicate the exact environment you have on your development machine in production. The typical solution to this problem is to use pip freeze. This command allows you to get exact versions for all 3rd party libraries currently installed, including the sub-dependencies pip installed automatically. So you can freeze everything in development to ensure that you have the same environment in production. Executing pip freeze results in pinned dependencies you can add to a requirements.txt: 123456click==6.7Flask==0.12.1itsdangerous==0.24Jinja2==2.10MarkupSafe==1.0Werkzeug==0.14.1 With these pinned dependencies, you can ensure that the packages installed in your production environment match those in your development environment exactly, so your product doesn’t unexpectedly break. This “solution,” unfortunately, leads to a whole new set of problems. Now that you’ve specified the exact versions of every third-party package, you are responsible for keeping these versions up to date, even though they’re sub-dependencies of flask. What if there’s a security hole discovered in Werkzeug==0.14.1 that the package maintainers immediately patched in Werkzeug==0.14.2? You really need to update to Werkzeug==0.14.2 to avoid any security issues arising from the earlier, unpatched version of Werkzeug. First, you need to be aware that there’s an issue with the version you have. Then, you need to get the new version in your production environment before someone exploits the security hole. So, you have to change your requirements.txt manually to specify the new version Werkzeug==0.14.2. As you can see in this situation, the responsibility of staying up to date with necessary updates falls on you. The truth is that you really don’t care what version of Werkzeug gets installed as long as it doesn’t break your code. In fact, you probably want the latest version to ensure that you’re getting bug fixes, security patches, new features, more optimization, and so on. The real question is: “How do you allow for deterministic builds for your Python project without gaining the responsibility of updating versions of sub-dependencies?” Development of Projects with Different DependenciesLet’s switch gears a bit to talk about another common issue that arises when you’re working on multiple projects. Imagine that ProjectA needs django==1.9, but ProjectB needs django==1.10. By default, Python tries to store all your third-party packages in a system-wide location. This means that every time you want to switch between ProjectA and ProjectB, you have to make sure the right version of django is installed. This makes switching between projects painful because you have to uninstall and reinstall packages to meet the requirements for each project. The standard solution is to use a virtual environment that has its own Python executable and third-party package storage. That way, ProjectA and ProjectB are adequately separated. Now you can easily switch between projects since they’re not sharing the same package storage location. PackageA can have whatever version of django it needs in its own environment, and PackageB can have what it needs totally separate. A very common tool for this is virtualenv (or venv in Python 3). Pipenv has virtual environment management built in so that you have a single tool for your package management. Dependency ResolutionWhat do I mean by dependency resolution? Let’s say you’ve got a requirements.txt file that looks something like this: 12package_apackage_b Let’s say package_a has a sub-dependency package_c, and package_a requires a specific version of this package: package_c&gt;=1.0. In turn, package_b has the same sub-dependency but needs package_c&lt;=2.0. Ideally, when you try to install package_a and package_b, the installation tool would look at the requirements for package_c (being &gt;=1.0 and &lt;=2.0) and select a version that fulfills those requirements. You’d hope that the tool resolves the dependencies so that your program works in the end. This is what I mean by “dependency resolution.” Unfortunately, pip itself doesn’t have real dependency resolution at the moment, but there’s an open issue to support it. The way pip would handle the above scenario is as follows: It installs package_a and looks for a version of package_c that fulfills the first requirement (package_c&gt;=1.0). Pip then installs the latest version of package_c to fulfill that requirement. Let’s say the latest version of package_c is 3.1. This is where the trouble (potentially) starts. If the version of package_c selected by pip doesn’t fit future requirements (such as package_b needing package_c&lt;=2.0), the installation will fail. The “solution” to this problem is to specify the range required for the sub-dependency (package_c) in the requirements.txt file. That way, pip can resolve this conflict and install a package that meets those requirements: 123package_c&gt;=1.0,&lt;=2.0package_apackage_b Just like before though, you’re now concerning yourself directly with sub-dependencies (package_c). The issue with this is that if package_a changes their requirement without you knowing, the requirements you specified (package_c&gt;=1.0,&lt;=2.0) may no longer be acceptable, and installation may fail… again. The real problem is that once again, you’re responsible for staying up to date with requirements of sub-dependencies. Ideally, your installation tool would be smart enough to install packages that meet all the requirements without you explicitly specifying sub-dependency versions. Install PipenvBecause pipenv is official replacement of pip and venv, so we use pip installs pipenv after that we can forget its precedence. 1python3 -m pip install pipenv unlike venv, it will create project folder for us in we assigned folder, for pipenv, we need to create project folder by ourselves then enter it, in this case, our project folder is pydev2, after we create a virtual environment, pipenv created a environment folder for us in /home/user/.local/share/virtualenvs, sounds like wired, but it’s good setting, let’s see later on. 123cd ~mkdir pydev2cd pydev2 we’d better create a soft link for pipenv in /user/bin/ so that we don’t need to type python3 -m every time. 1ln -s /usr/local/python3/bin/pipenv /usr/bin/pipenv Create pipenv virtual environment for projectThere are 2 ways to do that, first one is use pipenv shell command to launch environment shell; the other way is install a package by use pipenv 1pipenv --python 3.8 || pipenv shell From above return message, we know our virtual environment has been created with location: /home/user/.local/share/virtualenvs/pydev2-ro0T2QJC(suffix is system generated randomly), you might be confused there why pipenv create a separated folder rather than using user created project folder? I would say that a good feature of pipenv over venv, Pipenv separates your development environment and python environment, which makes you more focus on your project stuff other than those python supporting files, actually these two folders are associated with each other, every time you install a new package, package file will be on virtual environment folder but the relevant configuration value will be written into Pipfile which is a replacement for requirement.txt file in old age. Launch pipenv virtual environment and install a new packageWe use pipenv shell command to start to use new virtual environment: 12cd /home/user/pydev2pipenv shell our new virtual environment is launched successfully! We can now check our project and environment information 12pipenv --wherepipenv --venv Now, let’s install another data visualization package seaborn in pydev2. 1pipenv install seaborn like previous chapter mentioned, when package has multiple dependencies and sub-dependencies, Pipenv will generate Pipfile.lock automatically which is a JSON file, if you familiar with JavaScript or NodeJS, you might find it is very similar with packages.json file after you install a module by using npm install express --save you can see seaborn and its dependent packages are all installed into virtual environment folder. To exist Pipenv, use deactivate command in shell. IDE preparation for virtual environment – PyCharm and VS CodePyCharm provides three virtual environment options virtualenv or venv pipenv conda similar with previous chapter mentioned on venv, launch PyCharm, open our project folder pydev2, PyCharm is smart enough to be able to recognize pipenv environment project, all we need to do is just click ok on prompt window, then every thing done. go to PyCharm interpreter setting page, you can see all libraries reside under the virtual environment VS Code is also very similar with venv setting, open project folder /home/user/pydev2 then click left part of status bar (at bottom), choose “Python 3.8.3 64-bit (‘pydev2’: pipenv)” from prompt window Conda environment and package management toolFrom personal perspective, my favorite environment and package management tool is conda, by leveraging powerful CLI tool and makes your work never become such easy and clear. With conda, you can create, export, list, remover, and update environments that have different versions of Python and/or packages installed in them. Switching or moving between environments is called activating the environment. You can also share an environment file. Create a conda virtual environment with CLIIn Linux like centos or ubuntu, decompress miniconda package to our home directory, now we are good to go to create conda environment 1conda create --name pydev replace ‘pydev’ with any name you like, this command will create an empty environment without Python interpreter and is in /home/user/minicoda/env path. By default, environments are installed into the envs directory in your conda directory. See Specifying a location for an environment or run conda create --help for information on specifying a different path. To create an environment with a specific version of Python: 1conda create -n pydev python=3.6 To create an environment with a specific package: 1conda create -n pydev scipy OR 12conda create -n pydev pythonconda install -n pydev scipy conda will install the latest scipy version, for Python interpreter version will be align with system To create an environment with a specific version of a package: 1conda create -n myenv scipy=0.15.0 OR: 12conda create -n myenv pythonconda install -n myenv scipy=0.15.0 To create an environment with a specific version of Python and multiple packages: 1conda create -n pydev python=3.6 scipy=0.15.0 seaborn babel matplotlib Tip: Install all the programs that you want in this environment at the same time. Installing 1 program at a time can lead to dependency conflicts. Launch and exit conda environmentTo activate conda environment: 1conda activate pydev To exit conda environment: 1conda deactivate conda activate and conda deactivate only work on conda 4.6 and later versions. For conda versions prior to 4.6, run: Windows: activate or deactivate Linux and macOS: source activate or source deactivate View a list of your environmentsTo see a list of all of your environments, in your terminal window or an Anaconda Prompt, run: 1conda info --envs OR 1conda env list A list similar to the following is displayed: asterisk tells current activate environment View a list of the packages in an environmentTo see a list of all packages installed in a specific environment: If the environment is not activated, in your terminal window or an Anaconda Prompt, run: 1conda list -n myenv If the environment is activated, in your terminal window or an Anaconda Prompt, run: 1conda list To see if a specific package is installed in an environment, in your terminal window or an Anaconda Prompt, run: 1conda list -n myenv scipy Build identical conda environmentsYou can use explicit specification files to build an identical conda environment on the same operating system platform, either on the same machine or on a different machine. Use the terminal or an Anaconda Prompt for the following steps: Run conda list --explicit to produce a spec list such as: To create this spec list as a file in the current working directory, run: 1conda list -e &gt; spec-file.txt tip: You can use spec-file.txt as the filename or replace it with a filename of your choice. An explicit spec file is not usually cross platform, and therefore has a comment at the top such as # platform: osx-64 showing the platform where it was created. This platform is the one where this spec file is known to work. On other platforms, the packages specified might not be available or dependencies might be missing for some of the key packages already in the spec. To use the spec file to create an identical environment on the same machine or another machine: 1conda create --name myenv --file spec-file.txt To use the spec file to install its listed packages into an existing environment: 1conda install --name myenv --file spec-file.txt Conda does not check architecture or dependencies when installing from a spec file. To ensure that the packages work correctly, make sure that the file was created from a working environment, and use it on the same architecture, operating system, and platform, such as linux-64 or osx-64. Clone an environmentUse the terminal or an Anaconda Prompt for the following steps: You can make an exact copy of an environment by creating a clone of it: 1conda create --name myclone --clone myenv To verify that the copy was made: 1conda info --envs In the environments list that displays, you should see both the source environment and the new copy. Share an environmentYou may want to share your environment with someone else—for example, so they can re-create a test that you have done. To allow them to quickly reproduce your environment, with all of its packages and versions, give them a copy of your environment.yml file. Exporting the environment.yml file If you already have an environment.yml file in your current directory, it will be overwritten during this task. Activate the environment to export: conda activate pydev2 Export your active environment to a new file: 1conda env export &gt; environment.yml This file handles both the environment’s pip packages and conda packages. Email or copy the exported environment.yml file to the other person. Restore an environmentConda keeps a history of all the changes made to your environment, so you can easily “roll back” to a previous version. To list the history of each change to the current environment: conda list --revisions To restore environment to a previous revision: conda install --revision=REVNUM or conda install --rev REVNUM. Replace REVNUM with the revision number. Example: If you want to restore your environment to revision 8, run conda install --rev 8. Remove an environmentTo remove an environment, in your terminal window or an Anaconda Prompt, run: 12conda remove --name pydev --allconda remove --name pydev2 --all To verify that the environment was removed, in your terminal window or an Anaconda Prompt, run: 1conda info --envs The environments list that displays should not show the removed environment.","link":"/2021/01/10/Python-Development-Bootstrap-on-Managing-Environment-and-Packages/"},{"title":"Sandbox solution for BI reporting and business analytics","text":"At beginning, I’d like to share a story from my client and business partner. One day, my team worked on a big marketing project, data from all kinds of source like spreadsheet, csv, mainframe and SQL Server, we had to do cross reference all those data to generate analysis reports but the headache thing was they were isolated and no way to put them into a single unique environment to run one time query then return the results so we could only open multiple windows to compare them by eyeballs. During the project, we often composed some complex queries then ran for a long time to return the result dataset, those datasets were quite important for future further analysis and share with the whole team, but the another panic thing was we could not save those dataset into database due to insufficient access so what we did was copy and paste everything in excel spreadsheet, after for a while, we found number of excel file explode and hard to find the report among those huge files, we feed up with the tedious work and decided created a bunch of views in database but that was also not controlled by us but infrastructure team, all we could do was submit the request then followed infrastructure team’s schedule and waited for month end deployment, no matter how urgent those reports would be. That is the story, I think if you had ever experienced that, that solution might be right for you. You might get the common points from above story, it’s inefficient and even painful if you can only leverage data from data warehouse tables, that is reason why the sandbox database comes up which is a brand new data play zone on production with ability of pipeline to bring multiple sources of data based on major data warehouse you are using. In a brief mark, sandbox is aiming to build a homogeneous solution for heterogenous environment. How to build up and what is the foundation of sandbox database, now let’s take a look into it. The Major data source is so called big db, but it’s data warehouse so that user was only assigned read access which means you can do nothing but only select and query data. Now we carved out two new databases - sandbox and control db. Sandbox database brief introductionsandbox is the new data loading zone for slicing and dicing data in production, like you own backyard playground, you can do almost anything you want to do with it, so user will be granted both read and write permission. control_db that is control console station to provide access gateway for sandbox and also monitor and logging the users’ behaviors, just like a guardian to promise the safety and healthy for sandbox. Let’s take a closer look at these two databases. Sandbox, in general, user can perform 3 kinds of actions, DDL, DML as well as batch job execution based on stored procedure. With DDL command which is database define language, user can create table/view/stored procedure, alter them, even drop them. With DML command which is database manipulate language, user can insert/update/delete data from existing table and also import data from other source. The batch operation is fit for user with programming background and write multiple queries with logical sequence into it like conditional and looping function by using T-SQL scripting language in terms of stored procedure. What things should be considered beforeA very critical point for sandbox database is object ownership, which means you can only create and deal with your own database objects plus read others’ object. Image you data or analytical work were deleted by other accidently, how would you feel at the moment, so the restriction must be setup along with the sandbox database creation to promise the user data safety. Another important thing need to be considered before is user data volume control. Unlike data warehouse, data volume increasing is followed trend and stable for each year, but sandbox database size is unpredictable and totally depends on user personal behavior, in order to prevent non-critical user data dominants your server hard disk, it’s very necessary to set quota for each users. When reach to the limit then User won’t be allowed to create table User also won’t be allowed insert new data into existing table until manually clean data and release space. Creating sandbox database walk throughDefine and create a new database12345use master;goif DB_ID('sandbox') is not nulldrop database sandboxgo in real production, you might need to create a new file group and bunch of file under it for better management. 12345678910create database sandboxon primary(name=N'sandbox', filename=N'G:\\sqldata\\sandbox.mdf', size=1048kB, maxsize=5124KB, filegrowth=512KB),filegroup [yourTeam_sandbox](name=N'yourTeam_sandbox_FG1',filename=N'G:\\sqldata\\yourTeam_sandbox1.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB),(name=N'yourTeam_sandbox_FG2',filename=N'G:\\sqldata\\yourTeam_sandbox2.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB),(name=N'yourTeam_sandbox_FG3',filename=N'G:\\sqldata\\yourTeam_sandbox3.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB)log on(name=N'yourTeam_sandbox_log',filename=N'H:\\sqllog\\yourTeam_sandbox_log.ldf',size=1048KB, maxsize=5124KB, filegrowth=512KB)go make sure your sandbox and control console database have the same database ownership user 12345678use master;goalter authorization on database::sandboxto sagoalter authorization on database::control_dbto sago make sure your new created file group to be the default file group so that coming data is going to be allocated there 12345use sandbox;goif not exists (select name from sys.filegroups where is_default=1 and name=N'sandbox')alter database sandbox modify filegroup [yourTeam_sandbox] defaultgo Grant user permissionsFirst, you need to provide the basic read access to your business user 1234567use sandbox;goif exists (select * from sys.database_principals where name=N'yourDomain\\yourUserGroup')drop user [yourDomain\\yourUserGroup]elsecreate user [yourDomain\\yourUserGroup] for login [yourDomain\\yourUserGroup] with default_schema=dboexec sp_addrolemember 'db_datareader','yourDomain\\yourUserGroup'; then assign to them DDL and DML permissions 1234567891011121314151617181920use sandbox;gogrant alter any schema to [yourDomain\\yourUserGroup];gogrant create procedure to [yourDomain\\yourUserGroup];gogrant create table to [yourDomain\\yourUserGroup];gogrant create view to [yourDomain\\yourUserGroup];gogrant delete to [yourDomain\\yourUserGroup];gogrant insert to [yourDomain\\yourUserGroup];gogrant select to [yourDomain\\yourUserGroup];gogrant update to [yourDomain\\yourUserGroup];gogrant execute to [yourDomain\\yourUserGroup];go Create database level trigger on DDL query control for rules and restrictions encourage give table name starting with id number which is unique identifier for employee in your company like nameInitial2_tableName must be eligible user, identity check by control database no special characters are allowed in table name like !@#$%^&amp;- no over quota allowed not allowed user drop or delete others’ object and data let’s connect above constraints with workflow chart to be able to see the big picture First of all, we create the database trigger for enforcing object creation rules 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163create trigger ddltrg_create_tableon databasewith execute as 'dbo'for create_tableasbegin try set noncount on; declare @eventdata xml declare @orig_object_name varchar(100) declare @orig_user_name varchar(25) declare @alt_object_name varchar(100) declare @alt_user_name varchar(25) declare @valid_chars varchar(50) declare @create_status char(1) declare @create_err_msg varchar(250) declare @rename_required char(1) declare @trigger_sql varchar(max) declare @schema_name varchar(25) declare @trigger_insert varchar(max) declare @guid varchar(32) declare @overQuota_user table (userId varchar(25) null) set @guid='250' set @valid_chars = '%[^a-zA-Z0-9_]%' set @eventdata = EVENTDATA() set @orig_object_namne = @eventdata.value('(/EVENT_INSTANCE/objectName)[1]','nvarchar(100)') set @orig_user_name = upper(@eventdata.value('(/EVENT/INSTANCE/LoginName)[1]','nvarchar(25)')) set @create_status='Y' set @create_err_msg='' set @alt_user_name = right(@orig_user_name,len(@orig_user_name)-charindex('\\\\',@orig_user_name)) select @schema_name = SCHEMA_NAME(SCHEMA_ID) from sys.tables where name = @orig_object_name if (@alt_user_name = 'yourServiceAccout') return if substring(@orig_object_name,1,len(@alt_user_name)) = @orig_object_name begin set @alt_object_name = @orig_object_name set @rename_required = 'N' end else begin set @alt_object_name = @orig_object_name set @rename_required = 'Y' end /*check the user eligibility*/ if not exists (select * from control_db.dbo.Audit_Sandbox_User where User_ID = @alt_user_name) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' does not have privilege to use the sandbox database' end insert into @overQuota_user select user_id from control_db.dbo.Audit_Sandbox_User where sandbox_limit - current_usage &lt; 0 /*user control for over limit quota*/ if @alt_user_name in (select userId from @overQuota_user) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' exceeded designed space quota, ' + @alt_user_name + ' is not able to ' + N'create new table. Please delete data not needed' + N'to free up space so usage is less than quota 1024MB and rerun query again' end /*check object name eligibility*/ if (patindex(@valid_chars,@orig_object_name) &gt; 0) begin set @create_status = 'N' set @create_err_msg = 'Invalid characters in table name' end /*check object existency*/ if exists(select * from sys.objects where object_id=OBJECT_ID(@alt_object_name) and type in (N'U')) begin if @rename_required = 'Y' begin set @create_status = 'N' set @create_err_msg = 'Table: ' + @alt_object_name + ' already exists' end end /*check schema name eligibility*/ if @schema_name &lt;&gt; 'dbo' begin set @create_status = 'N' set @create_err_msg = 'Table: ' + @orig_object_name + ' not created. Schema is not specified: dbo.&lt;table name&gt;!' end if (@create_status = 'N') begin rollback print @create_err_msg end /*log sandbox event into tracking table in control_db*/ insert control_db.dbo.Audit_Sandbox_Event_Tracking (Event_Type, Event_Time, Event_User, Event_Database, Event_Object_Name ,Event_Object_Type, Event_SQK, Audit_Created_Status, Audit_Error_Message) value (@eventdata.value('(/EVENT_INSTANCE/EventType)[1]','nvarchar(50)'), @eventdata.value('(/EVENT_INSTANCE/PostTime)[1]','datetime'), @Orig_user_name, @eventdata.value('(EVENT_INSTANCE/DatabaseName)[1]','nvarchar(25)'), @orig_object_name, @eventdata.value('(/EVENT_INSTANCE/ObjectType)[1]','nvarchar(25)'), @eventdata.value('(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]','nvarchar(max)'), case when @create_status = 'Y' then 'success' else 'failed' end, @create_err_msg ) if (@create_status = 'N') return /*rename object to add user id prefix*/ begin exec sp_rename @orig_object_name, @alt_object_name print 'Table name is renamed with prefix of your user id' end /*creaet DML trigger on table level*/ set @trigger_sql = 'create trigger DML trig_'+@alt_object_name+ 'ON ' + @alt_object_name + '' set @trigger_sql = @trigger_sql + 'for delete, update, insert' set @trigger_sql = @trigger_sql + 'as' set @trigger_sql = @trigger_sql + 'begin ' set @trigger_sql = @trigger_sql + ' set nocount on;' set @trigger_sql = @trigger_sql + ' declare @user varchar(25)' set @trigger_sql = @trigger_sql + ' select @user = right(SUSER_NAME(),LEN(SUSER_NAME())' set @trigger_sql = @trigger_sql + ' - CHARINDEX(''\\\\'',SUSER_NAME()))' set @trigger_sql = @trigger_sql + ' IF @user &lt;&gt; '''+@alt_user_name+''' ' set @trigger_sql = @trigger_sql + ' begin' set @trigger_sql = @trigger_sql + ' rollback' set @trigger_sql = @trigger_sql + ' print ''User: ''+@user+'' does not have the privilege to perform a DML operation on table' +@alt_object_name + ''' ' set @trigger_sql = @trigger_sql + ' end' set @trigger_sql = @trigger_sql + ' end' exec (@trigger_sql) ; /*create over quota DML trigger on table level*/ create table #overQuota (userId varchar(25) Null) ; set @trigger_sql = 'create trigger DML trig_'+@alt_object_name+ '_insert ON ' + @alt_object_name + '' set @trigger_sql = @trigger_sql + 'after insert' set @trigger_sql = @trigger_sql + 'as' set @trigger_sql = @trigger_sql + 'begin ' set @trigger_sql = @trigger_sql + ' set nocount on;' set @trigger_sql = @trigger_sql + ' declare @user varchar(25)' set @trigger_sql = @trigger_sql + ' select @user = right(SUSER_NAME(),LEN(SUSER_NAME()) - CHARINDEX(''\\\\'',SUSER_NAME()))' set @trigger_sql = @trigger_sql + ' insert into #overQuota' set @trigger_sql = @trigger_sql + ' select user_id from control_db.dbo.Audit_Sandbox_User where sandbox_limit-current_usage&lt;0;' set @trigger_sql = @trigger_sql + ' IF @user in (select userId from #overQuota)' set @trigger_sql = @trigger_sql + ' begin' set @trigger_sql = @trigger_sql + ' rollback' set @trigger_sql = @trigger_sql + ' print ''User: ''+@user+'' is over quota on usage so it can not perform a INSERT operation on table' +@alt_object_name + ', please delete data that is not needed to free up space then rerun the query'' ' set @trigger_sql = @trigger_sql + ' end' set @trigger_sql = @trigger_sql + ' end' exec (@trigger_sql) ; drop table #overQuota ;end trybegin catch declare @err_msg varchar(900), @err_num int, @err_line int, @syserr varchar(900) select @err_msg = ERROR_MESSAGE(), @err_num = ERROR_NUMBER(), @err_line = ERROR_LINE() set @syserr = 'Ended in DDLTRIG_CREATE_TABLE with errors: Line= ' + convert(varchar(10), @err_line) + ', Error Num = ' + convert(varchar(10), @err_num) + ', Error Msg= ' + @err_msg /*save to log file or control_db table*/end catchgoenable trigger [ddltrg_create_table] on databasego One more run need to be applied is prevent user drop object which is not belong to that user. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051create trigger ddltrig_drop_tableon databasewith execute as 'dbo' for drop_tableas set nocount on; declare @eventdata xml declare @orig_object_name varchar(100) declare @orig_user_name varchar(25) declare @alt_user_name varchar(25) declare @create_status char(1) declare @create_err_msg varchar(250) set @eventdata = EVENTDATA() set @orig_object_name = @eventdata.value('(/EVENT_INSTANCE/ObjectName)[1]','nvarchar(100)') set @orig_user_name = upper(@eventdata.value('(/EVENT_INSTANCE/LoginName)[1]','nvarchar(25)')) set @creaet_status = 'Y' set @create_err_msg = '' set @alt_user_name = right(@orig_user_name, len(@orig_user_name) - charindex('\\\\', @orig_user_name)) if @alt_user_name &lt;&gt; 'domainName/yourServiceAccount' begin if not exists (select * from control_db.dbo.Audit_Sandbox_User where User_ID = @alt_user_name) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' does not have privilege to use the sandbox database' end if substring(@orig_object_name, 1, len(@alt_user_name)) &lt;&gt; @alt_user_name begin set @create_status = 'N' set @create_err_msg = 'user: ' + @alt_user_name + ' does not have privilege to drop table: ' + @orig_object_name endendif (@create_Status = 'N')begin rollback print @create_err_msgendinsert control_db.dbo.Audit_Sandbox_Event_Tracking (Event_Type, Event_Time, Event_User, Event_Database, Event_Object_Name ,Event_Object_Type, Event_SQK, Audit_Created_Status, Audit_Error_Message) value (@eventdata.value('(/EVENT_INSTANCE/EventType)[1]','nvarchar(50)'), @eventdata.value('(/EVENT_INSTANCE/PostTime)[1]','datetime'), @Orig_user_name, @eventdata.value('(EVENT_INSTANCE/DatabaseName)[1]','nvarchar(25)'), @orig_object_name, @eventdata.value('(/EVENT_INSTANCE/ObjectType)[1]','nvarchar(25)'), @eventdata.value('(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]','nvarchar(max)'), case when @create_status = 'Y' then 'success' else 'failed' end, @create_err_msg )go At last, one very important setting need to be in placed in case cause issue when [sa] user cross database reference data 1alter database sandbox set TRUSTWORTHY ON; about trustworthy for detail, you can see Microsoft official document on below link Trustworthy database property","link":"/2020/12/02/Sandbox-solution-for-BI-reporting-and-business-analytics/"},{"title":"SQL Server Table Partitioning in Large Scale Data Warehouse 1","text":"This is a series articles to demonstrate table partitioning technology and how to apply it in a large scale data warehouse to improve database performance and even benefit for maintenance operation based on Microsoft SQL Server. This series is composed by three parts The first one is a fundamental introduction on basis knowledge The second part is showcase the entire production workflow to apply table partitioning to large tables Finally, it’s going to be advanced topic like partition merge, split, conversion and performance optimization in terms of different business demands Table Partitioning The BasicsFor this part, I will reference Cathrine Wilhelmsen’s work, she is Microsoft Data Platform MVP, BimlHero Certified Expert, international speaker, author, blogger, and chronic volunteer. She loves data and coding, as well as teaching and sharing knowledge - oh, and sci-fi, chocolate, coffee, and cats :) I learnt a lot and got many ideas from her blogs especially for table partitioning technology, below is her blog address, hope it can help you as well:blush: Cathrine blog This post is part 1 of 2 in the series Table Partitioning in SQL Server There are many benefits of partitioning large tables. You can speed up loading and archiving of data, you can perform maintenance operations on individual partitions instead of the whole table, and you may be able to improve query performance. However, implementing table partitioning is not a trivial task and you need a good understanding of how it works to implement and use it correctly. Being a business intelligence and data warehouse developer, not a DBA, it took me a while to understand table partitioning. I had to read a lot, get plenty of hands-on experience and make some mistakes along the way. (The illustration to the left is my Table Partitioning Cheat Sheet.) One of my favorite ways to learn something is to figure out how to explain it to others, so I recently did a webinar about table partitioning. (Update in 2020: The webinar has now been archived. Please contact Pragmatic Works if you would like to watch it, as they are the owners and publishers.) I wanted to follow that up with focused blog posts that included answers to questions I received during the webinar. This post covers the basics of partitioned tables, partition columns, partition functions and partition schemes. What is Table Partitioning? Table partitioning is a way to divide a large table into smaller, more manageable parts without having to create separate tables for each part. Data in a partitioned table is physically stored in groups of rows called partitions and each partition can be accessed and maintained separately. Partitioning is not visible to end users, a partitioned table behaves like one logical table when queried. This example illustration is used throughout this blog post to explain basic concepts. The table contains data from every day in 2012, 2013, 2014 and 2015, and there is one partition per year. To simplify the example, only the first and last day in each year is shown. An alternative to partitioned tables (for those who don’t have Enterprise Edition) is to create separate tables for each group of rows, union the tables in a view and then query the view instead of the tables. This is called a partitioned view. (Partitioned views are not covered in this blog post.) What is a Partition Column? Data in a partitioned table is partitioned based on a single column, the partition column, often called the partition key. Only one column can be used as the partition column, but it is possible to use a computed column. In the example illustration the date column is used as the partition column. SQL Server places rows in the correct partition based on the values in the date column. All rows with dates before or in 2012 are placed in the first partition, all rows with dates in 2013 are placed in the second partition, all rows with dates in 2014 are placed in the third partition, and all rows with dates in 2015 or after are placed in the fourth partition. If the partition column value is NULL, the rows are placed in the first partition. It is important to select a partition column that is almost always used as a filter in queries. When the partition column is used as a filter in queries, SQL Server can access only the relevant partitions. This is called partition elimination and can greatly improve performance when querying large tables. What is a Partition Function? The partition function defines how to partition data based on the partition column. The partition function does not explicitly define the partitions and which rows are placed in each partition. Instead, the partition function specifies boundary values, the points between partitions. The total number of partitions is always the total number of boundary values + 1. In the example illustration there are three boundary values. The first boundary value is between 2012 and 2013, the second boundary value is between 2013 and 2014, and the third boundary value is between 2014 and 2015. The three boundary values create four partitions. (The first partition also includes all rows with dates before 2012 and the last partition also includes all rows after 2015, but the example is kept simple with only four years for now.) But what are the actual boundary values used in the example? How do you know which date values are the points between two years? Is it December 31st or January 1st? The answer is that it can actually be either December 31st or January 1st, it depends on whether you use a range left or a range right partition function. Range Left and Range RightPartition functions are created as either range left or range right to specify whether the boundary values belong to their left or right partitions: Range left means that the actual boundary value belongs to its left partition, it is the last value in the left partition. Range right means that the actual boundary value belongs to its right partition, it is the first value in the right partition. Left and right partitions make more sense if the table is rotated: → → Range Left and Range Right using DatesThe first boundary value is between 2012 and 2013. This can be created in two ways, either by specifying a range left partition function with December 31st as the boundary value, or as a range right partition function with January 1st as the boundary value: → → Partition functions are created as either range left or range right, it is not possible to combine both in the same partition function. In a range left partition function, all boundary values are upper boundaries, they are the last values in the partitions. If you partition by year, you use December 31st. If you partition by month, you use January 31st, February 28th / 29th, March 31st, April 30th and so on. In a range right partition function, all boundary values are lower boundaries, they are the first values in the partitions. If you partition by year, you use January 1st. If you partition by month, you use January 1st, February 1st, March 1st, April 1st and so on: → Range Left and Range Right using the Wrong DatesIf the wrong dates are used as boundary values, the partitions incorrectly span two time periods: → What is a Partition Scheme? The partition scheme maps the logical partitions to physical filegroups. It is possible to map each partition to its own filegroup or all partitions to one filegroup. A filegroup contains one or more data files that can be spread on one or more disks. Filegroups can be set to read-only, and filegroups can be backed up and restored individually. There are many benefits of mapping each partition to its own filegroup. Less frequently accessed data can be placed on slower disks and more frequently accessed data can be placed on faster disks. Historical, unchanging data can be set to read-only and then be excluded from regular backups. If data needs to be restored it is possible to restore the partitions with the most critical data first. How do I create a Partitioned Table?The following script (for SQL Server 2012 and higher) first creates a numbers table function created by Itzik Ben-Gan that is used to insert test data. The script then creates a partition function, a partition scheme and a partitioned table. (It is important to notice that this script is meant to demonstrate the basic concepts of table partitioning, it does not create any indexes or constraints and it maps all partitions to the [PRIMARY] filegroup. This script is not meant to be used in a real-world project.) Finally it inserts test data and shows information about the partitioned table. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/* – ------------------------------------------------ – Create helper function GetNums by Itzik Ben-Gan – https://www.itprotoday.com/sql-server/virtual-auxiliary-table-numbers – GetNums is used to insert test data------------------------------------------------ – */ – Drop helper function if it already existsIF OBJECT_ID('GetNums') IS NOT NULL DROP FUNCTION GetNums;GO – Create helper functionCREATE FUNCTION GetNums(@n AS BIGINT) RETURNS TABLE AS RETURN WITH L0 AS(SELECT 1 AS c UNION ALL SELECT 1), L1 AS(SELECT 1 AS c FROM L0 AS A CROSS JOIN L0 AS B), L2 AS(SELECT 1 AS c FROM L1 AS A CROSS JOIN L1 AS B), L3 AS(SELECT 1 AS c FROM L2 AS A CROSS JOIN L2 AS B), L4 AS(SELECT 1 AS c FROM L3 AS A CROSS JOIN L3 AS B), L5 AS(SELECT 1 AS c FROM L4 AS A CROSS JOIN L4 AS B), Nums AS(SELECT ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) AS n FROM L5) SELECT TOP (@n) n FROM Nums ORDER BY n;GO/* – ---------------------------------------------------------- – Create example Partitioned Table (Heap) – The Partition Column is a DATE column – The Partition Function is RANGE RIGHT – The Partition Scheme maps all partitions to [PRIMARY]---------------------------------------------------------- – */ – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'Sales') DROP TABLE Sales;IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = N'psSales') DROP PARTITION SCHEME psSales;IF EXISTS (SELECT * FROM sys.partition_functions WHERE name = N'pfSales') DROP PARTITION FUNCTION pfSales; – Create the Partition Function CREATE PARTITION FUNCTION pfSales (DATE)AS RANGE RIGHT FOR VALUES ('2013-01-01', '2014-01-01', '2015-01-01'); – Create the Partition SchemeCREATE PARTITION SCHEME psSalesAS PARTITION pfSales ALL TO ([Primary]); – Create the Partitioned Table (Heap) on the Partition SchemeCREATE TABLE Sales ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO Sales(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – View Partitioned Table informationSELECTOBJECT_SCHEMA_NAME(pstats.object_id) AS SchemaName,OBJECT_NAME(pstats.object_id) AS TableName,ps.name AS PartitionSchemeName,ds.name AS PartitionFilegroupName,pf.name AS PartitionFunctionName,CASE pf.boundary_value_on_right WHEN 0 THEN 'Range Left' ELSE 'Range Right' END AS PartitionFunctionRange,CASE pf.boundary_value_on_right WHEN 0 THEN 'Upper Boundary' ELSE 'Lower Boundary' END AS PartitionBoundary,prv.value AS PartitionBoundaryValue,c.name AS PartitionKey,CASE WHEN pf.boundary_value_on_right = 0 THEN c.name + ' &gt; ' + CAST(ISNULL(LAG(prv.value) OVER(PARTITION BY pstats.object_id ORDER BY pstats.object_id, pstats.partition_number), 'Infinity') AS VARCHAR(100)) + ' and ' + c.name + ' &lt;= ' + CAST(ISNULL(prv.value, 'Infinity') AS VARCHAR(100)) ELSE c.name + ' &gt;= ' + CAST(ISNULL(prv.value, 'Infinity') AS VARCHAR(100)) + ' and ' + c.name + ' &lt; ' + CAST(ISNULL(LEAD(prv.value) OVER(PARTITION BY pstats.object_id ORDER BY pstats.object_id, pstats.partition_number), 'Infinity') AS VARCHAR(100)) END AS PartitionRange ,pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCount ,p.data_compression_desc AS DataCompressionFROM sys.dm_db_partition_stats AS pstatsINNER JOIN sys.partitions AS p ON pstats.partition_id = p.partition_idINNER JOIN sys.destination_data_spaces AS dds ON pstats.partition_number = dds.destination_idINNER JOIN sys.data_spaces AS ds ON dds.data_space_id = ds.data_space_idINNER JOIN sys.partition_schemes AS ps ON dds.partition_scheme_id = ps.data_space_idINNER JOIN sys.partition_functions AS pf ON ps.function_id = pf.function_idINNER JOIN sys.indexes AS i ON pstats.object_id = i.object_id AND pstats.index_id = i.index_id AND dds.partition_scheme_id = i.data_space_id AND i.type &lt;= 1 /* Heap or Clustered Index */INNER JOIN sys.index_columns AS ic ON i.index_id = ic.index_id AND i.object_id = ic.object_id AND ic.partition_ordinal &gt; 0INNER JOIN sys.columns AS c ON pstats.object_id = c.object_id AND ic.column_id = c.column_idLEFT JOIN sys.partition_range_values AS prv ON pf.function_id = prv.function_id AND pstats.partition_number = (CASE pf.boundary_value_on_right WHEN 0 THEN prv.boundary_id ELSE (prv.boundary_id+1) END)WHERE pstats.object_id = OBJECT_ID('Sales')ORDER BY TableName, PartitionNumber; Summary The partition function defines how to partition a table based on the values in the partition column. The partitioned table is created on the partition scheme that uses the partition function to map the logical partitions to physical filegroups. If each partition is mapped to a separate filegroup, partitions can be placed on slower or faster disks based on how frequently they are accessed, historical partitions can be set to read-only, and partitions can be backed up and restored individually based on how critical the data is. This post is the first in a series of Table Partitioning in SQL Server blog posts. It covers the basics of partitioned tables, partition columns, partition functions and partition schemes. Future blog posts in this series will build upon this information and these examples to explain other and more advanced concepts. Table Partitioning - Partition SwitchingThis post is part 2 of 2 in the series Table Partitioning in SQL Server Inserts, updates and deletes on large tables can be very slow and expensive, cause locking and blocking, and even fill up the transaction log. One of the main benefits of table partitioning is that you can speed up loading and archiving of data by using partition switching. Partition switching moves entire partitions between tables almost instantly. It is extremely fast because it is a metadata-only operation that updates the location of the data, no data is physically moved. New data can be loaded to separate tables and then switched in, old data can be switched out to separate tables and then archived or purged. All data preparation and manipulation can be done in separate tables without affecting the partitioned table. Partition Switching RequirementsThere are always two tables involved in partition switching. Data is switched from a source table to a target table. The target table (or target partition) must always be empty. (The first time I heard about partition switching, I thought it meant “partition swapping“. I thought it was possible to swap two partitions that both contained data. This is currently not possible, but I hope it will change in a future SQL Server version.) Partition switching is easy – as long as the source and target tables meet all the requirements :) There are many requirements, but the most important to remember are: The source and target tables (or partitions) must have identical columns, indexes and use the same partition column The source and target tables (or partitions) must exist on the same filegroup The target table (or partition) must be empty If all the requirements are not met, SQL Server is happy to tell you exactly what went wrong and provides detailed and informative error messages. Some of the most common examples are listed near the end of this blog post. Partition Switching ExamplesPartitions are switched by using the ALTER TABLE SWITCH statement. You ALTER the source table (or partition) and SWITCH to the target table (or partition). There are four ways to use the ALTER TABLE SWITCH statement: Switch from a non-partitioned table to another non-partitioned table Load data by switching in: Switch from a non-partitioned table to a partition in a partitioned table Archive data by switching out: Switch from a partition in a partitioned table to a non-partitioned table Switch from a partition in a partitioned table to a partition in another partitioned table The following examples use code from the previous Table Partitioning Basics blog post. It is important to notice that these examples are meant to demonstrate the different ways of switching partitions, they do not create any indexes and they map all partitions to the [PRIMARY] filegroup. These examples are not meant to be used in real-world projects. 1. Switch from Non-Partitioned to Non-PartitionedThe first way to use the ALTER TABLE SWITCH statement is to switch all the data from a non-partitioned table to an empty non-partitioned table: 1ALTER TABLE Source SWITCH TO Target Before switch: After switch: This is probably not used a lot, but it is a great way to start learning the ALTER TABLE SWITCH statement without having to create partition functions and partition schemes: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesSource') DROP TABLE SalesSource;IF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesTarget') DROP TABLE SalesTarget; – Create the Non-Partitioned Source Table (Heap) on the [PRIMARY] filegroupCREATE TABLE SalesSource ( SalesDate DATE, Quantity INT) ON [PRIMARY]; – Insert test dataINSERT INTO SalesSource(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Create the Non-Partitioned Target Table (Heap) on the [PRIMARY] filegroupCREATE TABLE SalesTarget ( SalesDate DATE, Quantity INT) ON [PRIMARY]; – Verify row count before switchSELECT COUNT(*) FROM SalesSource; – 1461000 rowsSELECT COUNT(*) FROM SalesTarget; – 0 rows – Turn on statisticsSET STATISTICS TIME ON; – Is it really that fast...?ALTER TABLE SalesSource SWITCH TO SalesTarget; – YEP! SUPER FAST! – Turn off statisticsSET STATISTICS TIME OFF; – Verify row count after switchSELECT COUNT(*) FROM SalesSource; – 0 rowsSELECT COUNT(*) FROM SalesTarget; – 1461000 rows – If we try to switch again we will get an error:ALTER TABLE SalesSource SWITCH TO SalesTarget; – Msg 4905, ALTER TABLE SWITCH statement failed. The target table 'SalesTarget' must be empty. – But if we try to switch back to the now empty Source table, it works:ALTER TABLE SalesTarget SWITCH TO SalesSource; – (...STILL SUPER FAST!) 2. Load data by switching in: Switch from Non-Partitioned to PartitionThe second way to use the ALTER TABLE SWITCH statement is to switch all the data from a non-partitioned table to an empty specified partition in a partitioned table: 1ALTER TABLE Source SWITCH TO Target PARTITION 1 Before switch: After switch: This is usually referred to as switching in to load data into partitioned tables. The non-partitioned table must specify WITH CHECK constraints to ensure that the data can be switched into the specified partition: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687 – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesSource') DROP TABLE SalesSource;IF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesTarget') DROP TABLE SalesTarget;IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = N'psSales') DROP PARTITION SCHEME psSales;IF EXISTS (SELECT * FROM sys.partition_functions WHERE name = N'pfSales') DROP PARTITION FUNCTION pfSales; – Create the Partition Function CREATE PARTITION FUNCTION pfSales (DATE)AS RANGE RIGHT FOR VALUES ('2013-01-01', '2014-01-01', '2015-01-01'); – Create the Partition SchemeCREATE PARTITION SCHEME psSalesAS PARTITION pfSales ALL TO ([Primary]); – Create the Non-Partitioned Source Table (Heap) on the [PRIMARY] filegroupCREATE TABLE SalesSource ( SalesDate DATE, Quantity INT) ON [PRIMARY]; – Insert test dataINSERT INTO SalesSource(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2013-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Create the Partitioned Target Table (Heap) on the Partition SchemeCREATE TABLE SalesTarget ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO SalesTarget(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2013-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2013-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Verify row count before switchSELECT COUNT(*) FROM SalesSource; – 366000 rowsSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesTarget')ORDER BY PartitionNumber; – 0 rows in Partition 1, 365000 rows in Partitions 2-4 – Turn on statisticsSET STATISTICS TIME ON; – Is it really that fast...?ALTER TABLE SalesSource SWITCH TO SalesTarget PARTITION 1; – NOPE! We get an error: – Msg 4982, ALTER TABLE SWITCH statement failed. Check constraints of source table 'SalesSource' – allow values that are not allowed by range defined by partition 1 on target table 'Sales'. – Add constraints to the source table to ensure it only contains data with values – that are allowed in partition 1 on the target tableALTER TABLE SalesSourceWITH CHECK ADD CONSTRAINT ckMinSalesDate CHECK (SalesDate IS NOT NULL AND SalesDate &gt;= '2012-01-01');ALTER TABLE SalesSourceWITH CHECK ADD CONSTRAINT ckMaxSalesDate CHECK (SalesDate IS NOT NULL AND SalesDate &lt; '2013-01-01'); – Try again. Is it really that fast...?ALTER TABLE SalesSource SWITCH TO SalesTarget PARTITION 1; – YEP! SUPER FAST! – Turn off statisticsSET STATISTICS TIME OFF; – Verify row count after switchSELECT COUNT(*) FROM SalesSource; – 0 rowsSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesTarget')ORDER BY PartitionNumber; – 366000 rows in Partition 1, 365000 rows in Partitions 2-4 3. Archive data by switching out: Switch from Partition to Non-PartitionedThe third way to use the ALTER TABLE SWITCH statement is to switch all the data from a specified partition in a partitioned table to an empty non-partitioned table: 1ALTER TABLE Source SWITCH PARTITION 1 TO Target Before switch: After switch: This is usually referred to as switching out to archive data from partitioned tables: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesSource') DROP TABLE SalesSource;IF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesTarget') DROP TABLE SalesTarget;IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = N'psSales') DROP PARTITION SCHEME psSales;IF EXISTS (SELECT * FROM sys.partition_functions WHERE name = N'pfSales') DROP PARTITION FUNCTION pfSales; – Create the Partition Function CREATE PARTITION FUNCTION pfSales (DATE)AS RANGE RIGHT FOR VALUES ('2013-01-01', '2014-01-01', '2015-01-01'); – Create the Partition SchemeCREATE PARTITION SCHEME psSalesAS PARTITION pfSales ALL TO ([Primary]); – Create the Partitioned Source Table (Heap) on the Partition SchemeCREATE TABLE SalesSource ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO SalesSource(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Create the Non-Partitioned Target Table (Heap) on the [PRIMARY] filegroupCREATE TABLE SalesTarget ( SalesDate DATE, Quantity INT) ON [PRIMARY]; – Verify row count before switchSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('Sales')ORDER BY PartitionNumber; – 366000 rows in Partition 1, 365000 rows in Partitions 2-4SELECT COUNT(*) FROM SalesTarget; – 0 rows – Turn on statisticsSET STATISTICS TIME ON; – Is it really that fast...?ALTER TABLE SalesSource SWITCH PARTITION 1 TO SalesTarget; – YEP! SUPER FAST! – Turn off statisticsSET STATISTICS TIME OFF; – Verify row count after switchSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesSource')ORDER BY PartitionNumber; – 0 rows in Partition 1, 365000 rows in Partitions 2-4SELECT COUNT(*) FROM SalesTarget; – 366000 rows 4. Switch from Partition to PartitionThe fourth way to use the ALTER TABLE SWITCH statement is to switch all the data from a specified partition in a partitioned table to an empty specified partition in another partitioned table: 1ALTER TABLE Source SWITCH PARTITION 1 TO Target PARTITION 1 Before switch: After switch: This can be used when data needs to be archived in another partitioned table: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081 – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesSource') DROP TABLE SalesSource;IF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesTarget') DROP TABLE SalesTarget;IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = N'psSales') DROP PARTITION SCHEME psSales;IF EXISTS (SELECT * FROM sys.partition_functions WHERE name = N'pfSales') DROP PARTITION FUNCTION pfSales; – Create the Partition Function CREATE PARTITION FUNCTION pfSales (DATE)AS RANGE RIGHT FOR VALUES ('2013-01-01', '2014-01-01', '2015-01-01'); – Create the Partition SchemeCREATE PARTITION SCHEME psSalesAS PARTITION pfSales ALL TO ([Primary]); – Create the Partitioned Source Table (Heap) on the Partition SchemeCREATE TABLE SalesSource ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO SalesSource(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2013-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Create the Partitioned Target Table (Heap) on the Partition SchemeCREATE TABLE SalesTarget ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO SalesTarget(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2013-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2013-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Verify row count before switchSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesSource')ORDER BY PartitionNumber; – 366000 rows in Partition 1, 0 rows in Partitions 2-4SELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesTarget')ORDER BY PartitionNumber; – 0 rows in Partition 1, 365000 rows in Partitions 2-4 – Turn on statisticsSET STATISTICS TIME ON; – Is it really that fast...?ALTER TABLE SalesSource SWITCH PARTITION 1 TO SalesTarget PARTITION 1; – YEP! SUPER FAST! – Turn off statisticsSET STATISTICS TIME OFF; – Verify row count after switchSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesSource')ORDER BY PartitionNumber; – 0 rows in Partition 1-4SELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesTarget')ORDER BY PartitionNumber; – 366000 rows in Partition 1, 365000 rows in Partitions 2-4 Error messagesSQL Server provides detailed and informative error messages if not all requirements are met before switching partitions. You can see all messages related to ALTER TABLE SWITCH by executing the following query, it is also quite a handy requirements checklist: 1234SELECT message_id, text FROM sys.messages WHERE language_id = 1033AND text LIKE '%ALTER TABLE SWITCH%'; Summary Partition switching moves entire partitions between tables almost instantly. New data can be loaded to separate tables and then switched in, old data can be switched out to separate tables and then archived or purged. There are many requirements for switching partitions. It is important to understand and test how partition switching works with filegroups, indexes and constraints. This post is the second in a series of Table Partitioning in SQL Server blog posts. It covers the basics of partition switching. Future blog posts in this series will build upon this information and these examples to explain other and more advanced concepts.","link":"/2020/12/25/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-1/"},{"title":"SQL Server Table Partitioning in Large Scale Data Warehouse 2","text":"This post is part 2, we are focusing on design and create data process to extract, transform and load (ETL) big amount data into partitioned tables to be able to integrate to SSIS package then operate ETL process by scheduled job automatically. In this case, we suppose transaction data with 100 columns and 100 million records need to be loaded into data warehouse from staging table. Technically, we can do partition switching from staging table (non partitioned table) to data warehouse table (partitioned table), but under the business reality, we can’t just simply do it like that, because Source table and target table usually are in different database, it’s not able to switch data directly because of violating same file group condition by partition switching basic in part 1. Staging table would be empty after partition switching, but the most of data transformations are applied to staging table then load final results into target table in data warehouse, so in business point of view, we can’t do partition switching from staging to target neither because big impact for entire daily, weekly or monthly data process. There might be another question: why don’t we bulk load data from source to target or what benefits do we get from partition switching? Technically, yes, we can do bulk insert, however, for such big volume of data movement, the lead time is going to be hours (2-3 hours), if the process ran on business hours, it would cause big impact and it’s hard to tolerant by business users for critical data like transaction so from efficiency and performance perspective, we have to leverage partition switching to deliver transaction data in short period of time without interrupt BI reports, dashboard refresh and business analysis. What is the main idea for production operation?In order to promise transaction data is always available, we need to create a middle table to hold transaction data as source table for partition switching, we call that middle table as switch table. To satisfy all the requirements for partition switching, the switch table has to be created in as the same file group as target transaction table, identical columns, indexes, use the same partition column. We do the bulk insert from staging table to switch table then partition switch to target transaction table in data warehouse, as part 1 mentioned, this step will finish in flash as long as there is no blocking. At last, we drop switch table so the entire data process completes. Now let’s dig litter deeper on details for each steps. Create switch table taskFrom part 1 table partitioning basics, we know to create partition table we need to create partition function then partition schemes to associate with partition functions, and also need to replicate all indexes from target table such as cluster index and non cluster columnstore index. To be able to let us code more reusable, we’d better encapsulate those functions into stored procedures then create a single script to call those stored procedures to finish the task. Create partition function and scheme for switch tableBefore we jump to the code some condition need to be clarified there: Transaction table is partitioned by month Partition column is month_key which is 6 digit integer like 202010 (Oct, 2020), 202011 (Nov, 2020) Target table file group naming convention: [database name]_FG_[table name]_[year] Partition function naming convention: [database name]_PF_[table name] Partition scheme naming convention: [database name]_PS_[table name] Cluster indexes naming convention: PK_[table name] Non cluster columnstore index naming convention: CSI_[table name] Now, we are ready to create stored procedure to generate partition function and scheme for switch table: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166use business_dbgocreate procedure dbo.usp_makePartitionOnTable @table_name varchar(100) --target (structure source) table name,@created_table_name varchar(100) --switch table name,@lrang int --left range,@rrang int --right range,@partition_col varchar(100) = 'month_key',@dbname varchar(100) = 'business_db'asbeginbegin trydeclare @sp_msg varchar(max)--check input parameter value, can't be '' or NULLset @table_name = ltrim(rtrim(@table_name))if (@table_name = '' or @table_name is null)begin set @sp_msg = '@table_name is empty,check input value' goto errorProc --error capture blockendset @created_table_name = ltrim(rtrim(@created_table_name))if (@created_table_name = '' or @created_table_name is null)begin set @sp_msg = '@create_table_name is empty, check input value' goto errorProcendset @lrang = ltrim(rtrim(@lrang))if (@lrang = '' or @lrang is null)begin set @sp_msg = '@lrang is empty, check input value' goto errorProcendset @rrang = ltrim(rtrim(@rrang))if (@rrang = '' or @rrang is null)begin set @sp_msg = '@rrang is empty, check input value' goto errorProcend if (@rrange &lt; 100000)begin set @sp_msg = 'the boundary is out of range, check input value' goto errorProcend declare @sSQ varchar(max),@iRange int,@pfName varchar(100) --partition function,@srcpfName varchar(100) --target table partition function,@filegropus varchar(max),@psName varchar(100),@filegroup_name varchar(100),@param varchar(200),@minrange int,@maxrange intset @pfName = @dbname+'_PF_'+@created_table_nameset @srcpfName = @dbname+'PF'+@table_name--check if partition function exists, if not, create, if so drop for rerundeclare @name varchar(100),@year intset @sSQL = 'use '+@dbname+'; if(exists(select name from sys.objects where OBJECT_NAME(OBJECT_ID)='''+@created_table_name+''' and type in (''U'')))' +'drop table dbo.'+@created_table_nameprint @sSQLexec sp_executeSQL @sSQL--check partition schemeset @sSQL='use '+@dbname+ '; select @name=PS.name from '+@dbname+'.sys.partition_schemes as PSinner join '+@dbname+'.sys.partition_functions as PF on PF.function_id=PS.function_id'+'where PF.name='''+@pfName+''''print @sSQLexec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@spName outputif(@psName != '')beginset @sSQL = 'use '+@dbname+ '; drop partition scheme '+@psNameexec sp_executeSQL @sSQLend--check partition functionset @name=''set @sSQL='use '+@dbname+'; select @name=name from '+@dbname+'.sys.partition_functions wherename='''+@pfName+''''print @sSQLexec sp_executeSQL @sSQL '@name varchar(100) output', @name=@name outputif(@name != '')beginset @sSQL = 'use '+@dbname+'; drop partition function '+@pfNameprint @sSQLexec sp_executeSQL @sSQLend--create partition function--check if lrange is in the lrange for the src table, and rrange is in the rrange. otherwise,--add one more in left and one more in rightif(@created_table_name&lt;&gt;@table_name)beginset @sSQL='use '+@dbname+'; select @lrange=convert(int,min(prv.value)), @rrange=convert(int,max(pre.value))'+' from sys.partition_range_values as prv join sys.partition_functions as pfs'+' on prv.function_id=pfs.function_id '+' where pfs.name='''+@srcpfName+''' 'set @param = '@lrange int output, @rrange int output'print @sSQLexec sp_executeSQL @sSQL, @param, @lrange=minrange output, @rrange=@maxrange outputendelsebeginset @minrange=0set @maxrange=0endif(@lrange&gt;@minrange and @rrange&lt;@maxrange)--check the input parameter value if fall in the right rangebegin--month_key calculate for Jan for lrangeif(@lrange%100=1)set iRange = @lrange-89elseset @iRange = @lrange-1endelseset @iRange = @lrange --set default value for @iRangeif(@rrange&lt;=@maxrange)beginif(@rrange%100=12)set @rrange=@rrange+89elseset @rrange=@rrange+1endset @filegroups=''set @sSQL='use '+@dbname+';'+'create partition function '+@pfName+'(int) as range left for values('while(@iRange&lt;=@rrange)beginset @filegroup_name=@dbname+'_FG_'+@table_name+'_'+convert(char(4),@year)if(convert(int, right(convert(char(6),@iRange),2))=12)set @iRange=(convert(int,left(convert(char(6),@iRange),4))+1)*100+1elseset @iRange+=1end--last boundaryif(@iRange&gt;@rrange)beginset @sSQL=@sSQL+')'set @filegroups=@filegroups+@filegroup_name+')'endelsebeginset @sSQL=@sSQL+','endendprint @sSQLexec sp_executeSQL @sSQL--create partition schemeset @psName=@dbname+'_PS_'+@created_table_nameset @sSQL='use '+@dbname+'; '+'create partition scheme '+@psName+'as partition '+@pfName+' to ('+@filegroupsexec sp_executeSQL @sSQLreturn 0end trybegin catch--add some try catch statementreturn -1end catcherrorProc:--add some error catch statementreturn -1end Replicate target table structure (column and data type) for switch table123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102use business_dbgocreate procedure dbo.usp_getTableModel( @dbName as varchar(100) ,@table_name as varchar(100) ,@create_table as varcahr(100) ,@sSQL as varchar(max) output)asbegindeclare @colTemp as table( name varchar(100) ,seqNo int ,IsNullable bit ,col_def varchar(max) ,data_type varchar(128) ,char_len int ,num_precs int ,num_dec int)set @sSQL = 'select ltrim(rtrim(column_name)) as name,ordinal_position as seqNo,case when ltrim(rtrim(is_nullable))=''no'' then 1 else 0 end as IsNullable,ltrim(rtrim(column_default)) as col_def,upper(ltrim(rtrim(data_type))) as data_type,character_maximum_length as char_len,numeric_precision as num_precs,numeric_scale as num_decfrom '+@dbname+'.information_schema.columnswhere table_name=@table_name and table_catalog=@dbname'insert into @colTempexec sp_executeSQL @sSQL, '@table_name varchar(100),@dbname varchar(100)',@table_name=@table_name,@dbname=@dbnamedeclare @i int,@name varchar(100),@seqNo int,@IsNull int,@col_def varchar(max),@data_type varchar(100),@char_len int,@num_precs int,@num_dec int,@maxcount intselect @maxcount = max(seqNo) from @colTemp;select @i=min(seqNo) from @colTemp;select@name=name,@seqNo=seqNo,@IsNull=IsNullable,@col_def=col_def,@data_type=data_typ,@char_len=char_len,@num_precs=num_precs,@num_dec=num_decfrom @colTempwhere seqNo=@iset @sSQL='create table '+quotename(@dbname)+'.dbo.'+quotename(@created_table) +'('while not @i is Nullbeginset @sSQL = @sSQL + @Nameset @sSQL = @sSQL+' '+@data_typeif(charindex('char',lower(@date_type))&gt;0 or charindex('var',lover(@data_type))&gt;0)set @sSQL=@sSQL+'('+ltrim(rtrim(convert(varchar(4),@char_len)))+')'if(charindex('decimal',lower(@data_type))&gt;0)set @sSQL = @sSQL+'('+ltrim(rtrim(convert(varchar(4),@num_precs)))+','+ltrim(rtrim(convert(4),@num_dec)))+')'if(@col_def&lt;&gt;'')set @sSQL=@sSQL+' '+'default '+@col_defif(@IsNull=0)set @sSQL=@sSQL+' Null'elseset @sSQL=@sSQL+' not Null'if(@i&lt;@maxcount)set @sSQL=@sSQL+','elseset @sSQL=@sSQL+')'delete from @colTempwhere seqNo=@iselect @i=min(seqNo) from @colTempselect@name=name,@seqNo=seqNo,@IsNull=IsNullable,@col_def=col_def,@data_type=data_typ,@char_len=char_len,@num_precs=num_precs,@num_dec=num_decfrom @colTempwhere seqNo=@iendendgo Replicate target table cluster index for switch tablewe also need to clone cluster index from src (target) to switch table 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677use business_dbgocreate procedure dbo.usp_ReplicateClusterIndex @table_name varchar(100),@created_table varchar(100),@dbname varchar(100)='business_db'asbegindeclare @i int,@sSQL varchar(max),@ncount int,@colname varchar(100),@nloop int,@return_result int,@sp_msg varchar(100)begin tryset @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset @sp_msg='ended with @table_name empty, check input value'goto errorProcenddeclare @name varchar(100)--find out if the object existsset @sSQL = 'use '+@dbname+ '; select @name=name from sys.indexes wherename=''PK_'+@created_table+''''exec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif @name != ''beginset @sSQL='use '+@dbname+'; drop index PK_'+@created_table+' on dbo.'+@created_tableexec sp_executeSQL @sSQLendif (@name='' or @name is null)begindeclare @tblcol table(colID int not null, colNmae varchar(100) not null)set @sSQL='use '+@dbname+'; select b._key,''[''+c.name+'']''from sys.indexes a join sys.index_columns b on a.object_id=b.object_idand a.index_id=b.index_idjoin sys.columns c on b.column_id=c.column_id and b.object_id=c.object_idwhere a.name=''PK_'+@table_name+''''insert into @tblColexec sp_executeSQL @sSQLselect @ncount=count(*) from @tblcol;select @i=min(colID) from @tblcol;set nloop=oset @sSQL = 'use '+@dbname+ '; create clustered index PK_'+@created_table+' on '+quotename(@created_table)+'('while not @i is nullbeginselect @colName=colName from @tblcol where colID=@iif(@nloop=0)set @sSQL=@sSQL+@colNameelseset @sSQL=@sSQL+', '+@colNameset @nloop+=1delete from @tblcol where colID=@iset @i=min(colID) from @tblcolendset @sSQL=@sSQL+')'exec @return_result=sp_executeSQL @sSQLif(return_result&lt;0)begin--log statementreturn -1endelsereturn 0endend trybegin catch--try catch block statementreturn -1end catcherrorProc:--error log statementreturn -1end Create partitioned switch tableNow we have all store procedures to meet partitioned table requirement, it’s time to create a script (store procedure) to integrate in order to create partitioned switch table. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182use business_dbgocreate procedure dbo.usp_createPartitionTable @table_name varchar(100),@partition_col varchar(100)='month_key',@created_table varchar(100),@lrange int,@rrange int,@dbname varchar(100)='business_db'asbeginset nocount on;begin trydeclare @return_result int,@sSQL varchar(max),sp_msg varchar(100)set @table_name=ltrim(rtrim(@table_name))if(@table_name ='' or @table_name is null)beginset @sp_msg='end with @table_name is empty, check input value'goto errorProcendset @created_table =ltrim(rtrim(@created_table))if(@created_table='' or @created_table is null)beginset @sp_msg='end with @created_table is empty, check input value'goto errorProcendif(@lrange is null)beginset sp_msg='end with @lrange empty, check input value'goto errorProcendif(@rrange is null)beginset sp_msg='end with @rrange empty, check input value'goto errorProcenddeclare @name varchar(100),@ps_name varchar(100)set @sSQL='select @name=name from '+quotename(@dbname)+'.sys.tables where name='''+@created_table''''print @sSQLexec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif(@name !='')beginset @sSQL='drop table '+quotename(@dbname)+'.dbo.'+@created_tableexec sp_executeSQL @sSQLend--create partition function and schemeexec business_db.dbo.usp_makePartitionOnTable @table_name=@table_name,@created_table_name=@created_table,@lrange=@lrange,@rrange=@rrange,@partiton_col=@partition_col,@dbname=@dbname--create swich table structureexec business_db.dbo.usp_getTableModel @dbname=@dbname,@table_name=@table_name,@created_table=@created_table,@sSQL=@sSQL outputset @ps_name=@dbname+'_PS_'+@created_tableset @sSQL='use '+@dbname+';'+@sSQL+' on '+@ps_name+'('+@partition_col+')'print @sSQLexec sp_executedSQL @sSQL--create cluster index for switch tableexec @return_result=business_db.dbo.usp_ReplicateClusterIndex @table_name=@table_name,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endelsereturn 0end trybegin catch--add try catch block statementreturn -1end catcherrorProc:--add some error handling statementreturn -1end Business data APIWe have everything to create a partitioned table so far, the only thing left is create a data API to pass in input value in terms of business RSD (Requirement Specifics document). 12345678910111213141516171819202122232425use business_dbgocreate procedure switch_transactionas beginbegin trydeclare @left_range int,@right_range int,@return_result intselect top 1 @left_range=month_key from staging.dbo.transactionexec @return_result=business_db.dbo.usp_createPartitionTable @table_name='transaction',@partition_col='month_key',@created_table='switch_transaction',@lrange=@left_range,@rrange=@left_range,@dbname='business_db'if(return_result&lt;0)begin--add log statementreturn -1endelsereturn 0end tryend Load staging transaction data into switch tableThis step, we need to prepare data for switch table in order to make partition switching later on. Because switch table is a kind of temp table and it’s not awareness by business data users, so no matter how long the loading time is, it won’t cause down time on production. For the bulk insert, the simplest and fastest way is use SSIS data flow which provides bulk load functionality. for Data access mode option, there is drop-down list, make sure Table or view - fast load being selected. Create columnstore index for switch tableIn this step, we create columnstore index for switch table, you might ask why we don’t create it along with cluster index prior data loading? We separate columnstore index from cluster index is by its special properties, because we can’t load data with columnstore index enabled. Now let’s brief take a look at the definition by Microsoft Columnstore indexes are the standard for storing and querying large data warehousing fact tables. This index uses column-based data storage and query processing to achieve gains up to 10 times the query performance in your data warehouse over traditional row-oriented storage. You can also achieve gains up to 10 times the data compression over the uncompressed data size. Beginning with SQL Server 2016 SP1, columnstore indexes enable operational analytics: the ability to run performant real-time analytics on a transactional workload. A columnstore index is a technology for storing, retrieving, and managing data by using a columnar data format, called a columnstore Obviously, we can gain a lot of performance benefits from columnstore index, but its specialty decides we have to treat it very carefully, because it costs a quite long time to build it for large data warehousing fact table. we are going to create a stored procedure to achieve the function of building columnstore index then write another API to create columnstore index for switch table in terms of business requirement. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475use business_dbgocreate procedure usp_createColmart @table_name varchar(100),@src_table varchar(100) --target table ,@dbname varchar(100)='business_db'asbegindeclare @i int,@sSQL varchar(max),@ncount int,@colName varchar(100),@nloop int,@return_result int,@sp_msg varchar(100)begin tryset @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset sp_msg='end with @table_name is empty, check the input value'goto errorProcendset @dbname=ltrim(rtrim(@dbname))declare @name varchar(100)set @sSQL='use '+@dbname+'; select @name=name from sys.indexes where name=''CSI_'+@table_name+''''exec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif(@name !='')set @sSQL='use '+@dbname+'; drop index CSI_'+@table_name+' on dbo.'+@table_nameexec sp_executeSQL @sSQLendelsedeclare @tblcol table(colID int not null, colName varchar(100) not null)set @sSQL='use '+@dbname+'; select c.column_id,c.name from sys.indexes ainner join sys.index_columns b on a.object_id=b.object_id and a.index_id=b.index_idinner join sys.columns c on b.object_id=c.object_id and b.column_id=c.column_idwhere a.is_primary_key=0 anda.is_unique=0 anda.is_unique_constraint=0 anda.name=''CSI_'+@scr_table+''''insert into @tblcolexec sp_executeSQL @sSQLselect @ncount=count(*)from @tblcolset @nloop=0set @sSQL='use '+@dbname+'; create nonclustered columnstore index CSI_'+@table_name+' on '+quotename(@table_name)+'('while not @i is nullbeginselect @colName=colName from @tblcol where colID=@iif(@nloop=0)set @sSQL = @sSQL + @colNameelseset @sSQL=@sSQL+','+@colNameset nloop+=1delete from @tblcol where colID=@iselect @i=min(colID) from @tblcolendset @sSQL+=')'exec @return_result=sp_executeSQL @sSQLif(@return_result&lt;0)begin--add log statementreturn -1endelsereturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Now, we create business data API to pass in parament value based on business requirement, in this case, we deal with transaction table 1234567891011121314151617181920212223242526272829use business_dbgocreate procedure switch_transaction_csiasbeginbegin trydeclare @left_range int,@right_range int,@return_result intexec @return_result=business_db.dbo.usp_createColmart @table_name='switch_transaction',@scr_table='transaction',@dbname='business_db'if(@return_result&lt;0)begin--add log statementreturn -1endelsereturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Partition switching to load data to target tableSo far we create switch table and load transaction data into it then create columnstore index, next we are going to conduct major task to switch partition from switch table to target transaction table. For code reusable and more generic scenarios, we still need do some condition checks before we jump to partition switching. Check data in switch (staging) table is in the right range and ready to be proceeded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566use business_dbgocreate procedure usp_IsStgDataReady( @stgtbl_name varchar(100) ,@partition_col varchar(100)='month_key' ,@lrange int ,@urange int ,@dbname varchar(100)='business_db')asbeginbegin trydeclare @sSQL varchar(max),@month_key int,@sParam varchar(100),@range int,@sp_msg varchar(100)set @stgtbl_name=ltrim(rtrim(@stgtbl_name))if(@stgtbl_name='' or @stgtbl_name is null)beginset sp_msg='end up with @stgtbl_name empty, check input value'goto errorProcendset @partition_col=ltrim(rtrim(@partition_col))if(@partition_col='' or @partition_col is null)beginset @sp_msg='end up with @partition_col empty, check input value'goto errorProcendif(not len(convert(char(6),@urange))=6)beginset sp_msg='end up with @urange is out of range, check input value'goto errorProcendset @range=@lrangewhile @range &lt;= @urangebeginset @sParam='@month_keyOut int output'set @sSQL='select top 1 @month_keyOut='+@partition_col+' from'+@dbname+'.dbo.'+@stgtbl_name+'where'+@partition_key+' ='+convert(varchar(6),@range)exec sp_executeSQL @sSQL,@sParam,@month_keyOUt=@month_key outputif(@month_key!=@range or @month_key is null)beginset @sp_msg='data for '+convert(char(6),@range)+' not ready for loading'return -1endif(right(convert(char(6),@range),2)='12')beginset @range=convert(int,(left(convert(char(6),@range),4)+1))*100+1elseset @range+=1endreturn 0endend trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Check up target table partitionFrom previous post about table partitioning basics, the last requirement for partition switching is partition in target table must be empty, so it’s necessary to check whether target table partition has data in it, if it’s empty, that is good and we are ready to do the partition switching, but in production environment, we usually need to handle more complicated situations like rerun after job failed so definitely have to take it into account when target partition has data and deal with it. Firstly, let’s check target partition: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455use business_dbgocreate procedure IsPartitionLoaded @table_name,@partition_col varchar(100)='month_key',@range int,@ret int=0 output,@dbname varchar(100)='business_db'asbeginbegin trydeclare @sSQL varchar(max),@sParam varchar(100),@monthKey int,@sp_msg varchar(100)set @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset sp_msg='end up with @table_name empty, check input value'goto errorProcendset @partition_col=ltrim(rtrim(@partition_col))if(@partition_col='' or @partition_col is null)beginset sp_msg='end up with @partition_col empty, check input value'goto errorProcendif(isnumeric(@range)!=1 or not(len(convert(char(8),@range))=6))beginset @sp_msg='end up with @range out of range, check input value'goto errorProcendset @ret=1 --target partition has dataset sParam='@retOut int output'beginset @sSQL='select top 1 @retOut='+@partition_col+' from '+'.dbo.'+@table_name+'where '+@partition_col+' ='+convert(char(6),@range)exec sp_executeSQL @sSQL, @sParam, @retOut=@monthKey outputendif (@monthKey is null or @monthkey='')beginset @ret=0 --target partition is emptyreturn 0endreturn 0end trybegin catchset @ret = -1--add try catch block statmentreturn -1end catcherrorProc:set @ret = -1--error handling statementreturn -1end Get proper partition numbers for source (switch) and target tableLet’s first handle ideal key which is empty in target partition, the last preparation we have to know is find out proper partition number on both source and target sides so that we are able to switch to right target partition. 1234567891011121314151617181920212223242526272829303132333435363738use business_dbgocreate procedure usp_getPatitionNum @pf_name varchar(100),@range int,@pn int output,@dbname varchar(100)='business_db'asbeginbegin trydeclare @sp_msg varchar(100)set @pf_name=ltrim(rtrim(@pf_name))if(@pf_name='' or @pf_name is null)beginset sp_msg='end up with @pf_name is empty, check input value'goto errorProcendif((not isnumeric(@range)=1)or (@range&lt;100000))beginset sp_msg='end up with @range is out of range, check input value'goto errorProcenddeclare @sParam varchar(100),@sSQL varchar(max)if @range&lt;10000000set @sSQL='use '+@dbname+'; select @pnOut=$PARTITION.'+@pf_name+'('+convert(char(6),@range)+')'set @sParam='@pnOut int output'exec sp_executeSQL @sSQL, @sParam, @pnOut=@pn outputreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Partition switchingNow, it’s time for us to switch partition to target table 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849use business_dbgocreate procedure usp_switchPartition @srctbl_name varchar(100),@destbl_name varchar(100),@src_pn int=0,@des_pn int=0,@dbname varchar(100)='business_db'asbegindeclare @pn int,@sSQL varchar(max),@sParam varchar(100),@ncount int,@sp_msg varchar(100)begin tryset @srctbl_name=ltrim(rtrim(@srctbl_name))if(@srctbl_name='' or @srctbl_name is null)beginset sp_msg='end up with @srctbl_name empty, check input value'goto errorProcendset @destbl_name=ltrim(rtrim(@destbl_name))if(@destbl_name='' or @srctbl_name is null)beginset sp_msg='end up with @destbl_name empty, check input value'endset @sSQL='use '+@dbname+'; alter table '+@dbname+'.dbo.'+quotename(@srctbl_name)if(@src_pn&gt;0)set @sSQL=@sSQL+' switch partition '+convert(char(4),@src_pn)elseset @sSQL=@sSQL+' switch 'if(@des_pn &gt; 0)set @sSQL=@sSQL+' to'+@dbname+'.dbo.'+ quotename(@destbl_name)+' partition '+ convert(char(4),@des_pn)elseset @sSQL=@sSQL+' to '+@dbname+'.dbo.'+ quotename(#destbl_name)exec sp_executeSQL @sSQLreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Clean the switch table after partition switchingThe switch table is going to be empty after partition switching so it’s useless now and can be drop from database as well as partition function and scheme 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061use business_dbgocreate procedure usp_cleanTable @table_name varchar(100),@partition_col varchar(100)='month_key',@dbname varchar(100)='business_db'as begin begin trydeclare sp_msg varchar(100)set @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset sp_msg='end up with @table_name empty, check input value'goto errorProcenddeclare @sSQL varchar(max),@constraint_name varchar(100),@return_result int,@pf_name varchar(100),@ps_name varchar(100)@name varchar(100)set @pf_name=@dbname+'_PF_'+@table_nameset @ps_name=@dbname+'_PS_'+@table_name--drop tableset @name=''set @sSQL='use '+@dbname+'; select @name=name from sys.tables where name='''+@table_name+''''exec sp_executeSQL @sSQL, '@name varchar(100) output',@name=@name outputif(@name!='')beginset @sSQL='use '+@dbname+'; drop table dbo.'+@table_nameexec sp_executeSQL @sSQL--drop partition schemeset @name=''set @sSQL='use '+@dbname+'; select @name=name from sys.partition_schemes where name='''+@ps_name+''''exec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif(@name!='')beginset @sSQL='use '+@dbname+'; drop partition scheme'+@ps_nameexec sp_executeSQL @sSQLend--drop partition functionset @name=''set @sSQL='use '+@dbname+'; select @name=name from sys.partition_functions where name='''+@pf_name+''''exec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif(@name!='')beginset @sSQL='use '+@dbname+'; drop partition function'+@ps_nameexec sp_executeSQL @sSQLendendreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Swap data out of target partition to temp tableA special stored procedure is needed to switch data out of target partition if there would be data in there. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475use business_dbgocreate procedure dbo.unloadData @table_name varchar(100),@tempTbl varchar(100),@range int,,@partition_col varchar(100)='month_key',@dbname varchar(100)='business_db'asbeginbegin trydeclare @pn int,@pnd int,@return_result int,@filegroup_name varchar(100),@pf_name varchar(100),@isRead int,@sp_msg varchar(100),@ps_name varchar(100)set @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset @sp_msg='end up with @table_name empty, check input value'goto errorProcendif(not isnumeric(@range)=1 or @range&lt;100000)beginset @sp_msg='end up with @range out of range, check input value'goto errorProcendset @pf_name=@dbname+'_PF_'+@table_name--get src table partitionexec @return_result=business_db.dbo.usp_getPatitionNum @pf_name=@pf_name,@range=@range,@pn=@pn output,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endif(@tempTbl='')set @tempTbl='Temp_'+@table_name--get des table partitionset @pf_name=@dbname+'_PF_'+@tempTblexec @return_result=business_db.dbo.usp_getPatitionNum @pf_name=@pf_name,@range=@range,@pn=@pnd output,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--swap data out from target partition to temp tableexec @return_result=business_db.dbo.usp_switchPartition @srctbl_name=@table_name,@destbl_name=@tempTbl,@src_pn=@pn,@des_pn=@pnd,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Control console script to integrate all functionsWe are now all set so it’s time to integrate all those stored procedure into one control console to implement the partition switching task. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179use business_dbgocreate procedure dbo.usp_switchData @scrtable_name varchar(100),@table_name varchar(100),@lrange int,@urange int,@partiton_col varchar(100)='month_key',@dbname varchar(100)='business_db'asbeginbegin trydeclare @sp_msg varchar(100)--check input valuesset @srctable_name=ltrim(rtrim(@srctable_name))if(@srctable_name='' or @srctable_name is null)beginset @sp_msg='end up with @srctable_name empty, check input value'goto errorProcendset @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset @sp_msg='end up with @table_name empty, check input value'goto errorProcendif((not isnumeric(@lrange)=1) or (@lrange &lt; 100000))beginset sp_msg='end up with @lrange out of range, check input value'goto errorProcendif(@urange=0)set @urange=@lrangeset @partition_col=ltrim(rtrim(@partition_col))--declare local variablesdeclare @sSQ varchar(max),@check_constraint varchar(100),@pf_name varchar(100),@ps_name varchar(100),@staging_pf_name varchar(100),@staging_ps_name varchar(100),@filegroup_name varchar(100),@desfilegroup_name varchar(100),@range_count int,@range int,@sParam varchar(100),@ret int,@pn int,@pn_staging int,@index_name varchar(100),@return_result int,@destbl_lrange int--set valuesset @pf_name=@dbname+'_PF_'+@table_nameset @ps_name=@dbname+'_PS_'+table_nameset @staging_pf_name=@dbname+'_PF_'+srctable_nameset @staging_pf_name=@dbname+'_PS_'+srctable_nameset @range=@lrangeset @range_count=0set @pn_staging=0--check if switch table data is ready for the range from @lrange to @urangeexec @return_result=business_db.dbo.usp_IsStgDataReady @stgtbl_name=@srctable_name,@partition_col=@partiton_col,@lrange=@lrange,@urange=urange,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--begin while loop transactiondeclare @tempTbl varchar(100)set @temTbl='Temp'+@table_nameset @range=@lrangewhile @range&lt;=@urangebeginset @range_count+=1--check if data is in target partitionset @ret=0exec return_result=business_db.dbo.IsPartitionLoaded @table_name=@table_name,@partition_col=@partition_col,@range=@range,@ret=@ret output,@dbname=@dbnameif(@ret=1)begin--create temp table to hold swapped dataexec @return_result=busienss_db.dbo.usp_createPartitionTable @table_name=@table_name,@partition_col=@partition_col,@created_table=@tempTbl,@lrange=@lrange,@rrange=@urange,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--create columnstore index for temTblexec @return_result=business_db.dbo.usp_createColmart @table_name=temTbl,@scr_table=@table_name,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--swap data to temp tableexec @return_result=business_db.dbo.usp_unloadData @table_name=@table_name,@tempTbl=@tempTbl,@range=@range,@partition_col=@partition_col,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--get partition number from destination tableexec @return_result=business_db.dbo.usp_getPatitionNum @pf_name=@pf_name,@range=range,@pn=@pn output,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--get partition number from switch tableexec @return_result=business_db.dbo.usp_getPatitionNum @pf_name=@staging_pf_name,@range=range,@pn=@pn_staging output,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--partition switching from switch to target partitionexec @return_result=business_db.dbo.usp_switchPartition @srctbl_name=@src_name,@destbl_name=@table_name,@src_pn=@pn_staging,@des_pn@pn,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endif(@yearload_flag=0)beginif(right(convert(char(6),@range),2)='12')set @range=(convert(int,left(convert(6),@range)4))+1)*100+1elseset @range+=1endelseset @range+=100end--drop temp tableexec @return_result=business_db.dbo.usp_cleanTable @table_name=@tempTbl,@partition_col=@partition_col,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endendreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Business data API for final transaction partition switchingAll in all, we are going to integrate all encapsulated stored procedures into one data API to take input parameter value from outside then accomplish the task which is partition switch transaction data 100mm records in flash. 12345678910111213141516171819202122232425262728293031use business_dbgocreate procedure transaction_partition_switch asbeginbegin trydeclare @month_key int,@return_result intselect top 1 @month_key=month_key from staging.dbo.transactionexec @return_result=business_db.dbo.usp_switchData @scrtable_name='switch_transaction',@table_name='transaction',@lrange=@month_key,@urange=0,@partition_col='month_key',@yearload_flag=0,@dbname='business_db'if(@return_result&lt;0)begin--add some log statementreturn -1endend trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end A sub-process of partition switching for error handling and restatement in production[In previous chapter](Get proper partition numbers for source (switch) and target table), we talk about the ideal case which is there is no data or empty in target partition, but in real production environment, situation is more complicated, some unpredicted event might cause data process failed or business redefined some data elements so that reprocess and restatement is necessary, in those cases, the target partition usually has data in it, that is the thing we have to deal with. The approach is the same, partition switching will help us out. The main idea is to create another partition temp table with the same table structure, partition column and index setting as target table in the same file group then swap data out of target partition to temp table counterpart partition, finally drop the temp table and it’s partition function and scheme. ConclusionAbove, we go through the whole solution on how to apply SQL table partitioning technic to implement partition switching in real ETL process so that we can easily to schedule it run on regular basis, I believe process automation is a valuable AI technic for business operation, because the best use-case for AI projects will reduce costs, reduce risk and improve profits. more often than not, the best results are seen from implementing AI to handle the small, repetitive tasks that businesses do on a daily basis.","link":"/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/"}],"tags":[{"name":"SQL, query, SQL Server, optimization","slug":"SQL-query-SQL-Server-optimization","link":"/tags/SQL-query-SQL-Server-optimization/"},{"name":"file system, csv","slug":"file-system-csv","link":"/tags/file-system-csv/"},{"name":"SQL Server, database, monitoring, system optimization","slug":"SQL-Server-database-monitoring-system-optimization","link":"/tags/SQL-Server-database-monitoring-system-optimization/"},{"name":"Hadoop, Hive, Impala, HDFS, SQL Server, SSIS","slug":"Hadoop-Hive-Impala-HDFS-SQL-Server-SSIS","link":"/tags/Hadoop-Hive-Impala-HDFS-SQL-Server-SSIS/"},{"name":"python, automation, ETL, Excel","slug":"python-automation-ETL-Excel","link":"/tags/python-automation-ETL-Excel/"},{"name":"Linux, LVM, VMware, Disk management","slug":"Linux-LVM-VMware-Disk-management","link":"/tags/Linux-LVM-VMware-Disk-management/"},{"name":"hexo, blog, online image, config, Node.js, router","slug":"hexo-blog-online-image-config-Node-js-router","link":"/tags/hexo-blog-online-image-config-Node-js-router/"},{"name":"Python, Pandas, SQL, ETL","slug":"Python-Pandas-SQL-ETL","link":"/tags/Python-Pandas-SQL-ETL/"},{"name":"Linux, Centos, network, config","slug":"Linux-Centos-network-config","link":"/tags/Linux-Centos-network-config/"},{"name":"oracle, SSIS, SAS, config","slug":"oracle-SSIS-SAS-config","link":"/tags/oracle-SSIS-SAS-config/"},{"name":"Linux, centos, ubuntu, network","slug":"Linux-centos-ubuntu-network","link":"/tags/Linux-centos-ubuntu-network/"},{"name":"python, sql, db2, CLI, SSIS, config","slug":"python-sql-db2-CLI-SSIS-config","link":"/tags/python-sql-db2-CLI-SSIS-config/"},{"name":"Linux, MySQL, Config, Database","slug":"Linux-MySQL-Config-Database","link":"/tags/Linux-MySQL-Config-Database/"},{"name":"SQL Server, partitioning","slug":"SQL-Server-partitioning","link":"/tags/SQL-Server-partitioning/"},{"name":"spark, pyspark, environment, linux","slug":"spark-pyspark-environment-linux","link":"/tags/spark-pyspark-environment-linux/"},{"name":"Python, virtual environment","slug":"Python-virtual-environment","link":"/tags/Python-virtual-environment/"},{"name":"sandbox, SQL, SQL Server, database","slug":"sandbox-SQL-SQL-Server-database","link":"/tags/sandbox-SQL-SQL-Server-database/"}],"categories":[{"name":"Data","slug":"Data","link":"/categories/Data/"},{"name":"IT","slug":"IT","link":"/categories/IT/"},{"name":"IT, BI","slug":"IT-BI","link":"/categories/IT-BI/"},{"name":"Data, BI","slug":"Data-BI","link":"/categories/Data-BI/"},{"name":"data, BI","slug":"data-BI","link":"/categories/data-BI/"},{"name":"Database, BI","slug":"Database-BI","link":"/categories/Database-BI/"}]}