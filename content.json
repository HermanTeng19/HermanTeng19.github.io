{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"A little thought on SQL query performance optimization 1","text":"Working with data and database, writing query is daily routine, query running time might be big different for different queries but serve the same purpose, which shows query performance is not 100 percent determined by database system configuration, server hard ware, table index and statistics etc. but how well do you use SQL to construct your query. To satisfy user strong demand, we built sandbox database on production for business partners to server their needs of BI reporting and business analytical, in the meanwhile, we are also in charge of database maintenance and monitoring so that we got chance to collect all kinds of user queries. Some of them even crash the system or drag the database performance down apparently, here I want to share my thinking of query optimization on large amount of data. Before writing a queryJust like a project, a query is able to achieve the things with backend business logics even if it is small, so make a plan before creation is very necessary to be able to let the query return result in flash and only consume minimum system resources. What level detail of data do you want to query from? Business data is built on certain level, e.g. in common scenario, there are customer level, account level and transaction level data. It’s better clarify target data is on which level or mix different level. What the time scope? The data amount volume would be big if you want to query multiple years even months, so if you evaluate the data is big, it’s better use loop or paging technic instead of returning outcome in one single page. What are query tables look like? A table contains a lot of information, not only business attributes or columns but data type, keys, index, constraints, triggers. Familiar with table structure is going to give additional benefits when you write your queries against those tables. SQL query optimization tips Correlated and uncorrelated subquery Business often asks about the highest transaction information in terms of product, product group, merchant category etc. in certain month or quarter. For this kind of request is not straight forward to be solved by single select from query but it needs subquery. Now let’s see what I got from a business partner 12345678select tran_date, Product_cd, Merchant_Name, Trans_Amtfrom trans awhere tran_date between '2020-10-01' and '2020-10-31' and trans_Amt=( select Max(b.Trans_Amt) from trans b where a.product_cd=b.product_cd and tran_date between '2020-10-01' and '2020-10-31')order by Product_cd subquery is correlated with main query so trans data is loaded into memory again and make a calculation to return data to main query, that query execution time is 32s based on 90mm data. What if change correlated to uncorrelated subquery like below 12345678910select tran_date, Product_cd, Merchant_Name, Trans_Amtfrom trans ajoin (select Product_cd, Max(Trans_Amt) as Trans_Amt, tran_date from trans where tran_date between '2020-10-01' and '2020-10-31' group by Producct_cd, tran_date) as bon a.Product_cd=b.Product_cd and a.Trans_Amt=b.Trans_Amt and a.tran_date=b.tran_dateorder by a.Product_cd the query execution time reduce to 26s. Exists clause and table join The most active accounts and their corresponding spending amount is another important KPI for product manage from account management perspective, additionally, if some condition applied such as account opened on certain month, we got the query from one business partner using exists statement down below 12345678select acct_id, trans_dt,count(*) as num_trans, sum(trans_amt) as tot_trans_amtfrom trans awhere date_key=202010 and exists(select 1 from acct b where a.date_key=b.date_key and a.acct_id=b.acct_id and year(acct_open_dt)*100+month(acct_open_dt)&gt;=202009)group by trans_dt, acct_idhaving count(acct_id)&gt;10order by num_trans desc from the query structure, we might be able to tell exists statement was put in where clause to get the account open date, its execution time is 13s, but it’s obviously not thinking about the whole business logic before, if the query changed to get target accounts for account open date first then did the calculation, the execution time will reduce to 3s like below 123456select a.acct_id, a.trans_dt, count(*) as num_trans, sum(trans_amt) as tot_trans_amtfrom trans a join acct b on a.acct_id=b.acct_id and a.date_key=b.date_keywhere year(b.acct_open_dt)*100+month(b.acct_open_dt)&gt;=202009 and a.date_key=202010group by a.trans_dt, a.acct_idhaving count(a.acct_id)&gt;10order by num_trans desc Reduce the data scope by subquery It’s very necessary to think about your query data scope first when you do a bunch of left outer join, especially, data volume is quite big on your main left table . Below query does a series left join by using all full tables data, but based on business requirement, only partial data is required on the major left table, so it cause somehow resources wasted in reflect on the execution time of 7s 123456select a.*from wfs_trans aleft join credit_decision b on a.tran_id=b.tran_idleft join finance_request c on a.req_id=c.req_idwhere cast(a.creation_date as date)&gt;='2019-01-01'order by creation_date desc after revised above query on left table, the execution time reduced to 5s (2000ms) 123456789select a.*from (select * from wfs_trans where cast(a.creation_date as date)&gt;='2019-01-01') as aleft join credit_decision b on a.tran_id=b.tran_idleft join finance_request c on a.req_id=c.req_idorder by a.creation_date desc Paging browse data Business users sometime complaint the SSRS or Power BI report is slow when they browse data by skipping pages, that is not the programming problem because on the backend the query to support the BI report like below 12345select prod_cd, post_dt, tran_dt, tran_amtfrom transwhere date_key=202010 and prod_cd='ABCD'order by post_dtoffset 1000000 rows fetch next 100 only; that will take 22s to return results based on 80mm data in single month. Thing thing is even the data is ordered by index column post_dt, database engine doesn’t know where is 1000000 row, it needs to recalculate again, so to answer that business concern, we recommend to apply some conditions parameter then start to browse data like below 12345select prod_cd, post_dt, tran_dt, tran_amtfrom transwhere date_key=202010 and prod_cd='ABCD' and post_dt='2020-10-15'order by post_dtoffset 1000 rows fetch next 100 only; by this way, the report will be presented by less than 1s.","link":"/2020/11/30/A-little-thought-on-SQL-query-performance-optimization-1/"},{"title":"Automation Process for Email Attachment Excel in Python","text":"Working with business data, Excel spreadsheet is the most common file type you might deal with in daily basis, because Excel is a dominated application in the business world. What is the most used way to transfer those Excel files for business operation team, obviously, it’s Outlook, because email attachment is the easiest way for business team to share data and reports. Following this business common logic and convention, you may get quite a lot Excel files from email attachment when you involved into a business initiative or project to design a data solution for BI reporting and business analysis. The pain points is too much manual work dragging down efficiency of data availability and also increasing the possibility of human error. Imagine, every day get data from email attachment, you need to check your inbox every once for a while, then download those files from attachment, open Excel to edit data or rename file to meet data process requirement such as remove the protected password, after those preparation works all done, push data to NAS drive, finally, launch the job to proceed the data. It’s not surprise that how easily you might make mistake because any single step contains error would cause the whole process failed. It’s very necessary to automate the whole data process if business project turns to BAU (Business As Usual) program and you have to proceed data in regular ongoing basis. Python and Windows Task Scheduler provides a simple and fast way to solve this problem, now let’s take a look. Overall speaking, this task can be broken down by a couple of steps: Access Outlook to read email and download the attachment Remove Excel file protected password (it’s common in business file to protect data privacy) Manipulate and edit Excel file Copy file to NAS drive Setup the Windows Task Scheduler to run python script automatically. Interact Outlook and Excel with PythonIn this case, we are focusing on local operation because server setting is vary for different production environment. Python standard library can’t meet our need on this request, we have to leverage its third-party libraries to access outlook and interact with Excel so we are going to import win32com, openpyxl and shutil. We firstly work out workflow to guide us to compose the python scripts From the workflow chart, we can tell there are four functions in the streamline. The first one is read email and download attachment readmail(), this step is critical and the most import business logic build-in it. we are going to apply two business logics, one is the latest email, the other one is if the email is latest (current date) then check if the current date -1 equals to data date in the subject line. we will process file when all the requirements are satisfied. For Excel manipulation and spreadsheet decryption are pretty straight forward, no additional business logic so just apply the single function. We create 3 python module files then encapsulate them into the main readmail() function script. fileTransfer_module.py1234567891011121314151617import win32com.client as win32import osimport os.pathimport openpyxl as xlfrom datetime import datetime, timedeltaimport sysimport shutildef cpfile(src_file, tgt_file): if os.path.exists(tgt_file): os.remove(tag_file) shutil.copy(src_file, tgt_file) else: shutil.copy(src_file, tgt_file)if __name__ == &quot;__main__&quot;: cpfile(src_file, tgt_file) excelOp_mudule.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546import openpyxl as xlfrom file_Transfer_module import cpfiledef xltemp(file, file_ind): prd_dir = r&quot;\\\\your NAS drive UNC address&quot; if file_ind == 0: ## for dealing with multiple Excel files fn = &quot;yourExcelFile_1.xlsx&quot; desc_fn = prd_dir + os.sep + fn cpfile(file, desc_fn) elif file_ind == 2: fn = &quot;yourExcelFile_2.xlsx&quot; desc_fn = prd_dir + os.sep + fn cpfile(file, desc_fn) elif file_ind == 1: ## Excel file need to be manipulated wb_raw = xl.load_workbook(file) fn_tgt = &quot;yourExcelFile_3.xlsx&quot; desc_file = prd_dir + os.sep + fn_tgt wb_tgt = xl.Workbook() ws_tgt = wb_tgt.active ws_tgt.title = &quot;Sheet1&quot; ## define work sheet name ws_tgt['A1'] = &quot;Date&quot; ## define the first column name ws_tgt['B1'] = &quot;ID&quot; ws_tgt['C1'] = &quot;Type&quot; ws_tgt['D1'] = &quot;Amt&quot; ws_tgt['E1'] = &quot;Info&quot; cur_sheet = str((datetime.today().day) - 1) ws_src = wb_raw.get_sheet_by_name(cur_sheet) 1 = 1 row_lt = [] for each in ws_src.rows: if each[0].value is not None: row_lt.append(i) i += 1 else: break num_max_row_raw = row_lt.pop() num_max_col = ws_tgt.max_column for i in range(2, num_max_row_raw+1): for j in range(1, num_max_col+1): c = ws_src.cell(row = i, column = j) ws_tgt.cell(row = i, column = j).value = c.value wb_tgt.save(desc_file) wb_tgt.close() wb_raw.close()if __name__ == &quot;__main__&quot;: xltemp(file, file_ind) pwdDecry_module.py123456789101112131415161718192021222324252627282930313233343536import win32com.client as win32import osimport os.pathfrom datetime import datetimeimport sysdef xlpwd(): ''' function xlpwd() is aiming to peel off attachment excel file password to be able to be ready by program arg: opt1_opt2 value: &quot;opt1&quot; to deal with one excel file; &quot;opt2&quot; to deal with another excel file ''' opt1_opt2 = sys.argv[1] excel = win32.Dispatch('Excel.Application') mon = '0' + str(datetime.today().month) ## suppose password is letters and 2 digits month combination if opt1_opt2 = &quot;opt1&quot;: pwd = &quot;randomletters&quot; + mon fn = &quot;yourExcelFile_1.xlsx&quot; sn = 2 ## sheet number st = &quot;tab name 1&quot; elif opt1_opt2 = &quot;opt2&quot;: pwd = &quot;otherrandomletters&quot; + mon fn = &quot;yourExcelFile_2.xlsx&quot; sn = 1 st = &quot;other tab name&quot; prd_dir = r&quot;\\\\your NAS UNC address&quot; file = prd_dir + os.sep + fn wb_tgt = excel.Workbooks.open(file,0,False,5,pwd) ## open encrypted Excel file wb_tgt.Password = &quot;&quot; ## remove password wb_tgt.Worksheets(sn).Name = st ## define the tagart worksheet order and name wb_tgt.Save() wb_tgt.Close() excel.Quit()if __name__ == &quot;__main__&quot;: xlpwd() readMail.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import os, os.pathimport sysfrom datetime import datetime, timedeltaimport win32com.client as win32from excelOp_mudule import xltempdef readMail(): ''' function readMail is aiming to read outlook email attachment and download them arg: lob, app, c_type value: lob == 'you line of business' and app == 'application name generate the source file' and c_type == 'business category' ''' lob = sys.argv[1] app = sys.argv[2] c_type = sys.argv[3] outlook = win32.Dispatch(&quot;Outlook.Application&quot;).GetNameSpace(&quot;MAPI&quot;) type1 = outlook.Folders[&quot;your email address&quot;].Folders[&quot;your inbox subfolder1&quot;] type2 = outlook.Folders[&quot;your email address&quot;].Folders[&quot;your inbox subfolder2&quot;] type3 = outlook.Folders[&quot;your email address&quot;].Folders[&quot;your inbox subfolder3&quot;] ## if your inbox subfolder name has convention you can use while loop inbox = outlook.GetDefaultFolder(6) ## Microsoft Outlook API number for inbox is 6 type1_msgs = type1.Items type2_msgs = type2.Items type3_msgs = type3.Items inbox_msgs = inbox.Items file_ind = 0 folder = &quot;&quot; if lob == &quot;finance&quot; and app == &quot;app1&quot; and c_type == &quot;consumer&quot;: mail_items, file_ind, folder = type1_msgs, 0, &quot;finance_files&quot; elif lob == &quot;marketing&quot; and app == &quot;app2&quot; and c_type == &quot;small business&quot;: mail_items, file_ind, folder = type2_msgs, 1, &quot;small business files&quot; elif lob == &quot;inventory&quot; and app == &quot;app3&quot; and c_type == &quot;cooperate&quot;: mail_items, file_ind, folder = type3_msgs, 2, &quot;cooperate files&quot; else: mail_items = None path = r&quot;\\\\your file staing folder directory&quot; + os.sep + folder num_mails = len(mail_items) lst_mails = list(reversed(range(num_mails))) id_mail = lst_mail[0] email = mail_items[id_mail] subject = email.Subject ## Outlook API Subject line object if file_ind in (0, 2): num = -13 ## depends on your own situation date_mail_str = subject[num:] if date_mail_str[0] != ' ': date_mail_dt = datetime.strptime(date_mail_str, &quot;%b. %d, %Y&quot;) else: date_mail_dt = datetime.strptime(date_mail_str, &quot; %b. %d, %Y&quot;) elif file_ind == 1 and datetime.today().strftime(&quot;%a&quot;) != &quot;Mon&quot; and datetime.today().day &lt;= 10: date_mail_str = subject[-8:-1] + '2020' date_mail_dt = datetime.strptime(date_mail_str, &quot; %B %d %Y&quot;) elif file_ind == 1 and datetime.today().strftime(&quot;%a&quot;) != &quot;Mon&quot; and datetime.today().day &lt;= 10: date_mail_str = subject[-8:-1] + '2020' date_mail_dt = datetime.strptime(date_mail_str, &quot;%B %d %Y&quot;) received_time = email.ReceivedTime ## Outlook API receive time object today = datetime.today() ## check availiability of the latest file if today.year == received_time.year and today.month == received_time.month and today.day == received_time.day: avail_ind = 1 else: raise AttributeError(&quot;the latest file is not available! check with business team&quot;) ## check if the file is right copy if avail_ind == 1 and (file_ind == 0 or file_ind == 1): val_dt = received_time - timedelta(days = 1) if date_mail_dt.day == val_dt.day and date_mail_dt.month == val_dt.month and date_mail_dt.year == val_dt.year: valid_copy_ind = 1 ## usually business file for current date is yesterday's data if data is daily basis else: raise AttributeError(&quot;file copy is not right! check with business team&quot;) elif avail_ind == 1 and file_ind == 2: val_dt = received_time if date_mail_dt.day == val_dt.day and date_mail_dt.month == val_dt.month and date_mail_dt.year == val_dt.year: valid_copy_ind = 1 ## sometime current date file is current date data depends on business process else: raise AttributeError(&quot;file copy is not right! check with business team&quot;) if valid_copy_ind == 1: attachment = email.Attachment.item(1) report_name = attachment.FileName os.chdir(path) input_file = os.getcwd() + os.sep + date_mail_str + report_name if not os.path.exists(input_file): attachment.SaveAsFile(input_file) xltemp(input_file, file_ind) if __name__ == &quot;__main__&quot;: readMail() Schedule jobs to auto check your inbox and execute python scriptsIf you use Linux as your local machine then there are so many scheduling tools such as crontab, for Windows users, you can use GUI tool Task Scheduler to to the automation scheduling task. In this case, we use Task Scheduler. simply follow the wizard to create local jobs to implement above readMail.py and pwdDery_module.py scripts. After decrypted Excel files push to your server then trigger server jobs so that make the whole data process automated, no more manual work.","link":"/2020/12/15/Automation-Process-for-Email-Attachment-Excel-in-Python/"},{"title":"Blocking Process Monitoring and Auto Email Notification in SQL Server","text":"From function perspective, to maintain a large scale business data warehouse is for making database system stable, robust and fast, which is a essential part to boost business team productivity and performance. However, for business users, the fundamental thing is data, so data can be delivered in high frequency and in time is the cornerstone for all business analysis and BI reporting. Obviously, the primary mandate for data management team is highly monitor ETL jobs to promise data process running well and smooth and never being blocked or corrupt by using process. In this article, I am going to talk about how to build up a ETL job monitoring system to watch user query automatically in designed frequency. To accomplish this task, we need Create a view to collect user query in real time Create a stored procedure to detect blocking in different situations Create another stored procedure to handle the notification email sending Create a console script to overall control the workflow to make clear with those step by a process workflow chart, we can easily to see the logic on behand the scene Firstly, SQL agent job is going to check blocking transaction every 30 minutes, if there are some user queries blocked ETL job execution service account, then check back database to determine if those transactions are new, if they are new then write into database Blocking_Transaction table and send email notification to data management team to raise awareness; but if those blocking are not new, update last_time and count column in table then calculate if count number reach to 4 if yes, then send email to user and data management team. Create a view to collect user query in real time The first and most important thing for us is get the all transaction records against database so that we are able to know which user query transactions would block service account conduct ETL job. We need to utilize sys schema tables or dmv (dynamic management views) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071use controlDB;gocreate view dbo.usrQueryMonioringaswith t1 as(select b.spid, c.DBName as [DB_Name] ,a.total_scheduled_time/1000 as TotalScheTime_SS ,a.total_scheduled_time/1000/60 as TotalScheTime_MM ,a.total_elapsed_time/1000 as TotalElapTime_SS ,a.total_elapsed_time/1000/60 as TotalElapTime_MM ,a.last_request_start_time ,a.last_request_end_time ,a.login_time ,a.login_name ,a.host_name ,a.program_name ,a.nt_domain ,a.nt_user_name ,a.status ,a.is_user_process ,b.blocked --add blocking query information to quick locate the blocking transactions ,b.cmd, ,b.memusage*8 as Memusage_KB ,b.memusage*8/1024 as Memusage_MB ,c.Query from master.sys.dm_exec_sessions a join master.sys.sysprocesses b on a.session_id=b.spid and a.login_time=b.login_time cross apply ( select from master.sys.dm_exec_sql_text(b.sql_handle) ) c where a.is_user_process=1 --consider spids for user only, no system spids and a.session_id!=@@SPID --don't include request from current spid),t2 as(select spid ,DB_Name ,TotalScheTime_SS ,TotalScheTime_MM ,TotalElapTime_SS ,TotalElapTime_MM ,blocked as BlockedBy ,last_request_start_tiem ,last_request_end_time ,login_time ,login_name ,case when host_name like 'your server name%' then 'Server' else 'Client' end as 'Host Name' ,case when program_name like 'Microsoft SQL Server Mangement Studio%' then 'SQL Query' when program_name like 'Python' then 'Python Access' else 'Server general job or data provider' end as 'Program Name' ,nt_domain ,nt_user_name ,status ,case is_user_process when 0 then 'SA System' when 1 then 'User Initiate' end as 'Is_User_Process' ,cmd ,Memusage_KB ,Query from t1)select *from t2;go Before moving forward, we need a middle table to store all service account blocking transaction records 1234567891011121314151617use controlDB;gocreate table dbo.blocking_transaction(TraceID int identity(1,1) not null ,spid smallint not null ,blokedBy ,login_name nvarchar(128) not null ,nt_user_name nvarchar(128) null ,TotalElapTime_MM int not null ,ps_time datetime not null ,fr_Time datetime not null --first record time ,lr_Time datetime not null --last record time ,counter smallint not null ,cmd nchar(16) not null ,query varchar(max) null) Create a stored procedure to detect blocking in different situationsNext we will apply the main logics to detect service account blocking transactions in terms of different scenarios when we have the query transaction records data. In this time, we need to create a stored procedure to be executed by SQL job. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586create proc dbo.usp_blocking_trans (@counter smallint output, @rowcnt int output)asif OBJECT_ID(N'tempdb..#blked') is not nulldrop table #blked;declare @row_cnt intdeclare @sSQL nvarchar(max)declare @sParam nvarchar(4)begin trywith t1 as(select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from controlDB.dbo.usrQueryMonioring where BlockedBy!=0 and login_name='your service account'),t2 as(select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from t1 except select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from t1 a join t1 b on a.spid=b.BlockedBy --remove duplicates),t3 as(select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from controlDB.dbo.usrQueryMonioring where spid in (select BlockedBy from t2) -- in operator to hold potential multiple transactions),t4 as(select * from t2 union all select * from t3)select *into #blkedfrom t4/*return 1 if no record in temp table*/if not exists(select * from #blked)beginprint 'no record in temp table'return 1end/*check for incremental load*/elseselect @row_cnt=count(a.spid)from #blked a left join controlDB.dbo.blocking_transaction bon a.spid=b.spid and a.last_request_start_time=b.ps_timewhere b.spid is null and b.ps_time is null;if @row_cnt&gt;=1beginset @sSQL=N'insert into dbo.blocking_transactionselect a.spid,a.BlockedBy,a.login_name,a.nt_user_name,a.TotalElapTime_MM,a.last_request_start_time,getdate(),getdate(),1,a.cmd,a.queryfrom #blked a'exec sp_executesql @sSQLselect @rowcnt=@@ROWCOUNT --output number of row insertedreturn 2end/*update the old records on last_record_time and count if counter&gt;=3*/elseselect top(1) @counter = counter from controlDB.dbo.blocking_transaction order by lr_time desc;update a set lr_time=getdate(),counter=@counter+1from blocking_transaction a inner join #blked bon a.spid=b.spid and a.ps_time=b.last_reqeust_start_time;if (select top(1) counter from blocking _transaction order by lr_time desc)%4 = 0return 3elsereturn 4drop table #blkedbegin catchdeclare @err_msg nvarchar(1000),@err_num int,@err_line int,@syserr nvarchar(1000)select @err_msg=ERROR_MESSAGE(),@err_num=ERROR_NUMBER(),@err_line=ERROR_LINE()SET @syserr=N'usp_blocking_trans ended with error: Line='+convert(nvarchar(3),@err_line)+', Error_Msg='+@err_msg+''return -1end catchend Create another stored procedure to handle the notification email sendingSending email to raise awareness is another important task for the entire monitoring system, in this case there are two server levels, if user query blocking time less than 4 (4*30=120 mins) only email to data management team, otherwise, email to both data management team and user to cancel the transaction. If user ignores then data management team would do something to clear the lock by policy. we use parameters output from stored procedure usp_blocking_trans as input to be the key conditions 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106use controlDB;gocreate procedure dbo.usp_blocking_Email( @case smallint, @times smallint, @rownum int --handle multiple blocking case)asbegin trydeclare @result intdeclare @runtime nvarchar(20)declare @subject nvarchar(500)declare @sSQL nvarchar(200)declare @sParam nvarchar(100)declare @spid nvarchar(4)declare @body nvarchar(max)declare @counter intset @runtime=convert(nvarchar,GETDATE())set @sSQL=N'select top 1 @bID=blockedBy from controlDB.dbo.blocking_transaction where nt_user_name=''your service account'' and blockedBy!=0 by tranceID desc'set @sParam=N'@bID nvarchar(4) output'exec sp_executesql @sSQL, @sParam, @bId=spid outputset @subject=N'Warning: ETL job being blocked on '+@runtime+N'by SPID'+@spid+N', check it out!'if @case=2set @body=N'&lt;p style=&quot;color:#000000;font-family: Georgia; font-size: 18px; line-height: 18px&quot;&gt;Hi Team,&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;color:#000000;font-family: Georgia; font-size: 18px; line-height: 18px&quot;&gt;A job is now being blocked by a user process with &lt;font color=&quot;#FF5733&quot;&gt;&lt;b&gt;SPIS='+@spid+'&lt;/b&gt;&lt;/font&gt;&lt;br /&gt;&lt;br /&gt;Please check [controlDB].dbo.[blocking_transaction] table.&lt;br /&gt;&lt;br /&gt;&lt;/i&gt;&lt;font size=&quot;4&quot;&gt;More details information shown on below&lt;/font&gt;&lt;br /&gt;&lt;br /&gt;Thanks and Best Regards,&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;'+N'&lt;style&gt; th { background: #33FFBD height: 30px; } table,th,td { border: 2px solid green; } table { width: 80%; color: black; text-align: center; }&lt;/style&gt;'+ N'&lt;h2&gt;&lt;center&gt;&lt;font color=&quot;red&quot;&gt;ETL Job Info for Blocking&lt;/font&gt;&lt;/center&gt;&lt;/h2&gt;'+ N'&lt;table align=&quot;center&quot;&gt;' + N'&lt;tr&gt;'+ N'&lt;th&gt;SPID&lt;/th&gt;&lt;th&gt;BLkdBY&lt;/th&gt;' + N'&lt;th&gt;User Name&lt;/th&gt;' + N'&lt;th&gt;Elapse Time in MM&lt;/th&gt;&lt;th&gt;Process Run Time&lt;/th&gt;' + N'&lt;th&gt;First Record Time&lt;/th&gt;&lt;th&gt;Last Record Time&lt;/th&gt;' + N'&lt;th&gt;Counter&lt;/th&gt;&lt;th&gt;CMD&lt;/th&gt;' + N'&lt;/tr&gt;' + cast(( select top (@rownum) td=[spid],'' ,td=[blockedBy],'' ,td=[nt_user_name],'' ,td=[TotalElapTime_in_MM],'' ,td=[ps_time],'' ,td=[fr_time],'' ,td=[lr_time],'' ,td=[counter],'' ,td=[cmd],'' from controlDB.dbo.blocking_transaction order by TranceID desc FOR XML PATH('tr'), TYPE ) as nvarchar(max)) +N'&lt;/table&gt;' +N'&lt;p&gt;&lt;br /&gt;&lt;/p&gt;' +N'your team name';elseif @case=3set @body=N'&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;color:#000000;font-family: Georgia; font-size: 18px; line-height: 18px&quot;&gt;Hi Team,&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;A ETL job is now still being blocked by a user process with spid='+@spdi+' for more than '+@times+' checks adn '+@times*30+' minutescheck previous email or controlDB.dbo.blocking_transaction table for more details!&lt;br /&gt;&lt;br /&gt;your team name'execute @result=msdb.dbo.sp_send_dbmail@profile_name=N'your SMTP profile name',@recipients = N'email1@yourcompany.com;email2@yourcompany.com',@copy_recipients = N'email3@yourcompany.com',@subject = @subject,@body = @body,@body_format='HTML';return 0end tryend Create a console script to overall control the workflowat the last step, we need a control script put all above stored procedure together to be able to schedule SQL Agent job. 1234567891011121314151617181920212223use controlDB;godeclare @result_status smallintdeclare @counter smallintdeclare @rowcnt intexec @result_status=usp_blocking_trans @counter=@counter output, @rowcnt=@rowcnt outputif @result_status=1beginprint('no blocking process')returnendelseif @result_status=4beginprint('no new blocking process')returnendelsebeginset @counter=@counter+1exec usp_blocked_Email @case=@result_status, @times=@counter, @rownum=rowcnt","link":"/2020/12/17/Blocking-Process-Monitoring-and-Auto-Email-Notification-in-SQL-Server/"},{"title":"Data Manipulation and ETL with Pandas","text":"Pandas is the most used library in Python to manipulate data and deal with data transformation, by leveraging Pandas in memory data frame and abundant build-in functions in terms of data frame, it almost can handle all kinds of ETL task. We are going to talk about a data process to read input data from Excel Spreadsheet, make some data transformations by business requirement then load reporting data into SQL Server database. Pandas functions brief introductionFirst, we use read_excel() function to read in input Excel file, then use slicing function iloc() or loc() to get the data which need to be proceeded. The difference between iloc() and loc() is iloc() use row and column index number to slice data but loc() is use column name instead of index number. If we need to rename the column, we use rename(column={},inplace=True/False) function to do that. We also can use drop() function to drop columns. If we want to re-order the sequence of columns, the reindex() will help us to do that. For the purpose of merging or joining multiple tables, we can use merge() function. we can use to_sql() function to write data into database table after transformations. If we want to create UDF (user defined function) to apply to Pandas data frame then we can use apply(UDF name) function. For SQL windows function, Pandas also has equivalent way on those kind of function such as Top N rows per group which is equivalent to row_number()over() sql window function. Use assign(().groupby().cumcount()).query().sort_values() function Pandas use cases to manipulate Excel data to load into SQL Server database1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import numpy as npimport pandas as pdimport pyodbcfrom datetime import datetimefrom sqlalchemy import create_enginedef py2mssql(): engine = create_engine(&quot;mssql+pyodbc://yourServerName/database?driver=SQL Server Native Client 11.0&quot;) con_str = pyodbc.connect(&quot;DRIVER={SQL Server Native Client 11.0}; SERVER=yourServerName; DATABASE=yourDBName; Trusted_connection=yes&quot;) return engine, con_strdef dfstaging(df, sql_cmd): df.to_sql('your target table',py2mssql()[0],if_exists='replace',schema='dbo',index=False) conn = py2mssql()[1] cur = conn.cursor() conn.commit() cur.close() conn.close()def rpt2mssql(): df_pdr = pd.read_excel(r'C:\\your input file path\\fileName.xlsx',sheet_name='Sheet1') df_pd1 = df_pdr.iloc[:,[0,1,2,3,4,5,6,8,9,10]] ## iloc[row,col in num] df_temp = df_pdr.loc[:,['col1','col2','col3']] ## loc[row,col in name] up_col = &quot;&quot;&quot; sql statement &quot;&quot;&quot; dfstaging(df_temp, up_col) df1=pd.read_sql_table('your target table', py2mssql()[0], schema='dbo', index_col=None) ## rename columns in data frame to match with target table df1.rename(column={&quot;user ID&quot;:&quot;User_ID&quot;, &quot;item Key&quot;:&quot;Item_Key&quot;, &quot;connect ID&quot;:&quot;Connect_ID&quot;},inplace=True) df1[&quot;Cust_Id&quot;] = np.nan up_custid = &quot;&quot;&quot; update sql statement &quot;&quot;&quot; dfstaging(df1, up_custid) df2 = pd.read_sql_table('your target table', py2mssql()[0], schema='dbo', index_col=None) ## drop data frame column df3 = df2.drop(columns=[&quot;Connect_ID&quot;,&quot;Item_Key&quot;]) ## join two data frames df_join = pd.merge(df_pd1, df3, left_index=True, right_index=True) df_join[&quot;Process_dt&quot;] = datetime.today().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) df_final = df_join.drop(columns=[&quot;Connect_Id&quot;,&quot;Item_Key&quot;]) ## loop through all columns need to be converted to datetime datatype cvt_lt = [&quot;Work_Queue_Datetime&quot;,&quot;Complete_Datetime&quot;,&quot;Current_StatementDate&quot;] for col in cvt_lt: df_final[col] = pd.to_datetime(df_final[col]) ## get duplicate record from column value df_final[&quot;Dup_Ind&quot;] = [1 if x == &quot;Dup Submission&quot; else 0 for x in df_final[&quot;Reason_Comment&quot;]] ## re-order the columns new_index = [&quot;col5&quot;,&quot;col3&quot;,&quot;col1&quot;,&quot;col2&quot;,&quot;col4&quot;] ## apply the new order df_final = df_final.reindex(columns=new_index) df_final.to_sql('your target table',py2mssql()[0], if_exists='append', schema='dbo', index=None) up_dup = &quot;&quot;&quot; sql statement &quot;&quot;&quot; dfstaging(df_final,up_dup) if __name__==&quot;__main__&quot;: rpt2mssql() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport pandas as pdimport pyodbcfrom sqlalchemy import create_enginedef split_dt(ef_dt): ef_day=(ef_dt//10000)%100 ## extract day info ef_mo=ef_dt//1000000 ## extract month info ef_yr=ef_dt%10000 return pd.Series({'dd': ef_day, 'mm': ef_mo, 'yy': ef_yr})def py2mssql(): engine = create_engine(&quot;mssql+pyodbc://yourServerName/database?driver=SQL Server Native Client 11.0&quot;) con_str = pyodbc.connect(&quot;DRIVER={SQL Server Native Client 11.0}; SERVER=yourServerName; DATABASE=yourDBName; Trusted_connection=yes&quot;) return engine, con_strdef df2staging(df, sql_cmd): df.to_sql('your target table',py2mssql()[0],if_exists='replace',schema='dbo',index=False) conn = py2mssql()[1] cur = conn.cursor() conn.commit() cur.close() conn.close()def df2db(df_type1,df_type2): engine = py2mssql()[0] df_type1.to_sql('your target table', engine, schema='dbo', if_exists='append', index=False) df_type2.to_sql('your target table', engine, schema='dbo', if_exists='append', index=False)def buz2df(): df = pd.read_excel(r'\\\\your input file path\\filename.xlsx', sheet_name='Sheet1', index_col=None) df1 = df.iloc[:,[0,2,3,4,5,8]] ## Pandas equivalents for some SQL analytic and aggregate function ## Top N rows per group which is equivalent to row_number()over() sql window function df2 = df1.assign(rn=df1.sort_values(['your order by column name'],ascending=True) .groupby(['your group column']).cumcount()+1).query('rn == 1').sort_values(['your group column']) df3 = df2['the date column you want to split'].apply(split_dt) df3 = df3.astype(str) ## convert data frame elements as string df3['the data column you want to split'] = df3['mm'] + '/' + df3['dd'] + '/' + df3['yy'] df3['the data column you want to split'] = pd.to_datetime(df3['the data column you want to split']) df2 = pd.concat([df2,df3['the data column you want to split']], axis=1) df_staging = df2.iloc[:,[0,1,5,7,8]] df_type1 = df_staging.loc[:['col1','col2','col3','col4']] df_type2 = df_staging.iloc[:,[2,3,4,5]] df2staging(df_staging) df2db(df_type1,df_type2)if __name__==&quot;__main__&quot;: buz2df()","link":"/2020/12/20/Data-Manipulation-and-ETL-with-Pandas/"},{"title":"Generate non comma delimiter CSV file","text":"This is some tip but sometime makes you life easier when you work with business team and deal with data from business input. Excel spreadsheet is kind of standard file format to communicate with business team, but in data manipulation side, it’s not a ideal input data format, technically, we usually convert spreadsheet to csv file. But default comma delimiter might cause some trouble because business data might contains quite a lot of , in attributes such as comments, suggestions, reasons etc. In order to better identify the column, non-comma delimiter should be used like | pipe. How do we generate the | delimiter csv file. There are two ways Change format setting system-wiseIn windows, open the control panel, find the Region setting, on the Formats tab click Additional setting, a pop-up window will show up, on that window, find the option list separator then type whatever delimiter you want to setup like |. ) save the setting, when you Save as csv file in excel, it will generate | separated csv file afterwards. Use database client tool to save query result to csv file with customized delimiterSome sql database client applications provide the functionality to save query result to csv file, like WinSQL. Before running your query, select execute to save to a text file, then select | from drop-down list","link":"/2020/12/07/Generate-non-comma-delimiter-CSV-file/"},{"title":"Hadoop Data Side Load from SQL Server","text":"Agile development and DevOps bring flexibilities and quick solutions to support business intelligent in timely manners. A technical platform and its associated applications and tools are able to turnaround very quick so that business analysts and data scientists would be able to leverage them to do the data modeling or machine learning, but in the other side, unlike functions buildup, data sync across different platforms is not that easy and quick especially for large organizations. Background and the gap of data modeling for Hadoop early adopterThese years, big data and Hadoop are kind of trend for next generation data technic. Many companies adopt that as major data platform, but the most of data is still allocated in RDBMS data warehouse, business intention is to leverage high quality data in SQL database to build their analytical work in Hadoop, the data consistency is the first consideration from data perspective, but it is not a easy task because data is going to migrate to different platform with different operating system (from Windows to Linux). Technically, the best solution for the project is the build the direct connection from SQL Server and SSIS to Hive by using Apache Sqoop or utilize the JVM to build JDBC connection by JAVA, but for large organization, applying a new tool on production needs a quite lot approve work; developing JDBC connection facility also needs multiple level testing, those are taking a long time. Therefore the solution is back to the foundation of the Hadoop - file system. Because SSIS cannot write to Hive directly using ODBC (before 2015 version). The alternative is to create a file with the appropriate file format and copy it directly to the Hadoop file system then use Hive command to write metadata to Hive metastore, the data will show up in the Hive table and also available in Cloudera Impala. File operation for data migration from SQL to HadoopIn our case, moving files can be a litter more complex because of crossing different operating system and platform as noted earlier. In this case, we need t a way to execute the dfs -put command on the remote server. Server tools enable us to execute the remote processes. Hadoop is build upon Linux, so bash shell script to execute the remote process. But that is quite challenged in real operation in production environment. What if the number of files is huge and cannot done by manually issuing command but have to do the batch operation automatically. What if files being moved crossing multiple platform from Windows to Linux local file system to Hadoop file system and if the data would be able to keep the original value and format in Hive or Impala table. Generate source data by CSV file from Windows sideFor multi-processing, SSIS is the sounds solution and tool set on Windows side which enables us to run SSH commands on the remote server from an Execute Process Task. Setting up a package to implement this process is relatively straight forward. Just set up a data flow as normal with a source component retrieving data from SQL Server data warehouse. Any transformation actions that need to be applied to the data can be performed. Ad the last step of the data flow, the data needs to be written to a file. The format of the file is determined by what the Hive system expects. The easiest format to work with from SSIS is a delimited format, with carriage return/line feeds delimiting rows, and a column delimiter like a comma(,) or pipe (|) separating column values. The SSIS flat file destination is designed to write these types of files. Automate file operation from Windows to Linux and HDFSOnce the file is produced, then use a file system task to copy it to a network location that is accessible to both SSIS server and Hadoop cluster. The next step is to call the process to copy the file into the HDFS. This is done through an Execute Process Task. It can be configured to use expressions to make this process more dynamic. In addition, if you are moving multiple files, it can be used inside a For loop in SSIS to repeat the process a specified number of times. To FTP data file from Windows to Linux, we can use either sftp or scp command but first you should make sure you have Hadoop access. But the problem is when we copy data to Hadoop, we need to provide the password to access the system, so the process is manual. As mentioned before, for larger scale data migration, we have to figure out the way of automation so that silent mode is needed to avoid manually provide user password then we can use script to schedule job to automate data process. Check OpenSSH Client installed and enable in your machineOpen Windows setting -&gt; App -&gt; Apps &amp; features -&gt; Optional feature, check if OpenSSH Client is on the list, if not you need to install that client tool On source side to create a public access keyThis step we need use cmd prompt or git bash, open git bash, change directory to home folder by issuing command cd $HOME, you will find the hidden folder .ssh, you can display all folder by command ls -la Issue below command to generate the public access key in order to enable the silent mode to Hadoop 1ssh-keygen -b 2048 -t rsa tap 3 time enter key to generate the key, the value for argument -b is key size and has to be the number at least 2048, value of -t is algorithm. You will find the 2 new files are generated, id_rsa stores your identification information and id_rsa.pub is your public key which we need to copy to Linux server. On target side FTP public key file to itWe can use scp command to upload public key to Linux server. 1scp -p 22 /home/yourUserName/.ssh/id_rsa.pub yourUserName@LinuxServerName:/home/yourUserName/.ssh/authorized_key You need to give password when issue above command to pass the authentication, but later this step will be bypass because of the public access key. Load file from Linux file system to HDFSEven Hadoop is installed on Linux, it has independent file operating system called HDFS, we need to issue command to transfer file between Linux and Hadoop. hdfs dfs command to manipulate Hadoop files, Hadoop is kind of remote server so we use -put argument to push file from Linux local to Hadoop; use -get to push file from Hadoop to Linux local, is this case we use put. 1hdfs dfs -put /yourLinuxFilePath/fileName.csv /HadoopFilePath/targetHiveDatabase.db Now, data file is in HDFS so that we can issue Hive command to write the table metadata into Hive metastore Configure Hive table metadata to match up with SQL ServerIn Hadoop side, we need to define Hive tables to store the data from SQL Serer. For keeping data consistency, we define the Hive table schema in terms of SQL table data type. We don’t use Ctrl-A(0x001) which is the default Hive column delimiter for flat file but use pipe bar(|) as field delimiter, because that isn’t supported well for use from SSIS in Window platform. Now let’s dig a litter deeper on Hive data type which is the critical element and the most important consideration to keep data consistent. Hive provides a layer on top of Hadoop data that resembles a RDBMS. In particular, Hive is designed to support the common operations for data warehousing scenarios. Thanks to Hive to build the bridge between Hadoop MapReduce and RDBMS so that many of these data types have equivalent values in SQL Server, but only a few are unique to Hive. Type Description Examples SQL Server Equivalent Float 4-byte single-precision floating point 25.189164 real Double 8-byte double-precision floating point 25.1897645126 float(53) Decimal A 38-digit precision number 25.1897654 decimal, numeric Boolean Boolean true or false TRUE FALSE bit Timestamp JDBC-compliant timestamp format YYYY-MM-DD HH:MM:SS:ffffffffff datetime, datetime2 define and create a Hive table is simple, the syntax just like SQL, but something different after all, it’s file system. 1234567891011121314create table yourTableName(col1 bigint, col2 int, col3 varchar(5), col4 decimal(5,2), col5 smallint, col6 tinyint, col7 timestamp, col8 float)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '|'STORED AS TEXTFILEtblproperties (&quot;skip.header.line.count&quot;=&quot;1&quot;) If data files come with header then you need to tell Hive skip the first row by tblproperties. We save above script as createTbl1.hql then use Hive beeline command to generate table 1beeline -f ./yourPath/createTbl1.hql &gt; /yourPath/hql.ot Load data file to Hive and Impala tableAs long as we push data file to Hadoop and Hive directory, we can easily to write data to Hive table by issuing beeline -e command like below 1beeline -e &quot;LOAD DATA INPATH '/HadoopFilePath/targetHiveDatabase.db/fileName.csv' INTO TABLE yourTableName&quot; the metadata will be in the Hive metastore. Cloudera Impala metastore is not sync with Hive automatically, so if you want to manipulate data in Impala, we need to issue impala command 12impala -q &quot;invalidate metadata yourTableName&quot;impala -q &quot;refresh yourTableName&quot; after that data is on both Hive and Impala table. Automate the whole processTo get a better understanding of how the process looks like, below workflow illustrates how data file read from SQL Server side and load into Hive table in HDFS cluster. now let’s create a script to bulk copy data file to Linux and save it as bulkcopy.sh 1scp -p 22 -r /yourDataFileFolder/ user@host:/home/user/dataFileFolder then for the task in Linux server side, we also create a script to push data from Linux local file system to HDFS then write data to Hive table and save it as bulkload.sh 1234hdfs dfs -put /yourLinuxFilePath/fileName.csv /HadoopFilePath/targetHiveDatabase.db/ &amp;&amp;beeline -e &quot;LOAD DATA INPATH '/HadoopFilePath/targetHiveDatabase.db/fileName.csv' INTO TABLE yourTableName&quot;impala -q &quot;invalidate metadata yourTableName&quot;impala -q &quot;refresh yourTableName&quot; so far, we have csv data file, bulkcopy.sh and bulkload.sh, now we will schedule a job to run those scripts automatically by using SSIS package last, configure the SSIS package to write log data either file or database for debugging. Both Hive table and Impala table will be good after run command invalidate metadata command in Impala shell for both HUE (GUI tool) and Linux server. Invalidate metadata command takes effect across both Impala shell and HUE web interface, but refresh command only tales effect on the environment the command run against. Data restatementIt’s quite common practice in production to reload historical data with reconcile with new or changed logic or even correct some mistake so remove partial data and then reload is also important. Than task is easy for RDBMS but what if the same situation take place in Hadoop, there is no update statement in Hive or Impala, so we need to do multiple steps: Start state: number of data, e.g. 3 files need to restate in HDFS Issue command hdfs dfs -rm to delete the 3 files with month_key (3 months) Run query from HUE, make sure 3 month data and won’t impact other month Run above scripts and SSIS package add new 3 data files back again, then the data would be back to the Hive and Impala table","link":"/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/"},{"title":"Linux Network Management Tool nmcli","text":"There are over 60 Linux networking commands you can utilize to do all the system network configurations, some of them are well known and widely used such as ifconfig, ip addr, traceroute, netstat and ping etc.., one command is very useful but relatively few being used, that is nmcli which is used for controlling your network, just like its name network manager and also can do all the thing to configure your network like displaying network device status, create, edit, activate/deactivate and delete network connection. syntax and optionsnmcli command has 2 arguments, one is option and the other one is object. 1nmcli [options] object {command | help} You can quick check help to get the information nmcli -h OPTIONS -t[erse] terse output -p[retty] pretty output -m[ode] tabular|multiline output mode -c[olors] auto|yes|no whether to use colors in output -f[ields] &lt;field1,field2,…&gt;|all|common specify fields to output -g[et-values] &lt;field1,field2,…&gt;|all|common shortcut for -m tabular -t -f -e[scape] yes|no escape columns separators in values -a[sk] ask for missing parameters -s[how-secrets] allow displaying passwords -w[ait] set timeout waiting for finishing operations -v[ersion] show program version -h[elp] print this help OBJECT g[eneral] NetworkManager’s general status and operations n[etworking] overall networking control r[adio] NetworkManager radio switches c[onnection] NetworkManager’s connections d[evice] devices managed by NetworkManager a[gent] NetworkManager secret agent or polkit agent m[onitor] monitor NetworkManager changes From command syntax, you can tell the options can be multiple but object is only one, because nmcli only return information or do configure for one object a time. the place of options and object can not be switched, must follow option first and object second. ExamplesShow network status1nmcli -p general status nmcli permission1nmcli general permission Enable and disable network1nmcli networking on | off | connectivity Radio wifi transmission control1nmcli radio wifi | wwan | all Show local network connection1nmcli connection show Show network device status1nmcli device status Show overall network and device information1nmcli device show that command will show device, device type, network connection, gateway, route, ip4, ip6, dns Show wifi connection status1nmcli device wifi list Configure wifi connections for UbuntuBy default, Ubuntu wifi connection is disabled, so if you want to use wifi, something need to be done to be able to connect to WAN. Firstly, show your wifi device 1nmcli device status Secondly, turn on the wifi radio transmission 1nmcli radio wifi on Thirdly, show all your wifi networks 1nmcli device wifi list Finally, connect your wifi network by password 1nmcli device wifi connect &lt;your wifi network name&gt; password &lt;your password&gt;","link":"/2020/12/14/Linux-Network-Management-Tool-nmcli/"},{"title":"How to let local images display on your hexo blog website","text":"Hexo blog framework friendly supports markdown which is the most popular syntax for technical blog writing. When you try to illustrate some ideas, it’s common to use screenshot photos in your articles to get reader better understanding so that you have to insert images into your blog, unlike typing text content that is straight forward, inserting images and deploying them to public blog site might be some tricks there, let’s see some common scenarios on images display issue and how to resolved them. Use absolute path to insert imageThe intuitive way for junior blogger is referencing absolute path + file name, I would say there is no any problem if the blog is only for local review or your local machine is a web server to host your blog website, otherwise, your images won’t be displayed on your public blog site after deployment if without any configuration. Like below shows when you use that way inert on your local blog, images can be displayed as expected but it will cause issue on display when you deploy your article to public blog website like below shown Use relative path based on _config.yml fileActually, hexo blog is powered by Node.js in the backend, its general configuration file _config.yml shows the way to properly allocate your resources including image, css, font and js etc. By default, your images should be put into blog/themes/your theme name/source/images , when you reference it, use below syntax no matter you use Windows or Linux, 1![your image's name](/images/your_image_file_name.png) now, you can push your blog to public blog site and your images can be presented there well but the problem is you lose your sight on local but only are able to see on blog website after you deployment, that is not you really want like below shown Use online image management toolTo be able to solve image display problem on both local and public site, the best way is utilize online image management website to upload your images then it will generate corresponding URLs for those photos, you can replace path name of your local images by URL. Now your image is going to be rendered by both your local markdown editor and web browser. Another advantage of managing your photos by online image tool is your images are in the cloud with permanent URLs so that you won’t lose them even something bad happened on your local machine. I use wailian.work to manage my images, it’s free and provide markdown link for each of your uploaded picture. A small thinking about Node.js top route design and file renderThe root cause of confusion here might be the Node.js route design, unlike Apache and Nginx web server which come with web container so that the route can be your local folder path name and render your local html file in the folder, in this case, you do tell the file local path from URL address, but for Node.js, it doesn’t have web container, therefore, we have to design route for local web files, in other words, you are not able to tell the local file path by URL address for Node.js web server. Let’s see an example 123456789101112131415161718const fs = require('fs');const path = require('path');const url = require('url');exports.static = function (request, response, staticPath) { let pathname = url.parse(request.url).pathname; pathname = pathname == '/' ? '/index.html' : pathname; let extname = path.extname(pathname); if (pathname != '/favicon.ico') { fs.readFile('./' + staticPath + pathname, async (err, data) =&gt; { if (!err) { let mime = await getFileMime(extname.split('.')[1]); response.writeHead(200, { 'Content-Type': `${mime}; charset=&quot;utf-8&quot;` }); response.end(data); } }) }} Assume user request images, the URL is probably like https://yourdomainname/images, but in the backend, in fact the Node.js read file from path './' + staticPath + pathname such as /root/web_project1/static/images. URL address doesn’t reflect file path, they are relatively independent in Node.js, by knowing that, we are going to dig a little deeper to take a look at _config.yml file and come up with idea why browser is able to render out to image file but text editor can’t. Open _config.yml file 1vim ./blog/_config.yml 12345# URL## If your site is put in a subdirectory, set url as 'http://example.com/child' and root as '/child'url: https://hermanteng19.github.ioroot: /permalink: :year/:month/:day/:title/ here list the route on your blog site which should be https://hermanteng19.github.io/ as home page and corresponding html file should be put into “/“ directory, what about images file? It is going to be “/images”, so far, we know the directory in below actually is route or web request URL, that is why text editor is not able to find the real file path. 1![mysql-connection](/images/mysql_conn_established.png) But, you might still confuse about why browser can render the image file, now let’s continue to see what’s going on web server. Enter your local hexo blog path 1ls -l ./blog you can find there is public/ folder which is your blog website “/root” folder, all web files such as html, css, js, image, font are in there, let’s enter it and take a look images/ folder is on the list, continue to drill down, mysql_conn_established.png file is on the list. So now you may be clear by Node.js route design, when web browser visits images on your blog website, it actually read the files from “/public/images/“, not from your local file absolute path like “themes/your_theme_name/source/images”. It does cause some confusion by this special type of route design, but in the other way, I would say it’s quite smart, you can design out a very nice, neat and beautiful URL for your website, which would bring very good user experience and impress website visitors. Think about no matter how ugly your local file path is like /root/jfdioaw/_jfidosjo_jfi123/oneMoreFolder/13u8030/product_1.html, but your website URL always looks like http://yourdomainname/business/product/product_1.html. At last, I want to thank CodeSheep for help me with the idea of leveraging online photo mangement tool to solve this problem!","link":"/2020/11/27/How-to-let-local-images-display-on-your-hexo-blog-website/"},{"title":"Oracle client side configuration for 12C","text":"The client side need to do some configurations after Oracle 11g upgrade to 12C on Server in order to make database server is connectable. Before starting to configurate your clients, you have to get the below new server information from DBA Host name Port number Service name Your user name(usually it won’t be changed and replicated from old version) Password(initial password for test connection then you need to update it) and then you need to make a new connection strings to add it into ORA file(*.ora) Oracle SQL Developer configurationThe simplest way to connect to oracle 12C is by using Oracle sql client tool SQL Developer, it uses build-in JDBC driver to make connections and GUI connection wizard guide you fill in the server information without updating ora file. It’s better use version 19.1 and above. fill in all info in connection window, then click Test button to test connection Other client tool connection by ODBC driver such as SSIS and WinSqlThe first step, you need to install the ODBC driver, there is another client tool comes with ODBC driver called Oracle Database Client12cSQLDev 12.1.0.2 R02, install that in your local machine, then update your environment variable to make sure Oracle home directory being added into it. Update environment variableIf you installed both Oracle SQL Developer and Client12cSQLDev 12.1.0.2 R02, there is going to be 2 Oracle folders, one is client_32 which is 32 bit, the other one is client_1 which is 64 bit application, you need to add both of them into you environment variable. The path is usually C:\\app\\product\\12.1.0\\client_* Configurate ora file after setup oracle environment variableThe tricky thing is the ODBC driver only works well for 32 bit version not 64 bit so we can only edit the ora file for client_32. You can find out the ora file on below directory C:\\app\\product\\12.1.0\\client_32\\network\\admin\\tnsnames.ora append following string to the ora file save andclose the ora file, then go to windows ODBC data source center to finalize ODBC driver configuration. Configurate ODBC driver and DSN to test connection Data Source Name is customizable; TNS Service Name is the connection string name in ora file CAP12CPRD_32bit in this case; fill in user ID then click test connection button, a prompt window will pop out to let you input password. SSIS package connection manager configurationOpen SSIS client SSDT to configure connection manager for Oracle data source. SSDT -&gt; open a project -&gt; right click data source on right side panel -&gt; follow the wizard both preinstalled .net provider and native ole db provider for Oracle are working well server name is as the same as connection string name in ora file in this case is CAP12CPRD_32bit. Give a name for oracle new data source then connection manager can be created to Oracle database. SAS connection to Oracle 12C prerequisite: SAS grid server local DSN need to be created before testing; PC SAS login server is necessary SAS grid local DSN for Oracle 12C (ask for system admin create local DSN and return the name to you and it is going to be the value for path when you create new library for Oracle 12C) SAS EG connection to 12C: open SAS EG 7.1 and make sure it connects to grid server open a new code window and create a new library by running below statement 1libname ora12C oracle user=&quot;yourUserName&quot; password=&quot;xxxx&quot; path=sas_grid_local_DSN schema=oracle_schema make sure the program run against SASApp rather than localhost new Oracle library would be created under the SASApp dataset list","link":"/2020/12/11/Oracle-client-side-configuration-for-12C/"},{"title":"Python Environment Setup for Implementation","text":"Python is a good scripting language to boost your productivity on data analysis and BI reporting. As open source language, you can easily get the binary installation file from python official site for windows and source code on vary versions for Linux, in production, it’s better choose installation approach by source code. We also need to setup python environment after installation so that we can not only use python interpreter to develop but also make it executable by CLI and even some ETL tool such as Microsoft SSIS. Python environment variable configuration and local folder set up for your file, package and libraryIf python is installed in system-wise, then you need to create some new folders to store you python file, package and library, e.g. python install path is “D:\\Python36&quot;, then you need to add python executable interpreter to be a part of the PATH variable. Next create python environment variable PYTHONPATH with the following paths and create empty file __init__.py file in each of these folders: create a new folder under D drive “D:\\pyLib” and set that directory as value of PYTHONPATH and create __init__.py file in “D:\\pyLib” you can also create subfolder to assign different permissions for different user group create a subfolder “D:\\pyLib\\AD-group1” and create the __init__.py file in it. create a subfolder “D:\\pyLib\\AD-group2” and create the __init__.py file in it. For Linux, if you install python3 by source code and directory is /usr/local/python3, then edit ~/.bash_profile file, append the python directory into PATH 12# Python3export PATH=/usr/local/python3/bin/:$PATH then run source ~/.bash_profile let setting take effect if your system pre-installed python2 then it’s necessary to make a soft link 12ln -s /user/local/python3/bin/python3 /user/bin/python3ln -s /user/local/python3/bin/pip3 /user/bim/pip3 setup name space and package python scripts for development project to be able to importable create or edit environment variable and add your python files folder into your system directory enter your python file folder to create an empty file __init__.py file open terminal prompt type python to active python interactive console import sys execute sys.path to make sure your python file folder is recognizable by python Python readiness test in localhost for SQL database connection (Anaconda virtual environment)First check python and Ipython version by issue command python --version and ipython --version. Anaconda almost pre-installs all python prevailing and popular libraries in its virtual environment, to check library list by using command pip list Python and SQL database connection facility with supported driversDepends on what python library do you install for database connectivity, it usually comes with function to show you available drivers to connect python to your database, e.g: pyodbc, use drivers() function to list the odbc drivers Python SQL Server database connection and data taskSQL Server database can be connected both by DB API (pyodbc) and ORM (sqlalchemy), create a py script and run sql query from user input 123456789101112131415161718192021222324252627282930313233343536373839import pyodbcimport sqlalchemyfrom sqlalchemy import create_engineimport pandas as pddef pyquery(conn): cnxn = pyodbc.connect(conn) cur = cnxn.cursor() sql_cmd = input(&quot;Input your sql command with database name: &quot;) result = cur.execute(sql_cmd) for row in result: print(row) cur.close() cnxn.close() def py2mssql(): way2conn = input(&quot;Do you want to connect to db by [ORM] or [DBAPI]: &quot;) if way2conn.upper()==&quot;DPAPI&quot;: dsn_str = input(&quot;Do you want to connect to db by [connection string] or [DSN]: &quot;) if dsn_str.upper()==&quot;DSN&quot;: dsn_name=input(&quot;Please enter your DSN name: &quot;) conn_dsn='DSN{};Trusted_connection=yes'.format(dsn_name) conn=conn_dsn pyquery(conn) else: conn_str='DRIVER={SQL Server Native Client 11.0}; SERVER=YOURSERVERNAME;DATABASE=YOURDB;Trusted_connection=yes' conn=conn_str pyquery(conn) else: ser_name=input('Please enter your server name: ') db_name=input('Please enter your database name: ') tbl_name=input('Enter the table name: ') orm_dict={'servername':'{}'.format(ser_name), 'database':'{}'.format(db_name), 'driver':'driver=SQL Server Native Client 11.0'} engine=create_engine('mssql+pyodbc://'+orm_dict['servername']+'/'+orm_dict['database']+'?'+orm_dict['driver']) df=pd.read_sql_table(tbl_name, engine, index_col=0, schema='dbo') print(df.head(10)) if __name__==&quot;__main__&quot;: py2mssql() script both can be ran directly or imported (recommend) after setup PYTHONPATH variable in your account and copy that script over to the path of environment variable. Python DB2 database connection and data taskWe can only connect to DB2 by DBAPI (pyodbc), connection string doesn’t work but only DSN (PRD1 was setup as system DSN in local and server) plus user id and password. Use below script to try to connect 123456789101112131415161718192021222324252627import pyodbcimport getpassimport pandas as pddef py2edw(): uid=getpass.getuser() print(&quot;Your user id is '{}'&quot;.format(uid)) pwd=getpass.getpass(&quot;Please enter your db2 password: &quot;) dsn=input(&quot;Please enter your db2 dsn name: &quot;) conn_dsn='DSN={0}; UID={1}; PWD={2}'.format(dsn, uid, pwd) conn=pyodbc.connect(conn_dsn) cur=conn.cursor() sql_cmd= ''' select * from table ''' result=cur.execute(sql_cmd) for row in result: print(row) cur.close() df=pd.read_sql_query(sql_cmd, conn) print(df.info()) print(df) conn.close() if __name__==&quot;__main__&quot;: py2edw() one thing need to be aware is the script better runs on the shell than python interactive console because pyQT doesn’t support password masking Productionize Python script by passing in parametersIn this section we will demonstrate how you can parameterize your code in python or pyspark so that you can use these techniques before deployed your script into production for automation. It’s best practice to parameterize database names, tables names, and dates so that you can pass these values as inputs to your script. This is beneficial when writing code for values that are dynamic in nature, which can change depending on the environment and/or use case. The key module is from python standard library: sys. Assign variables through sys.argv[...] 1234567891011121314151617import pandas as pdfrom sqlalchemy import create_engineimport sysdef py2mssql(): ser_name=sys.argv[1] # sys.argv[0] is assigned to python script itself, all other parameters start from 1 db_name=sys.argv[2] tbl_name=sys.argv[3] orm_dict={'servername':'{}'.format(ser_name),'database':'{}'.format(db_name), 'driver':'driver=SQL Server Native Client 11.0'} engine=create_engine('mssql+pyodbc://'+orm_dict['servername']+'/'+ orm_dict['database']+'?'+orm_dict['driver']) df=pd.read_sql_table(tbl_name, engine, index_col=0, schema='dbo') df.to_sql('pandas_sql_test', engine, schema='dbo', if_exists='replace', index=False) if __name__==&quot;__main__&quot;: py2mssql() run python script in CLI (command line interface) by following parameter values 1ipython py2mssql_argvdf.py yourSvrNm yourDBNm yourTblNm Encapsulate into SSIS to minimize change in production deployment – python interpreterAn alternative way to apply python in production is leverage current SSIS package and embed python script in process task you can hard code the configuration or use expression (VB) through variables in package Encapsulate into SSIS to minimize change in production deployment – batch processuse batch process by .bat file also can achieve that task Call user defined module or function in python scriptIt’s very efficient to create bunch of generic module packages to contain functions to be used widely by other python scripts for specific tasks. 1. Firstly, setup python environment variable to include directories which are recognized by python 2. Create __init__.py file (can be empty) in these folders 3. Create python programs and save script should end up with if __name__==”__main__“: main() 4. Ready to import user modules and functions 1234567import pandas as pdimport py2mssql_module as dbimport systbl_name=sys.argv[1]df=pd.read_sql_table(tbl_name, db.py2mssql('serverName','databaseName'), index_col=0, schema='dbo')print(df.head(10))","link":"/2020/12/08/Python-Environment-Setup-for-Implementation/"},{"title":"Linux network auto boot and restart","text":"Centos 7 network is disabled by default after installation and initialization, which causes network connection can not be made until you manually turn on it That is such annoying when you usually use SSH tool remote connect to centos workstation or server, so it’s quite necessary to turn on the network automatically every time reboot machine or VM. To accomplish that, we need to modify the network configuration file. Auto Boot NetworkVIM open config file: /etc/sysconfig/network-scripts/ifcfg-ens331vim /etc/sysconfig/network-scripts/ifcfg-ens33 Revise the ONBOOT value to be “yes” Save and quit config file by :wq vim commandRestart NetworkTo be able to make that change taking effect, network needs to be restarted by below command 1systemctl restart network.service something trick here is some blog articles mention that part is only systemctl restart, which won’t work if there is no network.service, I confuse and spend many time to figure out that, thanks to CodeSheep whose video shed the light on it and help me to solve that problem.","link":"/2020/11/25/Linux-network-auto-boot-and-restart/"},{"title":"Remote Connect MySQL Server from Client Machine Setup","text":"You can only connect to MySQL Server from localhost after MySQL installation by default, but in production, all MySQL clients remotely connect to server, for simulating real production environment in your home network, some configurations need to be made to be able to let you connect MySQL from client machine other than localhost. Revise or create MySQL configuration file (RHEL or Centos 7)Modify or create /etc/my.cnf file1vim /etc/my.cnf add a configuration item bind-address and let it value to be your MySQL server host ip address (eg. 192.168.1.114)1bind-address=192.168.1.114 save and exit1:wq Restart MySQL service1systemctl restart mysql.service Open TCP port 3306 using iptablesSetup /sbin/iptables and let firewall opens port on 3306 for any remote machine1/sbin/iptables -A INPUT -i eth0 -p tcp --destination-port 3306 -j ACCEPT To specific client host machine to access port 3306, you can explicitly assign ip address (eg. 192.168.1.134)1/sbin/iptables -A INPUT -i eth0 -s 192.168.1.134/24 -p tcp --destination-port 3306 -j ACCEPT Finally save IPv4 firewall rules1/sbin/iptables-save &gt; /etc/sysconfig/iptables If it doesn’t work, for testing and develop environment, you can turn off firewall12systemctl stop firewalld.servicesystemctl disable firewalld.service Grant remote access to new MySQL databaseCreate a new database1create database foo; Grant remote user access to a specific database on user host machine (192.168.1.134)1grant all privileges on foo.* to herman@'192.168.1.134' identified by 'youOwnPasswd'; Grant remote access to existing MySQL database for user (herman) on its host machine (eg. 192.168.1.134)Require a set of two commands12update db set Host='192.168.1.134' where Db='mysql';update user set Host='192.168.1.134' where user='herman'; Open MySQL client tool on workstation with address 192.168.1.134 like MySQL workbenchCreate new connection using username and passwordusername: hermanpassword: youOwnPasswdport: 3306 Connection is created and foo database is on the list under user herman","link":"/2020/11/25/Remote-Connect-MySQL-Server-from-Client-Machine-Setup/"},{"title":"Sandbox solution for BI reporting and business analytics","text":"At beginning, I’d like to share a story from my client and business partner. One day, my team worked on a big marketing project, data from all kinds of source like spreadsheet, csv, mainframe and SQL Server, we had to do cross reference all those data to generate analysis reports but the headache thing was they were isolated and no way to put them into a single unique environment to run one time query then return the results so we could only open multiple windows to compare them by eyeballs. During the project, we often composed some complex queries then ran for a long time to return the result dataset, those datasets were quite important for future further analysis and share with the whole team, but the another panic thing was we could not save those dataset into database due to insufficient access so what we did was copy and paste everything in excel spreadsheet, after for a while, we found number of excel file explode and hard to find the report among those huge files, we feed up with the tedious work and decided created a bunch of views in database but that was also not controlled by us but infrastructure team, all we could do was submit the request then followed infrastructure team’s schedule and waited for month end deployment, no matter how urgent those reports would be. That is the story, I think if you had ever experienced that, that solution might be right for you. You might get the common points from above story, it’s inefficient and even painful if you can only leverage data from data warehouse tables, that is reason why the sandbox database comes up which is a brand new data play zone on production with ability of pipeline to bring multiple sources of data based on major data warehouse you are using. In a brief mark, sandbox is aiming to build a homogeneous solution for heterogenous environment. How to build up and what is the foundation of sandbox database, now let’s take a look into it. The Major data source is so called big db, but it’s data warehouse so that user was only assigned read access which means you can do nothing but only select and query data. Now we carved out two new databases - sandbox and control db. Sandbox database brief introductionsandbox is the new data loading zone for slicing and dicing data in production, like you own backyard playground, you can do almost anything you want to do with it, so user will be granted both read and write permission. control_db that is control console station to provide access gateway for sandbox and also monitor and logging the users’ behaviors, just like a guardian to promise the safety and healthy for sandbox. Let’s take a closer look at these two databases. Sandbox, in general, user can perform 3 kinds of actions, DDL, DML as well as batch job execution based on stored procedure. With DDL command which is database define language, user can create table/view/stored procedure, alter them, even drop them. With DML command which is database manipulate language, user can insert/update/delete data from existing table and also import data from other source. The batch operation is fit for user with programming background and write multiple queries with logical sequence into it like conditional and looping function by using T-SQL scripting language in terms of stored procedure. What things should be considered beforeA very critical point for sandbox database is object ownership, which means you can only create and deal with your own database objects plus read others’ object. Image you data or analytical work were deleted by other accidently, how would you feel at the moment, so the restriction must be setup along with the sandbox database creation to promise the user data safety. Another important thing need to be considered before is user data volume control. Unlike data warehouse, data volume increasing is followed trend and stable for each year, but sandbox database size is unpredictable and totally depends on user personal behavior, in order to prevent non-critical user data dominants your server hard disk, it’s very necessary to set quota for each users. When reach to the limit then User won’t be allowed to create table User also won’t be allowed insert new data into existing table until manually clean data and release space. Creating sandbox database walk throughDefine and create a new database12345use master;goif DB_ID('sandbox') is not nulldrop database sandboxgo in real production, you might need to create a new file group and bunch of file under it for better management. 12345678910create database sandboxon primary(name=N'sandbox', filename=N'G:\\sqldata\\sandbox.mdf', size=1048kB, maxsize=5124KB, filegrowth=512KB),filegroup [yourTeam_sandbox](name=N'yourTeam_sandbox_FG1',filename=N'G:\\sqldata\\yourTeam_sandbox1.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB),(name=N'yourTeam_sandbox_FG2',filename=N'G:\\sqldata\\yourTeam_sandbox2.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB),(name=N'yourTeam_sandbox_FG3',filename=N'G:\\sqldata\\yourTeam_sandbox3.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB)log on(name=N'yourTeam_sandbox_log',filename=N'H:\\sqllog\\yourTeam_sandbox_log.ldf',size=1048KB, maxsize=5124KB, filegrowth=512KB)go make sure your sandbox and control console database have the same database ownership user 12345678use master;goalter authorization on database::sandboxto sagoalter authorization on database::control_dbto sago make sure your new created file group to be the default file group so that coming data is going to be allocated there 12345use sandbox;goif not exists (select name from sys.filegroups where is_default=1 and name=N'sandbox')alter database sandbox modify filegroup [yourTeam_sandbox] defaultgo Grant user permissionsFirst, you need to provide the basic read access to your business user 1234567use sandbox;goif exists (select * from sys.database_principals where name=N'yourDomain\\yourUserGroup')drop user [yourDomain\\yourUserGroup]elsecreate user [yourDomain\\yourUserGroup] for login [yourDomain\\yourUserGroup] with default_schema=dboexec sp_addrolemember 'db_datareader','yourDomain\\yourUserGroup'; then assign to them DDL and DML permissions 1234567891011121314151617181920use sandbox;gogrant alter any schema to [yourDomain\\yourUserGroup];gogrant create procedure to [yourDomain\\yourUserGroup];gogrant create table to [yourDomain\\yourUserGroup];gogrant create view to [yourDomain\\yourUserGroup];gogrant delete to [yourDomain\\yourUserGroup];gogrant insert to [yourDomain\\yourUserGroup];gogrant select to [yourDomain\\yourUserGroup];gogrant update to [yourDomain\\yourUserGroup];gogrant execute to [yourDomain\\yourUserGroup];go Create database level trigger on DDL query control for rules and restrictions encourage give table name starting with id number which is unique identifier for employee in your company like nameInitial2_tableName must be eligible user, identity check by control database no special characters are allowed in table name like !@#$%^&amp;- no over quota allowed not allowed user drop or delete others’ object and data let’s connect above constraints with workflow chart to be able to see the big picture First of all, we create the database trigger for enforcing object creation rules 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163create trigger ddltrg_create_tableon databasewith execute as 'dbo'for create_tableasbegin try set noncount on; declare @eventdata xml declare @orig_object_name varchar(100) declare @orig_user_name varchar(25) declare @alt_object_name varchar(100) declare @alt_user_name varchar(25) declare @valid_chars varchar(50) declare @create_status char(1) declare @create_err_msg varchar(250) declare @rename_required char(1) declare @trigger_sql varchar(max) declare @schema_name varchar(25) declare @trigger_insert varchar(max) declare @guid varchar(32) declare @overQuota_user table (userId varchar(25) null) set @guid='250' set @valid_chars = '%[^a-zA-Z0-9_]%' set @eventdata = EVENTDATA() set @orig_object_namne = @eventdata.value('(/EVENT_INSTANCE/objectName)[1]','nvarchar(100)') set @orig_user_name = upper(@eventdata.value('(/EVENT/INSTANCE/LoginName)[1]','nvarchar(25)')) set @create_status='Y' set @create_err_msg='' set @alt_user_name = right(@orig_user_name,len(@orig_user_name)-charindex('\\\\',@orig_user_name)) select @schema_name = SCHEMA_NAME(SCHEMA_ID) from sys.tables where name = @orig_object_name if (@alt_user_name = 'yourServiceAccout') return if substring(@orig_object_name,1,len(@alt_user_name)) = @orig_object_name begin set @alt_object_name = @orig_object_name set @rename_required = 'N' end else begin set @alt_object_name = @orig_object_name set @rename_required = 'Y' end /*check the user eligibility*/ if not exists (select * from control_db.dbo.Audit_Sandbox_User where User_ID = @alt_user_name) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' does not have privilege to use the sandbox database' end insert into @overQuota_user select user_id from control_db.dbo.Audit_Sandbox_User where sandbox_limit - current_usage &lt; 0 /*user control for over limit quota*/ if @alt_user_name in (select userId from @overQuota_user) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' exceeded designed space quota, ' + @alt_user_name + ' is not able to ' + N'create new table. Please delete data not needed' + N'to free up space so usage is less than quota 1024MB and rerun query again' end /*check object name eligibility*/ if (patindex(@valid_chars,@orig_object_name) &gt; 0) begin set @create_status = 'N' set @create_err_msg = 'Invalid characters in table name' end /*check object existency*/ if exists(select * from sys.objects where object_id=OBJECT_ID(@alt_object_name) and type in (N'U')) begin if @rename_required = 'Y' begin set @create_status = 'N' set @create_err_msg = 'Table: ' + @alt_object_name + ' already exists' end end /*check schema name eligibility*/ if @schema_name &lt;&gt; 'dbo' begin set @create_status = 'N' set @create_err_msg = 'Table: ' + @orig_object_name + ' not created. Schema is not specified: dbo.&lt;table name&gt;!' end if (@create_status = 'N') begin rollback print @create_err_msg end /*log sandbox event into tracking table in control_db*/ insert control_db.dbo.Audit_Sandbox_Event_Tracking (Event_Type, Event_Time, Event_User, Event_Database, Event_Object_Name ,Event_Object_Type, Event_SQK, Audit_Created_Status, Audit_Error_Message) value (@eventdata.value('(/EVENT_INSTANCE/EventType)[1]','nvarchar(50)'), @eventdata.value('(/EVENT_INSTANCE/PostTime)[1]','datetime'), @Orig_user_name, @eventdata.value('(EVENT_INSTANCE/DatabaseName)[1]','nvarchar(25)'), @orig_object_name, @eventdata.value('(/EVENT_INSTANCE/ObjectType)[1]','nvarchar(25)'), @eventdata.value('(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]','nvarchar(max)'), case when @create_status = 'Y' then 'success' else 'failed' end, @create_err_msg ) if (@create_status = 'N') return /*rename object to add user id prefix*/ begin exec sp_rename @orig_object_name, @alt_object_name print 'Table name is renamed with prefix of your user id' end /*creaet DML trigger on table level*/ set @trigger_sql = 'create trigger DML trig_'+@alt_object_name+ 'ON ' + @alt_object_name + '' set @trigger_sql = @trigger_sql + 'for delete, update, insert' set @trigger_sql = @trigger_sql + 'as' set @trigger_sql = @trigger_sql + 'begin ' set @trigger_sql = @trigger_sql + ' set nocount on;' set @trigger_sql = @trigger_sql + ' declare @user varchar(25)' set @trigger_sql = @trigger_sql + ' select @user = right(SUSER_NAME(),LEN(SUSER_NAME())' set @trigger_sql = @trigger_sql + ' - CHARINDEX(''\\\\'',SUSER_NAME()))' set @trigger_sql = @trigger_sql + ' IF @user &lt;&gt; '''+@alt_user_name+''' ' set @trigger_sql = @trigger_sql + ' begin' set @trigger_sql = @trigger_sql + ' rollback' set @trigger_sql = @trigger_sql + ' print ''User: ''+@user+'' does not have the privilege to perform a DML operation on table' +@alt_object_name + ''' ' set @trigger_sql = @trigger_sql + ' end' set @trigger_sql = @trigger_sql + ' end' exec (@trigger_sql) ; /*create over quota DML trigger on table level*/ create table #overQuota (userId varchar(25) Null) ; set @trigger_sql = 'create trigger DML trig_'+@alt_object_name+ '_insert ON ' + @alt_object_name + '' set @trigger_sql = @trigger_sql + 'after insert' set @trigger_sql = @trigger_sql + 'as' set @trigger_sql = @trigger_sql + 'begin ' set @trigger_sql = @trigger_sql + ' set nocount on;' set @trigger_sql = @trigger_sql + ' declare @user varchar(25)' set @trigger_sql = @trigger_sql + ' select @user = right(SUSER_NAME(),LEN(SUSER_NAME()) - CHARINDEX(''\\\\'',SUSER_NAME()))' set @trigger_sql = @trigger_sql + ' insert into #overQuota' set @trigger_sql = @trigger_sql + ' select user_id from control_db.dbo.Audit_Sandbox_User where sandbox_limit-current_usage&lt;0;' set @trigger_sql = @trigger_sql + ' IF @user in (select userId from #overQuota)' set @trigger_sql = @trigger_sql + ' begin' set @trigger_sql = @trigger_sql + ' rollback' set @trigger_sql = @trigger_sql + ' print ''User: ''+@user+'' is over quota on usage so it can not perform a INSERT operation on table' +@alt_object_name + ', please delete data that is not needed to free up space then rerun the query'' ' set @trigger_sql = @trigger_sql + ' end' set @trigger_sql = @trigger_sql + ' end' exec (@trigger_sql) ; drop table #overQuota ;end trybegin catch declare @err_msg varchar(900), @err_num int, @err_line int, @syserr varchar(900) select @err_msg = ERROR_MESSAGE(), @err_num = ERROR_NUMBER(), @err_line = ERROR_LINE() set @syserr = 'Ended in DDLTRIG_CREATE_TABLE with errors: Line= ' + convert(varchar(10), @err_line) + ', Error Num = ' + convert(varchar(10), @err_num) + ', Error Msg= ' + @err_msg /*save to log file or control_db table*/end catchgoenable trigger [ddltrg_create_table] on databasego One more run need to be applied is prevent user drop object which is not belong to that user. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051create trigger ddltrig_drop_tableon databasewith execute as 'dbo' for drop_tableas set nocount on; declare @eventdata xml declare @orig_object_name varchar(100) declare @orig_user_name varchar(25) declare @alt_user_name varchar(25) declare @create_status char(1) declare @create_err_msg varchar(250) set @eventdata = EVENTDATA() set @orig_object_name = @eventdata.value('(/EVENT_INSTANCE/ObjectName)[1]','nvarchar(100)') set @orig_user_name = upper(@eventdata.value('(/EVENT_INSTANCE/LoginName)[1]','nvarchar(25)')) set @creaet_status = 'Y' set @create_err_msg = '' set @alt_user_name = right(@orig_user_name, len(@orig_user_name) - charindex('\\\\', @orig_user_name)) if @alt_user_name &lt;&gt; 'domainName/yourServiceAccount' begin if not exists (select * from control_db.dbo.Audit_Sandbox_User where User_ID = @alt_user_name) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' does not have privilege to use the sandbox database' end if substring(@orig_object_name, 1, len(@alt_user_name)) &lt;&gt; @alt_user_name begin set @create_status = 'N' set @create_err_msg = 'user: ' + @alt_user_name + ' does not have privilege to drop table: ' + @orig_object_name endendif (@create_Status = 'N')begin rollback print @create_err_msgendinsert control_db.dbo.Audit_Sandbox_Event_Tracking (Event_Type, Event_Time, Event_User, Event_Database, Event_Object_Name ,Event_Object_Type, Event_SQK, Audit_Created_Status, Audit_Error_Message) value (@eventdata.value('(/EVENT_INSTANCE/EventType)[1]','nvarchar(50)'), @eventdata.value('(/EVENT_INSTANCE/PostTime)[1]','datetime'), @Orig_user_name, @eventdata.value('(EVENT_INSTANCE/DatabaseName)[1]','nvarchar(25)'), @orig_object_name, @eventdata.value('(/EVENT_INSTANCE/ObjectType)[1]','nvarchar(25)'), @eventdata.value('(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]','nvarchar(max)'), case when @create_status = 'Y' then 'success' else 'failed' end, @create_err_msg )go At last, one very important setting need to be in placed in case cause issue when [sa] user cross database reference data 1alter database sandbox set TRUSTWORTHY ON; about trustworthy for detail, you can see Microsoft official document on below link Trustworthy database property","link":"/2020/12/02/Sandbox-solution-for-BI-reporting-and-business-analytics/"},{"title":"SQL Server Table Partitioning in Large Scale Data Warehouse 1","text":"This is a series articles to demonstrate table partitioning technology and how to apply it in a large scale data warehouse to improve database performance and even benefit for maintenance operation based on Microsoft SQL Server. This series is composed by three parts The first one is a fundamental introduction on basis knowledge The second part is showcase the entire production workflow to apply table partitioning to large tables Finally, it’s going to be advanced topic like partition merge, split, conversion and performance optimization in terms of different business demands Table Partitioning The BasicsFor this part, I will reference Cathrine Wilhelmsen’s work, she is Microsoft Data Platform MVP, BimlHero Certified Expert, international speaker, author, blogger, and chronic volunteer. She loves data and coding, as well as teaching and sharing knowledge - oh, and sci-fi, chocolate, coffee, and cats :) I learnt a lot and got many ideas from her blogs especially for table partitioning technology, below is her blog address, hope it can help you as well:blush: Cathrine blog This post is part 1 of 2 in the series Table Partitioning in SQL Server There are many benefits of partitioning large tables. You can speed up loading and archiving of data, you can perform maintenance operations on individual partitions instead of the whole table, and you may be able to improve query performance. However, implementing table partitioning is not a trivial task and you need a good understanding of how it works to implement and use it correctly. Being a business intelligence and data warehouse developer, not a DBA, it took me a while to understand table partitioning. I had to read a lot, get plenty of hands-on experience and make some mistakes along the way. (The illustration to the left is my Table Partitioning Cheat Sheet.) One of my favorite ways to learn something is to figure out how to explain it to others, so I recently did a webinar about table partitioning. (Update in 2020: The webinar has now been archived. Please contact Pragmatic Works if you would like to watch it, as they are the owners and publishers.) I wanted to follow that up with focused blog posts that included answers to questions I received during the webinar. This post covers the basics of partitioned tables, partition columns, partition functions and partition schemes. What is Table Partitioning? Table partitioning is a way to divide a large table into smaller, more manageable parts without having to create separate tables for each part. Data in a partitioned table is physically stored in groups of rows called partitions and each partition can be accessed and maintained separately. Partitioning is not visible to end users, a partitioned table behaves like one logical table when queried. This example illustration is used throughout this blog post to explain basic concepts. The table contains data from every day in 2012, 2013, 2014 and 2015, and there is one partition per year. To simplify the example, only the first and last day in each year is shown. An alternative to partitioned tables (for those who don’t have Enterprise Edition) is to create separate tables for each group of rows, union the tables in a view and then query the view instead of the tables. This is called a partitioned view. (Partitioned views are not covered in this blog post.) What is a Partition Column? Data in a partitioned table is partitioned based on a single column, the partition column, often called the partition key. Only one column can be used as the partition column, but it is possible to use a computed column. In the example illustration the date column is used as the partition column. SQL Server places rows in the correct partition based on the values in the date column. All rows with dates before or in 2012 are placed in the first partition, all rows with dates in 2013 are placed in the second partition, all rows with dates in 2014 are placed in the third partition, and all rows with dates in 2015 or after are placed in the fourth partition. If the partition column value is NULL, the rows are placed in the first partition. It is important to select a partition column that is almost always used as a filter in queries. When the partition column is used as a filter in queries, SQL Server can access only the relevant partitions. This is called partition elimination and can greatly improve performance when querying large tables. What is a Partition Function? The partition function defines how to partition data based on the partition column. The partition function does not explicitly define the partitions and which rows are placed in each partition. Instead, the partition function specifies boundary values, the points between partitions. The total number of partitions is always the total number of boundary values + 1. In the example illustration there are three boundary values. The first boundary value is between 2012 and 2013, the second boundary value is between 2013 and 2014, and the third boundary value is between 2014 and 2015. The three boundary values create four partitions. (The first partition also includes all rows with dates before 2012 and the last partition also includes all rows after 2015, but the example is kept simple with only four years for now.) But what are the actual boundary values used in the example? How do you know which date values are the points between two years? Is it December 31st or January 1st? The answer is that it can actually be either December 31st or January 1st, it depends on whether you use a range left or a range right partition function. Range Left and Range RightPartition functions are created as either range left or range right to specify whether the boundary values belong to their left or right partitions: Range left means that the actual boundary value belongs to its left partition, it is the last value in the left partition. Range right means that the actual boundary value belongs to its right partition, it is the first value in the right partition. Left and right partitions make more sense if the table is rotated: → → Range Left and Range Right using DatesThe first boundary value is between 2012 and 2013. This can be created in two ways, either by specifying a range left partition function with December 31st as the boundary value, or as a range right partition function with January 1st as the boundary value: → → Partition functions are created as either range left or range right, it is not possible to combine both in the same partition function. In a range left partition function, all boundary values are upper boundaries, they are the last values in the partitions. If you partition by year, you use December 31st. If you partition by month, you use January 31st, February 28th / 29th, March 31st, April 30th and so on. In a range right partition function, all boundary values are lower boundaries, they are the first values in the partitions. If you partition by year, you use January 1st. If you partition by month, you use January 1st, February 1st, March 1st, April 1st and so on: → Range Left and Range Right using the Wrong DatesIf the wrong dates are used as boundary values, the partitions incorrectly span two time periods: → What is a Partition Scheme? The partition scheme maps the logical partitions to physical filegroups. It is possible to map each partition to its own filegroup or all partitions to one filegroup. A filegroup contains one or more data files that can be spread on one or more disks. Filegroups can be set to read-only, and filegroups can be backed up and restored individually. There are many benefits of mapping each partition to its own filegroup. Less frequently accessed data can be placed on slower disks and more frequently accessed data can be placed on faster disks. Historical, unchanging data can be set to read-only and then be excluded from regular backups. If data needs to be restored it is possible to restore the partitions with the most critical data first. How do I create a Partitioned Table?The following script (for SQL Server 2012 and higher) first creates a numbers table function created by Itzik Ben-Gan that is used to insert test data. The script then creates a partition function, a partition scheme and a partitioned table. (It is important to notice that this script is meant to demonstrate the basic concepts of table partitioning, it does not create any indexes or constraints and it maps all partitions to the [PRIMARY] filegroup. This script is not meant to be used in a real-world project.) Finally it inserts test data and shows information about the partitioned table. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/* – ------------------------------------------------ – Create helper function GetNums by Itzik Ben-Gan – https://www.itprotoday.com/sql-server/virtual-auxiliary-table-numbers – GetNums is used to insert test data------------------------------------------------ – */ – Drop helper function if it already existsIF OBJECT_ID('GetNums') IS NOT NULL DROP FUNCTION GetNums;GO – Create helper functionCREATE FUNCTION GetNums(@n AS BIGINT) RETURNS TABLE AS RETURN WITH L0 AS(SELECT 1 AS c UNION ALL SELECT 1), L1 AS(SELECT 1 AS c FROM L0 AS A CROSS JOIN L0 AS B), L2 AS(SELECT 1 AS c FROM L1 AS A CROSS JOIN L1 AS B), L3 AS(SELECT 1 AS c FROM L2 AS A CROSS JOIN L2 AS B), L4 AS(SELECT 1 AS c FROM L3 AS A CROSS JOIN L3 AS B), L5 AS(SELECT 1 AS c FROM L4 AS A CROSS JOIN L4 AS B), Nums AS(SELECT ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) AS n FROM L5) SELECT TOP (@n) n FROM Nums ORDER BY n;GO/* – ---------------------------------------------------------- – Create example Partitioned Table (Heap) – The Partition Column is a DATE column – The Partition Function is RANGE RIGHT – The Partition Scheme maps all partitions to [PRIMARY]---------------------------------------------------------- – */ – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'Sales') DROP TABLE Sales;IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = N'psSales') DROP PARTITION SCHEME psSales;IF EXISTS (SELECT * FROM sys.partition_functions WHERE name = N'pfSales') DROP PARTITION FUNCTION pfSales; – Create the Partition Function CREATE PARTITION FUNCTION pfSales (DATE)AS RANGE RIGHT FOR VALUES ('2013-01-01', '2014-01-01', '2015-01-01'); – Create the Partition SchemeCREATE PARTITION SCHEME psSalesAS PARTITION pfSales ALL TO ([Primary]); – Create the Partitioned Table (Heap) on the Partition SchemeCREATE TABLE Sales ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO Sales(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – View Partitioned Table informationSELECTOBJECT_SCHEMA_NAME(pstats.object_id) AS SchemaName,OBJECT_NAME(pstats.object_id) AS TableName,ps.name AS PartitionSchemeName,ds.name AS PartitionFilegroupName,pf.name AS PartitionFunctionName,CASE pf.boundary_value_on_right WHEN 0 THEN 'Range Left' ELSE 'Range Right' END AS PartitionFunctionRange,CASE pf.boundary_value_on_right WHEN 0 THEN 'Upper Boundary' ELSE 'Lower Boundary' END AS PartitionBoundary,prv.value AS PartitionBoundaryValue,c.name AS PartitionKey,CASE WHEN pf.boundary_value_on_right = 0 THEN c.name + ' &gt; ' + CAST(ISNULL(LAG(prv.value) OVER(PARTITION BY pstats.object_id ORDER BY pstats.object_id, pstats.partition_number), 'Infinity') AS VARCHAR(100)) + ' and ' + c.name + ' &lt;= ' + CAST(ISNULL(prv.value, 'Infinity') AS VARCHAR(100)) ELSE c.name + ' &gt;= ' + CAST(ISNULL(prv.value, 'Infinity') AS VARCHAR(100)) + ' and ' + c.name + ' &lt; ' + CAST(ISNULL(LEAD(prv.value) OVER(PARTITION BY pstats.object_id ORDER BY pstats.object_id, pstats.partition_number), 'Infinity') AS VARCHAR(100)) END AS PartitionRange ,pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCount ,p.data_compression_desc AS DataCompressionFROM sys.dm_db_partition_stats AS pstatsINNER JOIN sys.partitions AS p ON pstats.partition_id = p.partition_idINNER JOIN sys.destination_data_spaces AS dds ON pstats.partition_number = dds.destination_idINNER JOIN sys.data_spaces AS ds ON dds.data_space_id = ds.data_space_idINNER JOIN sys.partition_schemes AS ps ON dds.partition_scheme_id = ps.data_space_idINNER JOIN sys.partition_functions AS pf ON ps.function_id = pf.function_idINNER JOIN sys.indexes AS i ON pstats.object_id = i.object_id AND pstats.index_id = i.index_id AND dds.partition_scheme_id = i.data_space_id AND i.type &lt;= 1 /* Heap or Clustered Index */INNER JOIN sys.index_columns AS ic ON i.index_id = ic.index_id AND i.object_id = ic.object_id AND ic.partition_ordinal &gt; 0INNER JOIN sys.columns AS c ON pstats.object_id = c.object_id AND ic.column_id = c.column_idLEFT JOIN sys.partition_range_values AS prv ON pf.function_id = prv.function_id AND pstats.partition_number = (CASE pf.boundary_value_on_right WHEN 0 THEN prv.boundary_id ELSE (prv.boundary_id+1) END)WHERE pstats.object_id = OBJECT_ID('Sales')ORDER BY TableName, PartitionNumber; Summary The partition function defines how to partition a table based on the values in the partition column. The partitioned table is created on the partition scheme that uses the partition function to map the logical partitions to physical filegroups. If each partition is mapped to a separate filegroup, partitions can be placed on slower or faster disks based on how frequently they are accessed, historical partitions can be set to read-only, and partitions can be backed up and restored individually based on how critical the data is. This post is the first in a series of Table Partitioning in SQL Server blog posts. It covers the basics of partitioned tables, partition columns, partition functions and partition schemes. Future blog posts in this series will build upon this information and these examples to explain other and more advanced concepts. Table Partitioning - Partition SwitchingThis post is part 2 of 2 in the series Table Partitioning in SQL Server Inserts, updates and deletes on large tables can be very slow and expensive, cause locking and blocking, and even fill up the transaction log. One of the main benefits of table partitioning is that you can speed up loading and archiving of data by using partition switching. Partition switching moves entire partitions between tables almost instantly. It is extremely fast because it is a metadata-only operation that updates the location of the data, no data is physically moved. New data can be loaded to separate tables and then switched in, old data can be switched out to separate tables and then archived or purged. All data preparation and manipulation can be done in separate tables without affecting the partitioned table. Partition Switching RequirementsThere are always two tables involved in partition switching. Data is switched from a source table to a target table. The target table (or target partition) must always be empty. (The first time I heard about partition switching, I thought it meant “partition swapping“. I thought it was possible to swap two partitions that both contained data. This is currently not possible, but I hope it will change in a future SQL Server version.) Partition switching is easy – as long as the source and target tables meet all the requirements :) There are many requirements, but the most important to remember are: The source and target tables (or partitions) must have identical columns, indexes and use the same partition column The source and target tables (or partitions) must exist on the same filegroup The target table (or partition) must be empty If all the requirements are not met, SQL Server is happy to tell you exactly what went wrong and provides detailed and informative error messages. Some of the most common examples are listed near the end of this blog post. Partition Switching ExamplesPartitions are switched by using the ALTER TABLE SWITCH statement. You ALTER the source table (or partition) and SWITCH to the target table (or partition). There are four ways to use the ALTER TABLE SWITCH statement: Switch from a non-partitioned table to another non-partitioned table Load data by switching in: Switch from a non-partitioned table to a partition in a partitioned table Archive data by switching out: Switch from a partition in a partitioned table to a non-partitioned table Switch from a partition in a partitioned table to a partition in another partitioned table The following examples use code from the previous Table Partitioning Basics blog post. It is important to notice that these examples are meant to demonstrate the different ways of switching partitions, they do not create any indexes and they map all partitions to the [PRIMARY] filegroup. These examples are not meant to be used in real-world projects. 1. Switch from Non-Partitioned to Non-PartitionedThe first way to use the ALTER TABLE SWITCH statement is to switch all the data from a non-partitioned table to an empty non-partitioned table: 1ALTER TABLE Source SWITCH TO Target Before switch: After switch: This is probably not used a lot, but it is a great way to start learning the ALTER TABLE SWITCH statement without having to create partition functions and partition schemes: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesSource') DROP TABLE SalesSource;IF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesTarget') DROP TABLE SalesTarget; – Create the Non-Partitioned Source Table (Heap) on the [PRIMARY] filegroupCREATE TABLE SalesSource ( SalesDate DATE, Quantity INT) ON [PRIMARY]; – Insert test dataINSERT INTO SalesSource(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Create the Non-Partitioned Target Table (Heap) on the [PRIMARY] filegroupCREATE TABLE SalesTarget ( SalesDate DATE, Quantity INT) ON [PRIMARY]; – Verify row count before switchSELECT COUNT(*) FROM SalesSource; – 1461000 rowsSELECT COUNT(*) FROM SalesTarget; – 0 rows – Turn on statisticsSET STATISTICS TIME ON; – Is it really that fast...?ALTER TABLE SalesSource SWITCH TO SalesTarget; – YEP! SUPER FAST! – Turn off statisticsSET STATISTICS TIME OFF; – Verify row count after switchSELECT COUNT(*) FROM SalesSource; – 0 rowsSELECT COUNT(*) FROM SalesTarget; – 1461000 rows – If we try to switch again we will get an error:ALTER TABLE SalesSource SWITCH TO SalesTarget; – Msg 4905, ALTER TABLE SWITCH statement failed. The target table 'SalesTarget' must be empty. – But if we try to switch back to the now empty Source table, it works:ALTER TABLE SalesTarget SWITCH TO SalesSource; – (...STILL SUPER FAST!) 2. Load data by switching in: Switch from Non-Partitioned to PartitionThe second way to use the ALTER TABLE SWITCH statement is to switch all the data from a non-partitioned table to an empty specified partition in a partitioned table: 1ALTER TABLE Source SWITCH TO Target PARTITION 1 Before switch: After switch: This is usually referred to as switching in to load data into partitioned tables. The non-partitioned table must specify WITH CHECK constraints to ensure that the data can be switched into the specified partition: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687 – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesSource') DROP TABLE SalesSource;IF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesTarget') DROP TABLE SalesTarget;IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = N'psSales') DROP PARTITION SCHEME psSales;IF EXISTS (SELECT * FROM sys.partition_functions WHERE name = N'pfSales') DROP PARTITION FUNCTION pfSales; – Create the Partition Function CREATE PARTITION FUNCTION pfSales (DATE)AS RANGE RIGHT FOR VALUES ('2013-01-01', '2014-01-01', '2015-01-01'); – Create the Partition SchemeCREATE PARTITION SCHEME psSalesAS PARTITION pfSales ALL TO ([Primary]); – Create the Non-Partitioned Source Table (Heap) on the [PRIMARY] filegroupCREATE TABLE SalesSource ( SalesDate DATE, Quantity INT) ON [PRIMARY]; – Insert test dataINSERT INTO SalesSource(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2013-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Create the Partitioned Target Table (Heap) on the Partition SchemeCREATE TABLE SalesTarget ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO SalesTarget(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2013-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2013-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Verify row count before switchSELECT COUNT(*) FROM SalesSource; – 366000 rowsSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesTarget')ORDER BY PartitionNumber; – 0 rows in Partition 1, 365000 rows in Partitions 2-4 – Turn on statisticsSET STATISTICS TIME ON; – Is it really that fast...?ALTER TABLE SalesSource SWITCH TO SalesTarget PARTITION 1; – NOPE! We get an error: – Msg 4982, ALTER TABLE SWITCH statement failed. Check constraints of source table 'SalesSource' – allow values that are not allowed by range defined by partition 1 on target table 'Sales'. – Add constraints to the source table to ensure it only contains data with values – that are allowed in partition 1 on the target tableALTER TABLE SalesSourceWITH CHECK ADD CONSTRAINT ckMinSalesDate CHECK (SalesDate IS NOT NULL AND SalesDate &gt;= '2012-01-01');ALTER TABLE SalesSourceWITH CHECK ADD CONSTRAINT ckMaxSalesDate CHECK (SalesDate IS NOT NULL AND SalesDate &lt; '2013-01-01'); – Try again. Is it really that fast...?ALTER TABLE SalesSource SWITCH TO SalesTarget PARTITION 1; – YEP! SUPER FAST! – Turn off statisticsSET STATISTICS TIME OFF; – Verify row count after switchSELECT COUNT(*) FROM SalesSource; – 0 rowsSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesTarget')ORDER BY PartitionNumber; – 366000 rows in Partition 1, 365000 rows in Partitions 2-4 3. Archive data by switching out: Switch from Partition to Non-PartitionedThe third way to use the ALTER TABLE SWITCH statement is to switch all the data from a specified partition in a partitioned table to an empty non-partitioned table: 1ALTER TABLE Source SWITCH PARTITION 1 TO Target Before switch: After switch: This is usually referred to as switching out to archive data from partitioned tables: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesSource') DROP TABLE SalesSource;IF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesTarget') DROP TABLE SalesTarget;IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = N'psSales') DROP PARTITION SCHEME psSales;IF EXISTS (SELECT * FROM sys.partition_functions WHERE name = N'pfSales') DROP PARTITION FUNCTION pfSales; – Create the Partition Function CREATE PARTITION FUNCTION pfSales (DATE)AS RANGE RIGHT FOR VALUES ('2013-01-01', '2014-01-01', '2015-01-01'); – Create the Partition SchemeCREATE PARTITION SCHEME psSalesAS PARTITION pfSales ALL TO ([Primary]); – Create the Partitioned Source Table (Heap) on the Partition SchemeCREATE TABLE SalesSource ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO SalesSource(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Create the Non-Partitioned Target Table (Heap) on the [PRIMARY] filegroupCREATE TABLE SalesTarget ( SalesDate DATE, Quantity INT) ON [PRIMARY]; – Verify row count before switchSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('Sales')ORDER BY PartitionNumber; – 366000 rows in Partition 1, 365000 rows in Partitions 2-4SELECT COUNT(*) FROM SalesTarget; – 0 rows – Turn on statisticsSET STATISTICS TIME ON; – Is it really that fast...?ALTER TABLE SalesSource SWITCH PARTITION 1 TO SalesTarget; – YEP! SUPER FAST! – Turn off statisticsSET STATISTICS TIME OFF; – Verify row count after switchSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesSource')ORDER BY PartitionNumber; – 0 rows in Partition 1, 365000 rows in Partitions 2-4SELECT COUNT(*) FROM SalesTarget; – 366000 rows 4. Switch from Partition to PartitionThe fourth way to use the ALTER TABLE SWITCH statement is to switch all the data from a specified partition in a partitioned table to an empty specified partition in another partitioned table: 1ALTER TABLE Source SWITCH PARTITION 1 TO Target PARTITION 1 Before switch: After switch: This can be used when data needs to be archived in another partitioned table: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081 – Drop objects if they already existIF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesSource') DROP TABLE SalesSource;IF EXISTS (SELECT * FROM sys.tables WHERE name = N'SalesTarget') DROP TABLE SalesTarget;IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = N'psSales') DROP PARTITION SCHEME psSales;IF EXISTS (SELECT * FROM sys.partition_functions WHERE name = N'pfSales') DROP PARTITION FUNCTION pfSales; – Create the Partition Function CREATE PARTITION FUNCTION pfSales (DATE)AS RANGE RIGHT FOR VALUES ('2013-01-01', '2014-01-01', '2015-01-01'); – Create the Partition SchemeCREATE PARTITION SCHEME psSalesAS PARTITION pfSales ALL TO ([Primary]); – Create the Partitioned Source Table (Heap) on the Partition SchemeCREATE TABLE SalesSource ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO SalesSource(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2012-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2012-01-01','2013-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Create the Partitioned Target Table (Heap) on the Partition SchemeCREATE TABLE SalesTarget ( SalesDate DATE, Quantity INT) ON psSales(SalesDate); – Insert test dataINSERT INTO SalesTarget(SalesDate, Quantity)SELECT DATEADD(DAY,dates.n-1,'2013-01-01') AS SalesDate, qty.n AS QuantityFROM GetNums(DATEDIFF(DD,'2013-01-01','2016-01-01')) datesCROSS JOIN GetNums(1000) AS qty; – Verify row count before switchSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesSource')ORDER BY PartitionNumber; – 366000 rows in Partition 1, 0 rows in Partitions 2-4SELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesTarget')ORDER BY PartitionNumber; – 0 rows in Partition 1, 365000 rows in Partitions 2-4 – Turn on statisticsSET STATISTICS TIME ON; – Is it really that fast...?ALTER TABLE SalesSource SWITCH PARTITION 1 TO SalesTarget PARTITION 1; – YEP! SUPER FAST! – Turn off statisticsSET STATISTICS TIME OFF; – Verify row count after switchSELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesSource')ORDER BY PartitionNumber; – 0 rows in Partition 1-4SELECT pstats.partition_number AS PartitionNumber ,pstats.row_count AS PartitionRowCountFROM sys.dm_db_partition_stats AS pstatsWHERE pstats.object_id = OBJECT_ID('SalesTarget')ORDER BY PartitionNumber; – 366000 rows in Partition 1, 365000 rows in Partitions 2-4 Error messagesSQL Server provides detailed and informative error messages if not all requirements are met before switching partitions. You can see all messages related to ALTER TABLE SWITCH by executing the following query, it is also quite a handy requirements checklist: 1234SELECT message_id, text FROM sys.messages WHERE language_id = 1033AND text LIKE '%ALTER TABLE SWITCH%'; Summary Partition switching moves entire partitions between tables almost instantly. New data can be loaded to separate tables and then switched in, old data can be switched out to separate tables and then archived or purged. There are many requirements for switching partitions. It is important to understand and test how partition switching works with filegroups, indexes and constraints. This post is the second in a series of Table Partitioning in SQL Server blog posts. It covers the basics of partition switching. Future blog posts in this series will build upon this information and these examples to explain other and more advanced concepts.","link":"/2020/12/25/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-1/"},{"title":"SQL Server Table Partitioning in Large Scale Data Warehouse 2","text":"This post is part 2, we are focusing on design and create data process to extract, transform and load (ETL) big amount data into partitioned tables to be able to integrate to SSIS package then operate ETL process by scheduled job automatically. In this case, we suppose transaction data with 100 columns and 100 million records need to be loaded into data warehouse from staging table. Technically, we can do partition switching from staging table (non partitioned table) to data warehouse table (partitioned table), but under the business reality, we can’t just simply do it like that, because Source table and target table usually are in different database, it’s not able to switch data directly because of violating same file group condition by partition switching basic in part 1. Staging table would be empty after partition switching, but the most of data transformations are applied to staging table then load final results into target table in data warehouse, so in business point of view, we can’t do partition switching from staging to target neither because big impact for entire daily, weekly or monthly data process. There might be another question: why don’t we bulk load data from source to target or what benefits do we get from partition switching? Technically, yes, we can do bulk insert, however, for such big volume of data movement, the lead time is going to be hours (2-3 hours), if the process ran on business hours, it would cause big impact and it’s hard to tolerant by business users for critical data like transaction so from efficiency and performance perspective, we have to leverage partition switching to deliver transaction data in short period of time without interrupt BI reports, dashboard refresh and business analysis. What is the main idea for production operation?In order to promise transaction data is always available, we need to create a middle table to hold transaction data as source table for partition switching, we call that middle table as switch table. To satisfy all the requirements for partition switching, the switch table has to be created in as the same file group as target transaction table, identical columns, indexes, use the same partition column. We do the bulk insert from staging table to switch table then partition switch to target transaction table in data warehouse, as part 1 mentioned, this step will finish in flash as long as there is no blocking. At last, we drop switch table so the entire data process completes. Now let’s dig litter deeper on details for each steps. Create switch table taskFrom part 1 table partitioning basics, we know to create partition table we need to create partition function then partition schemes to associate with partition functions, and also need to replicate all indexes from target table such as cluster index and non cluster columnstore index. To be able to let us code more reusable, we’d better encapsulate those functions into stored procedures then create a single script to call those stored procedures to finish the task. Create partition function and scheme for switch tableBefore we jump to the code some condition need to be clarified there: Transaction table is partitioned by month Partition column is month_key which is 6 digit integer like 202010 (Oct, 2020), 202011 (Nov, 2020) Target table file group naming convention: [database name]_FG_[table name]_[year] Partition function naming convention: [database name]_PF_[table name] Partition scheme naming convention: [database name]_PS_[table name] Cluster indexes naming convention: PK_[table name] Non cluster columnstore index naming convention: CSI_[table name] Now, we are ready to create stored procedure to generate partition function and scheme for switch table: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166use business_dbgocreate procedure dbo.usp_makePartitionOnTable @table_name varchar(100) --target (structure source) table name,@created_table_name varchar(100) --switch table name,@lrang int --left range,@rrang int --right range,@partition_col varchar(100) = 'month_key',@dbname varchar(100) = 'business_db'asbeginbegin trydeclare @sp_msg varchar(max)--check input parameter value, can't be '' or NULLset @table_name = ltrim(rtrim(@table_name))if (@table_name = '' or @table_name is null)begin set @sp_msg = '@table_name is empty,check input value' goto errorProc --error capture blockendset @created_table_name = ltrim(rtrim(@created_table_name))if (@created_table_name = '' or @created_table_name is null)begin set @sp_msg = '@create_table_name is empty, check input value' goto errorProcendset @lrang = ltrim(rtrim(@lrang))if (@lrang = '' or @lrang is null)begin set @sp_msg = '@lrang is empty, check input value' goto errorProcendset @rrang = ltrim(rtrim(@rrang))if (@rrang = '' or @rrang is null)begin set @sp_msg = '@rrang is empty, check input value' goto errorProcend if (@rrange &lt; 100000)begin set @sp_msg = 'the boundary is out of range, check input value' goto errorProcend declare @sSQ varchar(max),@iRange int,@pfName varchar(100) --partition function,@srcpfName varchar(100) --target table partition function,@filegropus varchar(max),@psName varchar(100),@filegroup_name varchar(100),@param varchar(200),@minrange int,@maxrange intset @pfName = @dbname+'_PF_'+@created_table_nameset @srcpfName = @dbname+'PF'+@table_name--check if partition function exists, if not, create, if so drop for rerundeclare @name varchar(100),@year intset @sSQL = 'use '+@dbname+'; if(exists(select name from sys.objects where OBJECT_NAME(OBJECT_ID)='''+@created_table_name+''' and type in (''U'')))' +'drop table dbo.'+@created_table_nameprint @sSQLexec sp_executeSQL @sSQL--check partition schemeset @sSQL='use '+@dbname+ '; select @name=PS.name from '+@dbname+'.sys.partition_schemes as PSinner join '+@dbname+'.sys.partition_functions as PF on PF.function_id=PS.function_id'+'where PF.name='''+@pfName+''''print @sSQLexec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@spName outputif(@psName != '')beginset @sSQL = 'use '+@dbname+ '; drop partition scheme '+@psNameexec sp_executeSQL @sSQLend--check partition functionset @name=''set @sSQL='use '+@dbname+'; select @name=name from '+@dbname+'.sys.partition_functions wherename='''+@pfName+''''print @sSQLexec sp_executeSQL @sSQL '@name varchar(100) output', @name=@name outputif(@name != '')beginset @sSQL = 'use '+@dbname+'; drop partition function '+@pfNameprint @sSQLexec sp_executeSQL @sSQLend--create partition function--check if lrange is in the lrange for the src table, and rrange is in the rrange. otherwise,--add one more in left and one more in rightif(@created_table_name&lt;&gt;@table_name)beginset @sSQL='use '+@dbname+'; select @lrange=convert(int,min(prv.value)), @rrange=convert(int,max(pre.value))'+' from sys.partition_range_values as prv join sys.partition_functions as pfs'+' on prv.function_id=pfs.function_id '+' where pfs.name='''+@srcpfName+''' 'set @param = '@lrange int output, @rrange int output'print @sSQLexec sp_executeSQL @sSQL, @param, @lrange=minrange output, @rrange=@maxrange outputendelsebeginset @minrange=0set @maxrange=0endif(@lrange&gt;@minrange and @rrange&lt;@maxrange)--check the input parameter value if fall in the right rangebegin--month_key calculate for Jan for lrangeif(@lrange%100=1)set iRange = @lrange-89elseset @iRange = @lrange-1endelseset @iRange = @lrange --set default value for @iRangeif(@rrange&lt;=@maxrange)beginif(@rrange%100=12)set @rrange=@rrange+89elseset @rrange=@rrange+1endset @filegroups=''set @sSQL='use '+@dbname+';'+'create partition function '+@pfName+'(int) as range left for values('while(@iRange&lt;=@rrange)beginset @filegroup_name=@dbname+'_FG_'+@table_name+'_'+convert(char(4),@year)if(convert(int, right(convert(char(6),@iRange),2))=12)set @iRange=(convert(int,left(convert(char(6),@iRange),4))+1)*100+1elseset @iRange+=1end--last boundaryif(@iRange&gt;@rrange)beginset @sSQL=@sSQL+')'set @filegroups=@filegroups+@filegroup_name+')'endelsebeginset @sSQL=@sSQL+','endendprint @sSQLexec sp_executeSQL @sSQL--create partition schemeset @psName=@dbname+'_PS_'+@created_table_nameset @sSQL='use '+@dbname+'; '+'create partition scheme '+@psName+'as partition '+@pfName+' to ('+@filegroupsexec sp_executeSQL @sSQLreturn 0end trybegin catch--add some try catch statementreturn -1end catcherrorProc:--add some error catch statementreturn -1end Replicate target table structure (column and data type) for switch table123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102use business_dbgocreate procedure dbo.usp_getTableModel( @dbName as varchar(100) ,@table_name as varchar(100) ,@create_table as varcahr(100) ,@sSQL as varchar(max) output)asbegindeclare @colTemp as table( name varchar(100) ,seqNo int ,IsNullable bit ,col_def varchar(max) ,data_type varchar(128) ,char_len int ,num_precs int ,num_dec int)set @sSQL = 'select ltrim(rtrim(column_name)) as name,ordinal_position as seqNo,case when ltrim(rtrim(is_nullable))=''no'' then 1 else 0 end as IsNullable,ltrim(rtrim(column_default)) as col_def,upper(ltrim(rtrim(data_type))) as data_type,character_maximum_length as char_len,numeric_precision as num_precs,numeric_scale as num_decfrom '+@dbname+'.information_schema.columnswhere table_name=@table_name and table_catalog=@dbname'insert into @colTempexec sp_executeSQL @sSQL, '@table_name varchar(100),@dbname varchar(100)',@table_name=@table_name,@dbname=@dbnamedeclare @i int,@name varchar(100),@seqNo int,@IsNull int,@col_def varchar(max),@data_type varchar(100),@char_len int,@num_precs int,@num_dec int,@maxcount intselect @maxcount = max(seqNo) from @colTemp;select @i=min(seqNo) from @colTemp;select@name=name,@seqNo=seqNo,@IsNull=IsNullable,@col_def=col_def,@data_type=data_typ,@char_len=char_len,@num_precs=num_precs,@num_dec=num_decfrom @colTempwhere seqNo=@iset @sSQL='create table '+quotename(@dbname)+'.dbo.'+quotename(@created_table) +'('while not @i is Nullbeginset @sSQL = @sSQL + @Nameset @sSQL = @sSQL+' '+@data_typeif(charindex('char',lower(@date_type))&gt;0 or charindex('var',lover(@data_type))&gt;0)set @sSQL=@sSQL+'('+ltrim(rtrim(convert(varchar(4),@char_len)))+')'if(charindex('decimal',lower(@data_type))&gt;0)set @sSQL = @sSQL+'('+ltrim(rtrim(convert(varchar(4),@num_precs)))+','+ltrim(rtrim(convert(4),@num_dec)))+')'if(@col_def&lt;&gt;'')set @sSQL=@sSQL+' '+'default '+@col_defif(@IsNull=0)set @sSQL=@sSQL+' Null'elseset @sSQL=@sSQL+' not Null'if(@i&lt;@maxcount)set @sSQL=@sSQL+','elseset @sSQL=@sSQL+')'delete from @colTempwhere seqNo=@iselect @i=min(seqNo) from @colTempselect@name=name,@seqNo=seqNo,@IsNull=IsNullable,@col_def=col_def,@data_type=data_typ,@char_len=char_len,@num_precs=num_precs,@num_dec=num_decfrom @colTempwhere seqNo=@iendendgo Replicate target table cluster index for switch tablewe also need to clone cluster index from src (target) to switch table 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677use business_dbgocreate procedure dbo.usp_ReplicateClusterIndex @table_name varchar(100),@created_table varchar(100),@dbname varchar(100)='business_db'asbegindeclare @i int,@sSQL varchar(max),@ncount int,@colname varchar(100),@nloop int,@return_result int,@sp_msg varchar(100)begin tryset @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset @sp_msg='ended with @table_name empty, check input value'goto errorProcenddeclare @name varchar(100)--find out if the object existsset @sSQL = 'use '+@dbname+ '; select @name=name from sys.indexes wherename=''PK_'+@created_table+''''exec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif @name != ''beginset @sSQL='use '+@dbname+'; drop index PK_'+@created_table+' on dbo.'+@created_tableexec sp_executeSQL @sSQLendif (@name='' or @name is null)begindeclare @tblcol table(colID int not null, colNmae varchar(100) not null)set @sSQL='use '+@dbname+'; select b._key,''[''+c.name+'']''from sys.indexes a join sys.index_columns b on a.object_id=b.object_idand a.index_id=b.index_idjoin sys.columns c on b.column_id=c.column_id and b.object_id=c.object_idwhere a.name=''PK_'+@table_name+''''insert into @tblColexec sp_executeSQL @sSQLselect @ncount=count(*) from @tblcol;select @i=min(colID) from @tblcol;set nloop=oset @sSQL = 'use '+@dbname+ '; create clustered index PK_'+@created_table+' on '+quotename(@created_table)+'('while not @i is nullbeginselect @colName=colName from @tblcol where colID=@iif(@nloop=0)set @sSQL=@sSQL+@colNameelseset @sSQL=@sSQL+', '+@colNameset @nloop+=1delete from @tblcol where colID=@iset @i=min(colID) from @tblcolendset @sSQL=@sSQL+')'exec @return_result=sp_executeSQL @sSQLif(return_result&lt;0)begin--log statementreturn -1endelsereturn 0endend trybegin catch--try catch block statementreturn -1end catcherrorProc:--error log statementreturn -1end Create partitioned switch tableNow we have all store procedures to meet partitioned table requirement, it’s time to create a script (store procedure) to integrate in order to create partitioned switch table. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182use business_dbgocreate procedure dbo.usp_createPartitionTable @table_name varchar(100),@partition_col varchar(100)='month_key',@created_table varchar(100),@lrange int,@rrange int,@dbname varchar(100)='business_db'asbeginset nocount on;begin trydeclare @return_result int,@sSQL varchar(max),sp_msg varchar(100)set @table_name=ltrim(rtrim(@table_name))if(@table_name ='' or @table_name is null)beginset @sp_msg='end with @table_name is empty, check input value'goto errorProcendset @created_table =ltrim(rtrim(@created_table))if(@created_table='' or @created_table is null)beginset @sp_msg='end with @created_table is empty, check input value'goto errorProcendif(@lrange is null)beginset sp_msg='end with @lrange empty, check input value'goto errorProcendif(@rrange is null)beginset sp_msg='end with @rrange empty, check input value'goto errorProcenddeclare @name varchar(100),@ps_name varchar(100)set @sSQL='select @name=name from '+quotename(@dbname)+'.sys.tables where name='''+@created_table''''print @sSQLexec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif(@name !='')beginset @sSQL='drop table '+quotename(@dbname)+'.dbo.'+@created_tableexec sp_executeSQL @sSQLend--create partition function and schemeexec business_db.dbo.usp_makePartitionOnTable @table_name=@table_name,@created_table_name=@created_table,@lrange=@lrange,@rrange=@rrange,@partiton_col=@partition_col,@dbname=@dbname--create swich table structureexec business_db.dbo.usp_getTableModel @dbname=@dbname,@table_name=@table_name,@created_table=@created_table,@sSQL=@sSQL outputset @ps_name=@dbname+'_PS_'+@created_tableset @sSQL='use '+@dbname+';'+@sSQL+' on '+@ps_name+'('+@partition_col+')'print @sSQLexec sp_executedSQL @sSQL--create cluster index for switch tableexec @return_result=business_db.dbo.usp_ReplicateClusterIndex @table_name=@table_name,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endelsereturn 0end trybegin catch--add try catch block statementreturn -1end catcherrorProc:--add some error handling statementreturn -1end Business data APIWe have everything to create a partitioned table so far, the only thing left is create a data API to pass in input value in terms of business RSD (Requirement Specifics document). 12345678910111213141516171819202122232425use business_dbgocreate procedure switch_transactionas beginbegin trydeclare @left_range int,@right_range int,@return_result intselect top 1 @left_range=month_key from staging.dbo.transactionexec @return_result=business_db.dbo.usp_createPartitionTable @table_name='transaction',@partition_col='month_key',@created_table='switch_transaction',@lrange=@left_range,@rrange=@left_range,@dbname='business_db'if(return_result&lt;0)begin--add log statementreturn -1endelsereturn 0end tryend Load staging transaction data into switch tableThis step, we need to prepare data for switch table in order to make partition switching later on. Because switch table is a kind of temp table and it’s not awareness by business data users, so no matter how long the loading time is, it won’t cause down time on production. For the bulk insert, the simplest and fastest way is use SSIS data flow which provides bulk load functionality. for Data access mode option, there is drop-down list, make sure Table or view - fast load being selected. Create columnstore index for switch tableIn this step, we create columnstore index for switch table, you might ask why we don’t create it along with cluster index prior data loading? We separate columnstore index from cluster index is by its special properties, because we can’t load data with columnstore index enabled. Now let’s brief take a look at the definition by Microsoft Columnstore indexes are the standard for storing and querying large data warehousing fact tables. This index uses column-based data storage and query processing to achieve gains up to 10 times the query performance in your data warehouse over traditional row-oriented storage. You can also achieve gains up to 10 times the data compression over the uncompressed data size. Beginning with SQL Server 2016 SP1, columnstore indexes enable operational analytics: the ability to run performant real-time analytics on a transactional workload. A columnstore index is a technology for storing, retrieving, and managing data by using a columnar data format, called a columnstore Obviously, we can gain a lot of performance benefits from columnstore index, but its specialty decides we have to treat it very carefully, because it costs a quite long time to build it for large data warehousing fact table. we are going to create a stored procedure to achieve the function of building columnstore index then write another API to create columnstore index for switch table in terms of business requirement. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475use business_dbgocreate procedure usp_createColmart @table_name varchar(100),@src_table varchar(100) --target table ,@dbname varchar(100)='business_db'asbegindeclare @i int,@sSQL varchar(max),@ncount int,@colName varchar(100),@nloop int,@return_result int,@sp_msg varchar(100)begin tryset @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset sp_msg='end with @table_name is empty, check the input value'goto errorProcendset @dbname=ltrim(rtrim(@dbname))declare @name varchar(100)set @sSQL='use '+@dbname+'; select @name=name from sys.indexes where name=''CSI_'+@table_name+''''exec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif(@name !='')set @sSQL='use '+@dbname+'; drop index CSI_'+@table_name+' on dbo.'+@table_nameexec sp_executeSQL @sSQLendelsedeclare @tblcol table(colID int not null, colName varchar(100) not null)set @sSQL='use '+@dbname+'; select c.column_id,c.name from sys.indexes ainner join sys.index_columns b on a.object_id=b.object_id and a.index_id=b.index_idinner join sys.columns c on b.object_id=c.object_id and b.column_id=c.column_idwhere a.is_primary_key=0 anda.is_unique=0 anda.is_unique_constraint=0 anda.name=''CSI_'+@scr_table+''''insert into @tblcolexec sp_executeSQL @sSQLselect @ncount=count(*)from @tblcolset @nloop=0set @sSQL='use '+@dbname+'; create nonclustered columnstore index CSI_'+@table_name+' on '+quotename(@table_name)+'('while not @i is nullbeginselect @colName=colName from @tblcol where colID=@iif(@nloop=0)set @sSQL = @sSQL + @colNameelseset @sSQL=@sSQL+','+@colNameset nloop+=1delete from @tblcol where colID=@iselect @i=min(colID) from @tblcolendset @sSQL+=')'exec @return_result=sp_executeSQL @sSQLif(@return_result&lt;0)begin--add log statementreturn -1endelsereturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Now, we create business data API to pass in parament value based on business requirement, in this case, we deal with transaction table 1234567891011121314151617181920212223242526272829use business_dbgocreate procedure switch_transaction_csiasbeginbegin trydeclare @left_range int,@right_range int,@return_result intexec @return_result=business_db.dbo.usp_createColmart @table_name='switch_transaction',@scr_table='transaction',@dbname='business_db'if(@return_result&lt;0)begin--add log statementreturn -1endelsereturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Partition switching to load data to target tableSo far we create switch table and load transaction data into it then create columnstore index, next we are going to conduct major task to switch partition from switch table to target transaction table. For code reusable and more generic scenarios, we still need do some condition checks before we jump to partition switching. Check data in switch (staging) table is in the right range and ready to be proceeded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566use business_dbgocreate procedure usp_IsStgDataReady( @stgtbl_name varchar(100) ,@partition_col varchar(100)='month_key' ,@lrange int ,@urange int ,@dbname varchar(100)='business_db')asbeginbegin trydeclare @sSQL varchar(max),@month_key int,@sParam varchar(100),@range int,@sp_msg varchar(100)set @stgtbl_name=ltrim(rtrim(@stgtbl_name))if(@stgtbl_name='' or @stgtbl_name is null)beginset sp_msg='end up with @stgtbl_name empty, check input value'goto errorProcendset @partition_col=ltrim(rtrim(@partition_col))if(@partition_col='' or @partition_col is null)beginset @sp_msg='end up with @partition_col empty, check input value'goto errorProcendif(not len(convert(char(6),@urange))=6)beginset sp_msg='end up with @urange is out of range, check input value'goto errorProcendset @range=@lrangewhile @range &lt;= @urangebeginset @sParam='@month_keyOut int output'set @sSQL='select top 1 @month_keyOut='+@partition_col+' from'+@dbname+'.dbo.'+@stgtbl_name+'where'+@partition_key+' ='+convert(varchar(6),@range)exec sp_executeSQL @sSQL,@sParam,@month_keyOUt=@month_key outputif(@month_key!=@range or @month_key is null)beginset @sp_msg='data for '+convert(char(6),@range)+' not ready for loading'return -1endif(right(convert(char(6),@range),2)='12')beginset @range=convert(int,(left(convert(char(6),@range),4)+1))*100+1elseset @range+=1endreturn 0endend trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Check up target table partitionFrom previous post about table partitioning basics, the last requirement for partition switching is partition in target table must be empty, so it’s necessary to check whether target table partition has data in it, if it’s empty, that is good and we are ready to do the partition switching, but in production environment, we usually need to handle more complicated situations like rerun after job failed so definitely have to take it into account when target partition has data and deal with it. Firstly, let’s check target partition: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455use business_dbgocreate procedure IsPartitionLoaded @table_name,@partition_col varchar(100)='month_key',@range int,@ret int=0 output,@dbname varchar(100)='business_db'asbeginbegin trydeclare @sSQL varchar(max),@sParam varchar(100),@monthKey int,@sp_msg varchar(100)set @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset sp_msg='end up with @table_name empty, check input value'goto errorProcendset @partition_col=ltrim(rtrim(@partition_col))if(@partition_col='' or @partition_col is null)beginset sp_msg='end up with @partition_col empty, check input value'goto errorProcendif(isnumeric(@range)!=1 or not(len(convert(char(8),@range))=6))beginset @sp_msg='end up with @range out of range, check input value'goto errorProcendset @ret=1 --target partition has dataset sParam='@retOut int output'beginset @sSQL='select top 1 @retOut='+@partition_col+' from '+'.dbo.'+@table_name+'where '+@partition_col+' ='+convert(char(6),@range)exec sp_executeSQL @sSQL, @sParam, @retOut=@monthKey outputendif (@monthKey is null or @monthkey='')beginset @ret=0 --target partition is emptyreturn 0endreturn 0end trybegin catchset @ret = -1--add try catch block statmentreturn -1end catcherrorProc:set @ret = -1--error handling statementreturn -1end Get proper partition numbers for source (switch) and target tableLet’s first handle ideal key which is empty in target partition, the last preparation we have to know is find out proper partition number on both source and target sides so that we are able to switch to right target partition. 1234567891011121314151617181920212223242526272829303132333435363738use business_dbgocreate procedure usp_getPatitionNum @pf_name varchar(100),@range int,@pn int output,@dbname varchar(100)='business_db'asbeginbegin trydeclare @sp_msg varchar(100)set @pf_name=ltrim(rtrim(@pf_name))if(@pf_name='' or @pf_name is null)beginset sp_msg='end up with @pf_name is empty, check input value'goto errorProcendif((not isnumeric(@range)=1)or (@range&lt;100000))beginset sp_msg='end up with @range is out of range, check input value'goto errorProcenddeclare @sParam varchar(100),@sSQL varchar(max)if @range&lt;10000000set @sSQL='use '+@dbname+'; select @pnOut=$PARTITION.'+@pf_name+'('+convert(char(6),@range)+')'set @sParam='@pnOut int output'exec sp_executeSQL @sSQL, @sParam, @pnOut=@pn outputreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Partition switchingNow, it’s time for us to switch partition to target table 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849use business_dbgocreate procedure usp_switchPartition @srctbl_name varchar(100),@destbl_name varchar(100),@src_pn int=0,@des_pn int=0,@dbname varchar(100)='business_db'asbegindeclare @pn int,@sSQL varchar(max),@sParam varchar(100),@ncount int,@sp_msg varchar(100)begin tryset @srctbl_name=ltrim(rtrim(@srctbl_name))if(@srctbl_name='' or @srctbl_name is null)beginset sp_msg='end up with @srctbl_name empty, check input value'goto errorProcendset @destbl_name=ltrim(rtrim(@destbl_name))if(@destbl_name='' or @srctbl_name is null)beginset sp_msg='end up with @destbl_name empty, check input value'endset @sSQL='use '+@dbname+'; alter table '+@dbname+'.dbo.'+quotename(@srctbl_name)if(@src_pn&gt;0)set @sSQL=@sSQL+' switch partition '+convert(char(4),@src_pn)elseset @sSQL=@sSQL+' switch 'if(@des_pn &gt; 0)set @sSQL=@sSQL+' to'+@dbname+'.dbo.'+ quotename(@destbl_name)+' partition '+ convert(char(4),@des_pn)elseset @sSQL=@sSQL+' to '+@dbname+'.dbo.'+ quotename(#destbl_name)exec sp_executeSQL @sSQLreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Clean the switch table after partition switchingThe switch table is going to be empty after partition switching so it’s useless now and can be drop from database as well as partition function and scheme 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061use business_dbgocreate procedure usp_cleanTable @table_name varchar(100),@partition_col varchar(100)='month_key',@dbname varchar(100)='business_db'as begin begin trydeclare sp_msg varchar(100)set @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset sp_msg='end up with @table_name empty, check input value'goto errorProcenddeclare @sSQL varchar(max),@constraint_name varchar(100),@return_result int,@pf_name varchar(100),@ps_name varchar(100)@name varchar(100)set @pf_name=@dbname+'_PF_'+@table_nameset @ps_name=@dbname+'_PS_'+@table_name--drop tableset @name=''set @sSQL='use '+@dbname+'; select @name=name from sys.tables where name='''+@table_name+''''exec sp_executeSQL @sSQL, '@name varchar(100) output',@name=@name outputif(@name!='')beginset @sSQL='use '+@dbname+'; drop table dbo.'+@table_nameexec sp_executeSQL @sSQL--drop partition schemeset @name=''set @sSQL='use '+@dbname+'; select @name=name from sys.partition_schemes where name='''+@ps_name+''''exec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif(@name!='')beginset @sSQL='use '+@dbname+'; drop partition scheme'+@ps_nameexec sp_executeSQL @sSQLend--drop partition functionset @name=''set @sSQL='use '+@dbname+'; select @name=name from sys.partition_functions where name='''+@pf_name+''''exec sp_executeSQL @sSQL, '@name varchar(100) output', @name=@name outputif(@name!='')beginset @sSQL='use '+@dbname+'; drop partition function'+@ps_nameexec sp_executeSQL @sSQLendendreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Swap data out of target partition to temp tableA special stored procedure is needed to switch data out of target partition if there would be data in there. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475use business_dbgocreate procedure dbo.unloadData @table_name varchar(100),@tempTbl varchar(100),@range int,,@partition_col varchar(100)='month_key',@dbname varchar(100)='business_db'asbeginbegin trydeclare @pn int,@pnd int,@return_result int,@filegroup_name varchar(100),@pf_name varchar(100),@isRead int,@sp_msg varchar(100),@ps_name varchar(100)set @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset @sp_msg='end up with @table_name empty, check input value'goto errorProcendif(not isnumeric(@range)=1 or @range&lt;100000)beginset @sp_msg='end up with @range out of range, check input value'goto errorProcendset @pf_name=@dbname+'_PF_'+@table_name--get src table partitionexec @return_result=business_db.dbo.usp_getPatitionNum @pf_name=@pf_name,@range=@range,@pn=@pn output,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endif(@tempTbl='')set @tempTbl='Temp_'+@table_name--get des table partitionset @pf_name=@dbname+'_PF_'+@tempTblexec @return_result=business_db.dbo.usp_getPatitionNum @pf_name=@pf_name,@range=@range,@pn=@pnd output,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--swap data out from target partition to temp tableexec @return_result=business_db.dbo.usp_switchPartition @srctbl_name=@table_name,@destbl_name=@tempTbl,@src_pn=@pn,@des_pn=@pnd,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Control console script to integrate all functionsWe are now all set so it’s time to integrate all those stored procedure into one control console to implement the partition switching task. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179use business_dbgocreate procedure dbo.usp_switchData @scrtable_name varchar(100),@table_name varchar(100),@lrange int,@urange int,@partiton_col varchar(100)='month_key',@dbname varchar(100)='business_db'asbeginbegin trydeclare @sp_msg varchar(100)--check input valuesset @srctable_name=ltrim(rtrim(@srctable_name))if(@srctable_name='' or @srctable_name is null)beginset @sp_msg='end up with @srctable_name empty, check input value'goto errorProcendset @table_name=ltrim(rtrim(@table_name))if(@table_name='' or @table_name is null)beginset @sp_msg='end up with @table_name empty, check input value'goto errorProcendif((not isnumeric(@lrange)=1) or (@lrange &lt; 100000))beginset sp_msg='end up with @lrange out of range, check input value'goto errorProcendif(@urange=0)set @urange=@lrangeset @partition_col=ltrim(rtrim(@partition_col))--declare local variablesdeclare @sSQ varchar(max),@check_constraint varchar(100),@pf_name varchar(100),@ps_name varchar(100),@staging_pf_name varchar(100),@staging_ps_name varchar(100),@filegroup_name varchar(100),@desfilegroup_name varchar(100),@range_count int,@range int,@sParam varchar(100),@ret int,@pn int,@pn_staging int,@index_name varchar(100),@return_result int,@destbl_lrange int--set valuesset @pf_name=@dbname+'_PF_'+@table_nameset @ps_name=@dbname+'_PS_'+table_nameset @staging_pf_name=@dbname+'_PF_'+srctable_nameset @staging_pf_name=@dbname+'_PS_'+srctable_nameset @range=@lrangeset @range_count=0set @pn_staging=0--check if switch table data is ready for the range from @lrange to @urangeexec @return_result=business_db.dbo.usp_IsStgDataReady @stgtbl_name=@srctable_name,@partition_col=@partiton_col,@lrange=@lrange,@urange=urange,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--begin while loop transactiondeclare @tempTbl varchar(100)set @temTbl='Temp'+@table_nameset @range=@lrangewhile @range&lt;=@urangebeginset @range_count+=1--check if data is in target partitionset @ret=0exec return_result=business_db.dbo.IsPartitionLoaded @table_name=@table_name,@partition_col=@partition_col,@range=@range,@ret=@ret output,@dbname=@dbnameif(@ret=1)begin--create temp table to hold swapped dataexec @return_result=busienss_db.dbo.usp_createPartitionTable @table_name=@table_name,@partition_col=@partition_col,@created_table=@tempTbl,@lrange=@lrange,@rrange=@urange,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--create columnstore index for temTblexec @return_result=business_db.dbo.usp_createColmart @table_name=temTbl,@scr_table=@table_name,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--swap data to temp tableexec @return_result=business_db.dbo.usp_unloadData @table_name=@table_name,@tempTbl=@tempTbl,@range=@range,@partition_col=@partition_col,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--get partition number from destination tableexec @return_result=business_db.dbo.usp_getPatitionNum @pf_name=@pf_name,@range=range,@pn=@pn output,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--get partition number from switch tableexec @return_result=business_db.dbo.usp_getPatitionNum @pf_name=@staging_pf_name,@range=range,@pn=@pn_staging output,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1end--partition switching from switch to target partitionexec @return_result=business_db.dbo.usp_switchPartition @srctbl_name=@src_name,@destbl_name=@table_name,@src_pn=@pn_staging,@des_pn@pn,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endif(@yearload_flag=0)beginif(right(convert(char(6),@range),2)='12')set @range=(convert(int,left(convert(6),@range)4))+1)*100+1elseset @range+=1endelseset @range+=100end--drop temp tableexec @return_result=business_db.dbo.usp_cleanTable @table_name=@tempTbl,@partition_col=@partition_col,@dbname=@dbnameif(@return_result&lt;0)begin--add some log statementreturn -1endendreturn 0end trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end Business data API for final transaction partition switchingAll in all, we are going to integrate all encapsulated stored procedures into one data API to take input parameter value from outside then accomplish the task which is partition switch transaction data 100mm records in flash. 12345678910111213141516171819202122232425262728293031use business_dbgocreate procedure transaction_partition_switch asbeginbegin trydeclare @month_key int,@return_result intselect top 1 @month_key=month_key from staging.dbo.transactionexec @return_result=business_db.dbo.usp_switchData @scrtable_name='switch_transaction',@table_name='transaction',@lrange=@month_key,@urange=0,@partition_col='month_key',@yearload_flag=0,@dbname='business_db'if(@return_result&lt;0)begin--add some log statementreturn -1endend trybegin catch--add try catch block statmentreturn -1end catcherrorProc:--error handling statementreturn -1end A sub-process of partition switching for error handling and restatement in production[In previous chapter](Get proper partition numbers for source (switch) and target table), we talk about the ideal case which is there is no data or empty in target partition, but in real production environment, situation is more complicated, some unpredicted event might cause data process failed or business redefined some data elements so that reprocess and restatement is necessary, in those cases, the target partition usually has data in it, that is the thing we have to deal with. The approach is the same, partition switching will help us out. The main idea is to create another partition temp table with the same table structure, partition column and index setting as target table in the same file group then swap data out of target partition to temp table counterpart partition, finally drop the temp table and it’s partition function and scheme. ConclusionAbove, we go through the whole solution on how to apply SQL table partitioning technic to implement partition switching in real ETL process so that we can easily to schedule it run on regular basis, I believe process automation is a valuable AI technic for business operation, because the best use-case for AI projects will reduce costs, reduce risk and improve profits. more often than not, the best results are seen from implementing AI to handle the small, repetitive tasks that businesses do on a daily basis.","link":"/2020/12/26/SQL-Server-Table-Partitioning-in-Large-Scale-Data-Warehouse-2/"}],"tags":[{"name":"SQL, query, SQL Server, optimization","slug":"SQL-query-SQL-Server-optimization","link":"/tags/SQL-query-SQL-Server-optimization/"},{"name":"python, automation, ETL, Excel","slug":"python-automation-ETL-Excel","link":"/tags/python-automation-ETL-Excel/"},{"name":"SQL Server, database, monitoring, system optimization","slug":"SQL-Server-database-monitoring-system-optimization","link":"/tags/SQL-Server-database-monitoring-system-optimization/"},{"name":"Python, Pandas, SQL, ETL","slug":"Python-Pandas-SQL-ETL","link":"/tags/Python-Pandas-SQL-ETL/"},{"name":"file system, csv","slug":"file-system-csv","link":"/tags/file-system-csv/"},{"name":"Hadoop, Hive, Impala, HDFS, SQL Server, SSIS","slug":"Hadoop-Hive-Impala-HDFS-SQL-Server-SSIS","link":"/tags/Hadoop-Hive-Impala-HDFS-SQL-Server-SSIS/"},{"name":"Linux, centos, ubuntu, network","slug":"Linux-centos-ubuntu-network","link":"/tags/Linux-centos-ubuntu-network/"},{"name":"hexo, blog, online image, config, Node.js, router","slug":"hexo-blog-online-image-config-Node-js-router","link":"/tags/hexo-blog-online-image-config-Node-js-router/"},{"name":"oracle, SSIS, SAS, config","slug":"oracle-SSIS-SAS-config","link":"/tags/oracle-SSIS-SAS-config/"},{"name":"python, sql, db2, CLI, SSIS, config","slug":"python-sql-db2-CLI-SSIS-config","link":"/tags/python-sql-db2-CLI-SSIS-config/"},{"name":"Linux, Centos, network, config","slug":"Linux-Centos-network-config","link":"/tags/Linux-Centos-network-config/"},{"name":"Linux, MySQL, Config, Database","slug":"Linux-MySQL-Config-Database","link":"/tags/Linux-MySQL-Config-Database/"},{"name":"sandbox, SQL, SQL Server, database","slug":"sandbox-SQL-SQL-Server-database","link":"/tags/sandbox-SQL-SQL-Server-database/"},{"name":"SQL Server, partitioning","slug":"SQL-Server-partitioning","link":"/tags/SQL-Server-partitioning/"}],"categories":[{"name":"Data","slug":"Data","link":"/categories/Data/"},{"name":"Data, BI","slug":"Data-BI","link":"/categories/Data-BI/"},{"name":"IT, BI","slug":"IT-BI","link":"/categories/IT-BI/"},{"name":"data, BI","slug":"data-BI","link":"/categories/data-BI/"},{"name":"IT","slug":"IT","link":"/categories/IT/"},{"name":"Database, BI","slug":"Database-BI","link":"/categories/Database-BI/"}]}