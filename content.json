{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"Data Manipulation and ETL with Pandas","text":"Pandas is the most used library in Python to manipulate data and deal with data transformation, by leveraging Pandas in memory data frame and abundant build-in functions in terms of data frame, it almost can handle all kinds of ETL task. We are going to talk about a data process to read input data from Excel Spreadsheet, make some data transformations by business requirement then load reporting data into SQL Server database. Pandas functions brief introductionFirst, we use read_excel() function to read in input Excel file, then use slicing function iloc() or loc() to get the data which need to be proceeded. The difference between iloc() and loc() is iloc() use row and column index number to slice data but loc() is use column name instead of index number. If we need to rename the column, we use rename(column={},inplace=True/False) function to do that. We also can use drop() function to drop columns. If we want to re-order the sequence of columns, the reindex() will help us to do that. For the purpose of merging or joining multiple tables, we can use merge() function. we can use to_sql() function to write data into database table after transformations. If we want to create UDF (user defined function) to apply to Pandas data frame then we can use apply(UDF name) function. For SQL windows function, Pandas also has equivalent way on those kind of function such as Top N rows per group which is equivalent to row_number()over() sql window function. Use assign(().groupby().cumcount()).query().sort_values() function Pandas use cases to manipulate Excel data to load into SQL Server database1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import numpy as npimport pandas as pdimport pyodbcfrom datetime import datetimefrom sqlalchemy import create_enginedef py2mssql(): engine = create_engine(&quot;mssql+pyodbc://yourServerName/database?driver=SQL Server Native Client 11.0&quot;) con_str = pyodbc.connect(&quot;DRIVER={SQL Server Native Client 11.0}; SERVER=yourServerName; DATABASE=yourDBName; Trusted_connection=yes&quot;) return engine, con_strdef dfstaging(df, sql_cmd): df.to_sql('your target table',py2mssql()[0],if_exists='replace',schema='dbo',index=False) conn = py2mssql()[1] cur = conn.cursor() conn.commit() cur.close() conn.close()def rpt2mssql(): df_pdr = pd.read_excel(r'C:\\your input file path\\fileName.xlsx',sheet_name='Sheet1') df_pd1 = df_pdr.iloc[:,[0,1,2,3,4,5,6,8,9,10]] ## iloc[row,col in num] df_temp = df_pdr.loc[:,['col1','col2','col3']] ## loc[row,col in name] up_col = &quot;&quot;&quot; sql statement &quot;&quot;&quot; dfstaging(df_temp, up_col) df1=pd.read_sql_table('your target table', py2mssql()[0], schema='dbo', index_col=None) ## rename columns in data frame to match with target table df1.rename(column={&quot;user ID&quot;:&quot;User_ID&quot;, &quot;item Key&quot;:&quot;Item_Key&quot;, &quot;connect ID&quot;:&quot;Connect_ID&quot;},inplace=True) df1[&quot;Cust_Id&quot;] = np.nan up_custid = &quot;&quot;&quot; update sql statement &quot;&quot;&quot; dfstaging(df1, up_custid) df2 = pd.read_sql_table('your target table', py2mssql()[0], schema='dbo', index_col=None) ## drop data frame column df3 = df2.drop(columns=[&quot;Connect_ID&quot;,&quot;Item_Key&quot;]) ## join two data frames df_join = pd.merge(df_pd1, df3, left_index=True, right_index=True) df_join[&quot;Process_dt&quot;] = datetime.today().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) df_final = df_join.drop(columns=[&quot;Connect_Id&quot;,&quot;Item_Key&quot;]) ## loop through all columns need to be converted to datetime datatype cvt_lt = [&quot;Work_Queue_Datetime&quot;,&quot;Complete_Datetime&quot;,&quot;Current_StatementDate&quot;] for col in cvt_lt: df_final[col] = pd.to_datetime(df_final[col]) ## get duplicate record from column value df_final[&quot;Dup_Ind&quot;] = [1 if x == &quot;Dup Submission&quot; else 0 for x in df_final[&quot;Reason_Comment&quot;]] ## re-order the columns new_index = [&quot;col5&quot;,&quot;col3&quot;,&quot;col1&quot;,&quot;col2&quot;,&quot;col4&quot;] ## apply the new order df_final = df_final.reindex(columns=new_index) df_final.to_sql('your target table',py2mssql()[0], if_exists='append', schema='dbo', index=None) up_dup = &quot;&quot;&quot; sql statement &quot;&quot;&quot; dfstaging(df_final,up_dup) if __name__==&quot;__main__&quot;: rpt2mssql() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport pandas as pdimport pyodbcfrom sqlalchemy import create_enginedef split_dt(ef_dt): ef_day=(ef_dt//10000)%100 ## extract day info ef_mo=ef_dt//1000000 ## extract month info ef_yr=ef_dt%10000 return pd.Series({'dd': ef_day, 'mm': ef_mo, 'yy': ef_yr})def py2mssql(): engine = create_engine(&quot;mssql+pyodbc://yourServerName/database?driver=SQL Server Native Client 11.0&quot;) con_str = pyodbc.connect(&quot;DRIVER={SQL Server Native Client 11.0}; SERVER=yourServerName; DATABASE=yourDBName; Trusted_connection=yes&quot;) return engine, con_strdef df2staging(df, sql_cmd): df.to_sql('your target table',py2mssql()[0],if_exists='replace',schema='dbo',index=False) conn = py2mssql()[1] cur = conn.cursor() conn.commit() cur.close() conn.close()def df2db(df_type1,df_type2): engine = py2mssql()[0] df_type1.to_sql('your target table', engine, schema='dbo', if_exists='append', index=False) df_type2.to_sql('your target table', engine, schema='dbo', if_exists='append', index=False)def buz2df(): df = pd.read_excel(r'\\\\your input file path\\filename.xlsx', sheet_name='Sheet1', index_col=None) df1 = df.iloc[:,[0,2,3,4,5,8]] ## Pandas equivalents for some SQL analytic and aggregate function ## Top N rows per group which is equivalent to row_number()over() sql window function df2 = df1.assign(rn=df1.sort_values(['your order by column name'],ascending=True) .groupby(['your group column']).cumcount()+1).query('rn == 1').sort_values(['your group column']) df3 = df2['the date column you want to split'].apply(split_dt) df3 = df3.astype(str) ## convert data frame elements as string df3['the data column you want to split'] = df3['mm'] + '/' + df3['dd'] + '/' + df3['yy'] df3['the data column you want to split'] = pd.to_datetime(df3['the data column you want to split']) df2 = pd.concat([df2,df3['the data column you want to split']], axis=1) df_staging = df2.iloc[:,[0,1,5,7,8]] df_type1 = df_staging.loc[:['col1','col2','col3','col4']] df_type2 = df_staging.iloc[:,[2,3,4,5]] df2staging(df_staging) df2db(df_type1,df_type2)if __name__==&quot;__main__&quot;: buz2df()","link":"/2020/12/20/Data-Manipulation-and-ETL-with-Pandas/"},{"title":"A little thought on SQL query performance optimization 1","text":"Working with data and database, writing query is daily routine, query running time might be big different for different queries but serve the same purpose, which shows query performance is not 100 percent determined by database system configuration, server hard ware, table index and statistics etc. but how well do you use SQL to construct your query. To satisfy user strong demand, we built sandbox database on production for business partners to server their needs of BI reporting and business analytical, in the meanwhile, we are also in charge of database maintenance and monitoring so that we got chance to collect all kinds of user queries. Some of them even crash the system or drag the database performance down apparently, here I want to share my thinking of query optimization on large amount of data. Before writing a queryJust like a project, a query is able to achieve the things with backend business logics even if it is small, so make a plan before creation is very necessary to be able to let the query return result in flash and only consume minimum system resources. What level detail of data do you want to query from? Business data is built on certain level, e.g. in common scenario, there are customer level, account level and transaction level data. It’s better clarify target data is on which level or mix different level. What the time scope? The data amount volume would be big if you want to query multiple years even months, so if you evaluate the data is big, it’s better use loop or paging technic instead of returning outcome in one single page. What are query tables look like? A table contains a lot of information, not only business attributes or columns but data type, keys, index, constraints, triggers. Familiar with table structure is going to give additional benefits when you write your queries against those tables. SQL query optimization tips Correlated and uncorrelated subquery Business often asks about the highest transaction information in terms of product, product group, merchant category etc. in certain month or quarter. For this kind of request is not straight forward to be solved by single select from query but it needs subquery. Now let’s see what I got from a business partner 12345678select tran_date, Product_cd, Merchant_Name, Trans_Amtfrom trans awhere tran_date between '2020-10-01' and '2020-10-31' and trans_Amt=( select Max(b.Trans_Amt) from trans b where a.product_cd=b.product_cd and tran_date between '2020-10-01' and '2020-10-31')order by Product_cd subquery is correlated with main query so trans data is loaded into memory again and make a calculation to return data to main query, that query execution time is 32s based on 90mm data. What if change correlated to uncorrelated subquery like below 12345678910select tran_date, Product_cd, Merchant_Name, Trans_Amtfrom trans ajoin (select Product_cd, Max(Trans_Amt) as Trans_Amt, tran_date from trans where tran_date between '2020-10-01' and '2020-10-31' group by Producct_cd, tran_date) as bon a.Product_cd=b.Product_cd and a.Trans_Amt=b.Trans_Amt and a.tran_date=b.tran_dateorder by a.Product_cd the query execution time reduce to 26s. Exists clause and table join The most active accounts and their corresponding spending amount is another important KPI for product manage from account management perspective, additionally, if some condition applied such as account opened on certain month, we got the query from one business partner using exists statement down below 12345678select acct_id, trans_dt,count(*) as num_trans, sum(trans_amt) as tot_trans_amtfrom trans awhere date_key=202010 and exists(select 1 from acct b where a.date_key=b.date_key and a.acct_id=b.acct_id and year(acct_open_dt)*100+month(acct_open_dt)&gt;=202009)group by trans_dt, acct_idhaving count(acct_id)&gt;10order by num_trans desc from the query structure, we might be able to tell exists statement was put in where clause to get the account open date, its execution time is 13s, but it’s obviously not thinking about the whole business logic before, if the query changed to get target accounts for account open date first then did the calculation, the execution time will reduce to 3s like below 123456select a.acct_id, a.trans_dt, count(*) as num_trans, sum(trans_amt) as tot_trans_amtfrom trans a join acct b on a.acct_id=b.acct_id and a.date_key=b.date_keywhere year(b.acct_open_dt)*100+month(b.acct_open_dt)&gt;=202009 and a.date_key=202010group by a.trans_dt, a.acct_idhaving count(a.acct_id)&gt;10order by num_trans desc Reduce the data scope by subquery It’s very necessary to think about your query data scope first when you do a bunch of left outer join, especially, data volume is quite big on your main left table . Below query does a series left join by using all full tables data, but based on business requirement, only partial data is required on the major left table, so it cause somehow resources wasted in reflect on the execution time of 7s 123456select a.*from wfs_trans aleft join credit_decision b on a.tran_id=b.tran_idleft join finance_request c on a.req_id=c.req_idwhere cast(a.creation_date as date)&gt;='2019-01-01'order by creation_date desc after revised above query on left table, the execution time reduced to 5s (2000ms) 123456789select a.*from (select * from wfs_trans where cast(a.creation_date as date)&gt;='2019-01-01') as aleft join credit_decision b on a.tran_id=b.tran_idleft join finance_request c on a.req_id=c.req_idorder by a.creation_date desc Paging browse data Business users sometime complaint the SSRS or Power BI report is slow when they browse data by skipping pages, that is not the programming problem because on the backend the query to support the BI report like below 12345select prod_cd, post_dt, tran_dt, tran_amtfrom transwhere date_key=202010 and prod_cd='ABCD'order by post_dtoffset 1000000 rows fetch next 100 only; that will take 22s to return results based on 80mm data in single month. Thing thing is even the data is ordered by index column post_dt, database engine doesn’t know where is 1000000 row, it needs to recalculate again, so to answer that business concern, we recommend to apply some conditions parameter then start to browse data like below 12345select prod_cd, post_dt, tran_dt, tran_amtfrom transwhere date_key=202010 and prod_cd='ABCD' and post_dt='2020-10-15'order by post_dtoffset 1000 rows fetch next 100 only; by this way, the report will be presented by less than 1s.","link":"/2020/11/30/A-little-thought-on-SQL-query-performance-optimization-1/"},{"title":"Automation Process for Email Attachment Excel in Python","text":"Working with business data, Excel spreadsheet is the most common file type you might deal with in daily basis, because Excel is a dominated application in the business world. What is the most used way to transfer those Excel files for business operation team, obviously, it’s Outlook, because email attachment is the easiest way for business team to share data and reports. Following this business common logic and convention, you may get quite a lot Excel files from email attachment when you involved into a business initiative or project to design a data solution for BI reporting and business analysis. The pain points is too much manual work dragging down efficiency of data availability and also increasing the possibility of human error. Imagine, every day get data from email attachment, you need to check your inbox every once for a while, then download those files from attachment, open Excel to edit data or rename file to meet data process requirement such as remove the protected password, after those preparation works all done, push data to NAS drive, finally, launch the job to proceed the data. It’s not surprise that how easily you might make mistake because any single step contains error would cause the whole process failed. It’s very necessary to automate the whole data process if business project turns to BAU (Business As Usual) program and you have to proceed data in regular ongoing basis. Python and Windows Task Scheduler provides a simple and fast way to solve this problem, now let’s take a look. Overall speaking, this task can be broken down by a couple of steps: Access Outlook to read email and download the attachment Remove Excel file protected password (it’s common in business file to protect data privacy) Manipulate and edit Excel file Copy file to NAS drive Setup the Windows Task Scheduler to run python script automatically. Interact Outlook and Excel with PythonIn this case, we are focusing on local operation because server setting is vary for different production environment. Python standard library can’t meet our need on this request, we have to leverage its third-party libraries to access outlook and interact with Excel so we are going to import win32com, openpyxl and shutil. We firstly work out workflow to guide us to compose the python scripts From the workflow chart, we can tell there are four functions in the streamline. The first one is read email and download attachment readmail(), this step is critical and the most import business logic build-in it. we are going to apply two business logics, one is the latest email, the other one is if the email is latest (current date) then check if the current date -1 equals to data date in the subject line. we will process file when all the requirements are satisfied. For Excel manipulation and spreadsheet decryption are pretty straight forward, no additional business logic so just apply the single function. We create 3 python module files then encapsulate them into the main readmail() function script. fileTransfer_module.py1234567891011121314151617import win32com.client as win32import osimport os.pathimport openpyxl as xlfrom datetime import datetime, timedeltaimport sysimport shutildef cpfile(src_file, tgt_file): if os.path.exists(tgt_file): os.remove(tag_file) shutil.copy(src_file, tgt_file) else: shutil.copy(src_file, tgt_file)if __name__ == &quot;__main__&quot;: cpfile(src_file, tgt_file) excelOp_mudule.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546import openpyxl as xlfrom file_Transfer_module import cpfiledef xltemp(file, file_ind): prd_dir = r&quot;\\\\your NAS drive UNC address&quot; if file_ind == 0: ## for dealing with multiple Excel files fn = &quot;yourExcelFile_1.xlsx&quot; desc_fn = prd_dir + os.sep + fn cpfile(file, desc_fn) elif file_ind == 2: fn = &quot;yourExcelFile_2.xlsx&quot; desc_fn = prd_dir + os.sep + fn cpfile(file, desc_fn) elif file_ind == 1: ## Excel file need to be manipulated wb_raw = xl.load_workbook(file) fn_tgt = &quot;yourExcelFile_3.xlsx&quot; desc_file = prd_dir + os.sep + fn_tgt wb_tgt = xl.Workbook() ws_tgt = wb_tgt.active ws_tgt.title = &quot;Sheet1&quot; ## define work sheet name ws_tgt['A1'] = &quot;Date&quot; ## define the first column name ws_tgt['B1'] = &quot;ID&quot; ws_tgt['C1'] = &quot;Type&quot; ws_tgt['D1'] = &quot;Amt&quot; ws_tgt['E1'] = &quot;Info&quot; cur_sheet = str((datetime.today().day) - 1) ws_src = wb_raw.get_sheet_by_name(cur_sheet) 1 = 1 row_lt = [] for each in ws_src.rows: if each[0].value is not None: row_lt.append(i) i += 1 else: break num_max_row_raw = row_lt.pop() num_max_col = ws_tgt.max_column for i in range(2, num_max_row_raw+1): for j in range(1, num_max_col+1): c = ws_src.cell(row = i, column = j) ws_tgt.cell(row = i, column = j).value = c.value wb_tgt.save(desc_file) wb_tgt.close() wb_raw.close()if __name__ == &quot;__main__&quot;: xltemp(file, file_ind) pwdDecry_module.py123456789101112131415161718192021222324252627282930313233343536import win32com.client as win32import osimport os.pathfrom datetime import datetimeimport sysdef xlpwd(): ''' function xlpwd() is aiming to peel off attachment excel file password to be able to be ready by program arg: opt1_opt2 value: &quot;opt1&quot; to deal with one excel file; &quot;opt2&quot; to deal with another excel file ''' opt1_opt2 = sys.argv[1] excel = win32.Dispatch('Excel.Application') mon = '0' + str(datetime.today().month) ## suppose password is letters and 2 digits month combination if opt1_opt2 = &quot;opt1&quot;: pwd = &quot;randomletters&quot; + mon fn = &quot;yourExcelFile_1.xlsx&quot; sn = 2 ## sheet number st = &quot;tab name 1&quot; elif opt1_opt2 = &quot;opt2&quot;: pwd = &quot;otherrandomletters&quot; + mon fn = &quot;yourExcelFile_2.xlsx&quot; sn = 1 st = &quot;other tab name&quot; prd_dir = r&quot;\\\\your NAS UNC address&quot; file = prd_dir + os.sep + fn wb_tgt = excel.Workbooks.open(file,0,False,5,pwd) ## open encrypted Excel file wb_tgt.Password = &quot;&quot; ## remove password wb_tgt.Worksheets(sn).Name = st ## define the tagart worksheet order and name wb_tgt.Save() wb_tgt.Close() excel.Quit()if __name__ == &quot;__main__&quot;: xlpwd() readMail.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import os, os.pathimport sysfrom datetime import datetime, timedeltaimport win32com.client as win32from excelOp_mudule import xltempdef readMail(): ''' function readMail is aiming to read outlook email attachment and download them arg: lob, app, c_type value: lob == 'you line of business' and app == 'application name generate the source file' and c_type == 'business category' ''' lob = sys.argv[1] app = sys.argv[2] c_type = sys.argv[3] outlook = win32.Dispatch(&quot;Outlook.Application&quot;).GetNameSpace(&quot;MAPI&quot;) type1 = outlook.Folders[&quot;your email address&quot;].Folders[&quot;your inbox subfolder1&quot;] type2 = outlook.Folders[&quot;your email address&quot;].Folders[&quot;your inbox subfolder2&quot;] type3 = outlook.Folders[&quot;your email address&quot;].Folders[&quot;your inbox subfolder3&quot;] ## if your inbox subfolder name has convention you can use while loop inbox = outlook.GetDefaultFolder(6) ## Microsoft Outlook API number for inbox is 6 type1_msgs = type1.Items type2_msgs = type2.Items type3_msgs = type3.Items inbox_msgs = inbox.Items file_ind = 0 folder = &quot;&quot; if lob == &quot;finance&quot; and app == &quot;app1&quot; and c_type == &quot;consumer&quot;: mail_items, file_ind, folder = type1_msgs, 0, &quot;finance_files&quot; elif lob == &quot;marketing&quot; and app == &quot;app2&quot; and c_type == &quot;small business&quot;: mail_items, file_ind, folder = type2_msgs, 1, &quot;small business files&quot; elif lob == &quot;inventory&quot; and app == &quot;app3&quot; and c_type == &quot;cooperate&quot;: mail_items, file_ind, folder = type3_msgs, 2, &quot;cooperate files&quot; else: mail_items = None path = r&quot;\\\\your file staing folder directory&quot; + os.sep + folder num_mails = len(mail_items) lst_mails = list(reversed(range(num_mails))) id_mail = lst_mail[0] email = mail_items[id_mail] subject = email.Subject ## Outlook API Subject line object if file_ind in (0, 2): num = -13 ## depends on your own situation date_mail_str = subject[num:] if date_mail_str[0] != ' ': date_mail_dt = datetime.strptime(date_mail_str, &quot;%b. %d, %Y&quot;) else: date_mail_dt = datetime.strptime(date_mail_str, &quot; %b. %d, %Y&quot;) elif file_ind == 1 and datetime.today().strftime(&quot;%a&quot;) != &quot;Mon&quot; and datetime.today().day &lt;= 10: date_mail_str = subject[-8:-1] + '2020' date_mail_dt = datetime.strptime(date_mail_str, &quot; %B %d %Y&quot;) elif file_ind == 1 and datetime.today().strftime(&quot;%a&quot;) != &quot;Mon&quot; and datetime.today().day &lt;= 10: date_mail_str = subject[-8:-1] + '2020' date_mail_dt = datetime.strptime(date_mail_str, &quot;%B %d %Y&quot;) received_time = email.ReceivedTime ## Outlook API receive time object today = datetime.today() ## check availiability of the latest file if today.year == received_time.year and today.month == received_time.month and today.day == received_time.day: avail_ind = 1 else: raise AttributeError(&quot;the latest file is not available! check with business team&quot;) ## check if the file is right copy if avail_ind == 1 and (file_ind == 0 or file_ind == 1): val_dt = received_time - timedelta(days = 1) if date_mail_dt.day == val_dt.day and date_mail_dt.month == val_dt.month and date_mail_dt.year == val_dt.year: valid_copy_ind = 1 ## usually business file for current date is yesterday's data if data is daily basis else: raise AttributeError(&quot;file copy is not right! check with business team&quot;) elif avail_ind == 1 and file_ind == 2: val_dt = received_time if date_mail_dt.day == val_dt.day and date_mail_dt.month == val_dt.month and date_mail_dt.year == val_dt.year: valid_copy_ind = 1 ## sometime current date file is current date data depends on business process else: raise AttributeError(&quot;file copy is not right! check with business team&quot;) if valid_copy_ind == 1: attachment = email.Attachment.item(1) report_name = attachment.FileName os.chdir(path) input_file = os.getcwd() + os.sep + date_mail_str + report_name if not os.path.exists(input_file): attachment.SaveAsFile(input_file) xltemp(input_file, file_ind) if __name__ == &quot;__main__&quot;: readMail() Schedule jobs to auto check your inbox and execute python scriptsIf you use Linux as your local machine then there are so many scheduling tools such as crontab, for Windows users, you can use GUI tool Task Scheduler to to the automation scheduling task. In this case, we use Task Scheduler. simply follow the wizard to create local jobs to implement above readMail.py and pwdDery_module.py scripts. After decrypted Excel files push to your server then trigger server jobs so that make the whole data process automated, no more manual work.","link":"/2020/12/15/Automation-Process-for-Email-Attachment-Excel-in-Python/"},{"title":"Generate non comma delimiter CSV file","text":"This is some tip but sometime makes you life easier when you work with business team and deal with data from business input. Excel spreadsheet is kind of standard file format to communicate with business team, but in data manipulation side, it’s not a ideal input data format, technically, we usually convert spreadsheet to csv file. But default comma delimiter might cause some trouble because business data might contains quite a lot of , in attributes such as comments, suggestions, reasons etc. In order to better identify the column, non-comma delimiter should be used like | pipe. How do we generate the | delimiter csv file. There are two ways Change format setting system-wiseIn windows, open the control panel, find the Region setting, on the Formats tab click Additional setting, a pop-up window will show up, on that window, find the option list separator then type whatever delimiter you want to setup like |. ) save the setting, when you Save as csv file in excel, it will generate | separated csv file afterwards. Use database client tool to save query result to csv file with customized delimiterSome sql database client applications provide the functionality to save query result to csv file, like WinSQL. Before running your query, select execute to save to a text file, then select | from drop-down list","link":"/2020/12/07/Generate-non-comma-delimiter-CSV-file/"},{"title":"Blocking Process Monitoring and Auto Email Notification in SQL Server","text":"From function perspective, to maintain a large scale business data warehouse is for making database system stable, robust and fast, which is a essential part to boost business team productivity and performance. However, for business users, the fundamental thing is data, so data can be delivered in high frequency and in time is the cornerstone for all business analysis and BI reporting. Obviously, the primary mandate for data management team is highly monitor ETL jobs to promise data process running well and smooth and never being blocked or corrupt by using process. In this article, I am going to talk about how to build up a ETL job monitoring system to watch user query automatically in designed frequency. To accomplish this task, we need Create a view to collect user query in real time Create a stored procedure to detect blocking in different situations Create another stored procedure to handle the notification email sending Create a console script to overall control the workflow to make clear with those step by a process workflow chart, we can easily to see the logic on behand the scene Firstly, SQL agent job is going to check blocking transaction every 30 minutes, if there are some user queries blocked ETL job execution service account, then check back database to determine if those transactions are new, if they are new then write into database Blocking_Transaction table and send email notification to data management team to raise awareness; but if those blocking are not new, update last_time and count column in table then calculate if count number reach to 4 if yes, then send email to user and data management team. Create a view to collect user query in real time The first and most important thing for us is get the all transaction records against database so that we are able to know which user query transactions would block service account conduct ETL job. We need to utilize sys schema tables or dmv (dynamic management views) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071use controlDB;gocreate view dbo.usrQueryMonioringaswith t1 as(select b.spid, c.DBName as [DB_Name] ,a.total_scheduled_time/1000 as TotalScheTime_SS ,a.total_scheduled_time/1000/60 as TotalScheTime_MM ,a.total_elapsed_time/1000 as TotalElapTime_SS ,a.total_elapsed_time/1000/60 as TotalElapTime_MM ,a.last_request_start_time ,a.last_request_end_time ,a.login_time ,a.login_name ,a.host_name ,a.program_name ,a.nt_domain ,a.nt_user_name ,a.status ,a.is_user_process ,b.blocked --add blocking query information to quick locate the blocking transactions ,b.cmd, ,b.memusage*8 as Memusage_KB ,b.memusage*8/1024 as Memusage_MB ,c.Query from master.sys.dm_exec_sessions a join master.sys.sysprocesses b on a.session_id=b.spid and a.login_time=b.login_time cross apply ( select from master.sys.dm_exec_sql_text(b.sql_handle) ) c where a.is_user_process=1 --consider spids for user only, no system spids and a.session_id!=@@SPID --don't include request from current spid),t2 as(select spid ,DB_Name ,TotalScheTime_SS ,TotalScheTime_MM ,TotalElapTime_SS ,TotalElapTime_MM ,blocked as BlockedBy ,last_request_start_tiem ,last_request_end_time ,login_time ,login_name ,case when host_name like 'your server name%' then 'Server' else 'Client' end as 'Host Name' ,case when program_name like 'Microsoft SQL Server Mangement Studio%' then 'SQL Query' when program_name like 'Python' then 'Python Access' else 'Server general job or data provider' end as 'Program Name' ,nt_domain ,nt_user_name ,status ,case is_user_process when 0 then 'SA System' when 1 then 'User Initiate' end as 'Is_User_Process' ,cmd ,Memusage_KB ,Query from t1)select *from t2;go Before moving forward, we need a middle table to store all service account blocking transaction records 1234567891011121314151617use controlDB;gocreate table dbo.blocking_transaction(TraceID int identity(1,1) not null ,spid smallint not null ,blokedBy ,login_name nvarchar(128) not null ,nt_user_name nvarchar(128) null ,TotalElapTime_MM int not null ,ps_time datetime not null ,fr_Time datetime not null --first record time ,lr_Time datetime not null --last record time ,counter smallint not null ,cmd nchar(16) not null ,query varchar(max) null) Create a stored procedure to detect blocking in different situationsNext we will apply the main logics to detect service account blocking transactions in terms of different scenarios when we have the query transaction records data. In this time, we need to create a stored procedure to be executed by SQL job. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586create proc dbo.usp_blocking_trans (@counter smallint output, @rowcnt int output)asif OBJECT_ID(N'tempdb..#blked') is not nulldrop table #blked;declare @row_cnt intdeclare @sSQL nvarchar(max)declare @sParam nvarchar(4)begin trywith t1 as(select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from controlDB.dbo.usrQueryMonioring where BlockedBy!=0 and login_name='your service account'),t2 as(select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from t1 except select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from t1 a join t1 b on a.spid=b.BlockedBy --remove duplicates),t3 as(select spid, BlockedBy, login_name, nt_user_name, TotalElapTime_in_MM,last_request_start_time,cmd,query from controlDB.dbo.usrQueryMonioring where spid in (select BlockedBy from t2) -- in operator to hold potential multiple transactions),t4 as(select * from t2 union all select * from t3)select *into #blkedfrom t4/*return 1 if no record in temp table*/if not exists(select * from #blked)beginprint 'no record in temp table'return 1end/*check for incremental load*/elseselect @row_cnt=count(a.spid)from #blked a left join controlDB.dbo.blocking_transaction bon a.spid=b.spid and a.last_request_start_time=b.ps_timewhere b.spid is null and b.ps_time is null;if @row_cnt&gt;=1beginset @sSQL=N'insert into dbo.blocking_transactionselect a.spid,a.BlockedBy,a.login_name,a.nt_user_name,a.TotalElapTime_MM,a.last_request_start_time,getdate(),getdate(),1,a.cmd,a.queryfrom #blked a'exec sp_executesql @sSQLselect @rowcnt=@@ROWCOUNT --output number of row insertedreturn 2end/*update the old records on last_record_time and count if counter&gt;=3*/elseselect top(1) @counter = counter from controlDB.dbo.blocking_transaction order by lr_time desc;update a set lr_time=getdate(),counter=@counter+1from blocking_transaction a inner join #blked bon a.spid=b.spid and a.ps_time=b.last_reqeust_start_time;if (select top(1) counter from blocking _transaction order by lr_time desc)%4 = 0return 3elsereturn 4drop table #blkedbegin catchdeclare @err_msg nvarchar(1000),@err_num int,@err_line int,@syserr nvarchar(1000)select @err_msg=ERROR_MESSAGE(),@err_num=ERROR_NUMBER(),@err_line=ERROR_LINE()SET @syserr=N'usp_blocking_trans ended with error: Line='+convert(nvarchar(3),@err_line)+', Error_Msg='+@err_msg+''return -1end catchend Create another stored procedure to handle the notification email sendingSending email to raise awareness is another important task for the entire monitoring system, in this case there are two server levels, if user query blocking time less than 4 (4*30=120 mins) only email to data management team, otherwise, email to both data management team and user to cancel the transaction. If user ignores then data management team would do something to clear the lock by policy. we use parameters output from stored procedure usp_blocking_trans as input to be the key conditions 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106use controlDB;gocreate procedure dbo.usp_blocking_Email( @case smallint, @times smallint, @rownum int --handle multiple blocking case)asbegin trydeclare @result intdeclare @runtime nvarchar(20)declare @subject nvarchar(500)declare @sSQL nvarchar(200)declare @sParam nvarchar(100)declare @spid nvarchar(4)declare @body nvarchar(max)declare @counter intset @runtime=convert(nvarchar,GETDATE())set @sSQL=N'select top 1 @bID=blockedBy from controlDB.dbo.blocking_transaction where nt_user_name=''your service account'' and blockedBy!=0 by tranceID desc'set @sParam=N'@bID nvarchar(4) output'exec sp_executesql @sSQL, @sParam, @bId=spid outputset @subject=N'Warning: ETL job being blocked on '+@runtime+N'by SPID'+@spid+N', check it out!'if @case=2set @body=N'&lt;p style=&quot;color:#000000;font-family: Georgia; font-size: 18px; line-height: 18px&quot;&gt;Hi Team,&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;color:#000000;font-family: Georgia; font-size: 18px; line-height: 18px&quot;&gt;A job is now being blocked by a user process with &lt;font color=&quot;#FF5733&quot;&gt;&lt;b&gt;SPIS='+@spid+'&lt;/b&gt;&lt;/font&gt;&lt;br /&gt;&lt;br /&gt;Please check [controlDB].dbo.[blocking_transaction] table.&lt;br /&gt;&lt;br /&gt;&lt;/i&gt;&lt;font size=&quot;4&quot;&gt;More details information shown on below&lt;/font&gt;&lt;br /&gt;&lt;br /&gt;Thanks and Best Regards,&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;'+N'&lt;style&gt; th { background: #33FFBD height: 30px; } table,th,td { border: 2px solid green; } table { width: 80%; color: black; text-align: center; }&lt;/style&gt;'+ N'&lt;h2&gt;&lt;center&gt;&lt;font color=&quot;red&quot;&gt;ETL Job Info for Blocking&lt;/font&gt;&lt;/center&gt;&lt;/h2&gt;'+ N'&lt;table align=&quot;center&quot;&gt;' + N'&lt;tr&gt;'+ N'&lt;th&gt;SPID&lt;/th&gt;&lt;th&gt;BLkdBY&lt;/th&gt;' + N'&lt;th&gt;User Name&lt;/th&gt;' + N'&lt;th&gt;Elapse Time in MM&lt;/th&gt;&lt;th&gt;Process Run Time&lt;/th&gt;' + N'&lt;th&gt;First Record Time&lt;/th&gt;&lt;th&gt;Last Record Time&lt;/th&gt;' + N'&lt;th&gt;Counter&lt;/th&gt;&lt;th&gt;CMD&lt;/th&gt;' + N'&lt;/tr&gt;' + cast(( select top (@rownum) td=[spid],'' ,td=[blockedBy],'' ,td=[nt_user_name],'' ,td=[TotalElapTime_in_MM],'' ,td=[ps_time],'' ,td=[fr_time],'' ,td=[lr_time],'' ,td=[counter],'' ,td=[cmd],'' from controlDB.dbo.blocking_transaction order by TranceID desc FOR XML PATH('tr'), TYPE ) as nvarchar(max)) +N'&lt;/table&gt;' +N'&lt;p&gt;&lt;br /&gt;&lt;/p&gt;' +N'your team name';elseif @case=3set @body=N'&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;color:#000000;font-family: Georgia; font-size: 18px; line-height: 18px&quot;&gt;Hi Team,&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;A ETL job is now still being blocked by a user process with spid='+@spdi+' for more than '+@times+' checks adn '+@times*30+' minutescheck previous email or controlDB.dbo.blocking_transaction table for more details!&lt;br /&gt;&lt;br /&gt;your team name'execute @result=msdb.dbo.sp_send_dbmail@profile_name=N'your SMTP profile name',@recipients = N'email1@yourcompany.com;email2@yourcompany.com',@copy_recipients = N'email3@yourcompany.com',@subject = @subject,@body = @body,@body_format='HTML';return 0end tryend Create a console script to overall control the workflowat the last step, we need a control script put all above stored procedure together to be able to schedule SQL Agent job. 1234567891011121314151617181920212223use controlDB;godeclare @result_status smallintdeclare @counter smallintdeclare @rowcnt intexec @result_status=usp_blocking_trans @counter=@counter output, @rowcnt=@rowcnt outputif @result_status=1beginprint('no blocking process')returnendelseif @result_status=4beginprint('no new blocking process')returnendelsebeginset @counter=@counter+1exec usp_blocked_Email @case=@result_status, @times=@counter, @rownum=rowcnt","link":"/2020/12/17/Blocking-Process-Monitoring-and-Auto-Email-Notification-in-SQL-Server/"},{"title":"How to let local images display on your hexo blog website","text":"Hexo blog framework friendly supports markdown which is the most popular syntax for technical blog writing. When you try to illustrate some ideas, it’s common to use screenshot photos in your articles to get reader better understanding so that you have to insert images into your blog, unlike typing text content that is straight forward, inserting images and deploying them to public blog site might be some tricks there, let’s see some common scenarios on images display issue and how to resolved them. Use absolute path to insert imageThe intuitive way for junior blogger is referencing absolute path + file name, I would say there is no any problem if the blog is only for local review or your local machine is a web server to host your blog website, otherwise, your images won’t be displayed on your public blog site after deployment if without any configuration. Like below shows when you use that way inert on your local blog, images can be displayed as expected but it will cause issue on display when you deploy your article to public blog website like below shown Use relative path based on _config.yml fileActually, hexo blog is powered by Node.js in the backend, its general configuration file _config.yml shows the way to properly allocate your resources including image, css, font and js etc. By default, your images should be put into blog/themes/your theme name/source/images , when you reference it, use below syntax no matter you use Windows or Linux, 1![your image's name](/images/your_image_file_name.png) now, you can push your blog to public blog site and your images can be presented there well but the problem is you lose your sight on local but only are able to see on blog website after you deployment, that is not you really want like below shown Use online image management toolTo be able to solve image display problem on both local and public site, the best way is utilize online image management website to upload your images then it will generate corresponding URLs for those photos, you can replace path name of your local images by URL. Now your image is going to be rendered by both your local markdown editor and web browser. Another advantage of managing your photos by online image tool is your images are in the cloud with permanent URLs so that you won’t lose them even something bad happened on your local machine. I use wailian.work to manage my images, it’s free and provide markdown link for each of your uploaded picture. A small thinking about Node.js top route design and file renderThe root cause of confusion here might be the Node.js route design, unlike Apache and Nginx web server which come with web container so that the route can be your local folder path name and render your local html file in the folder, in this case, you do tell the file local path from URL address, but for Node.js, it doesn’t have web container, therefore, we have to design route for local web files, in other words, you are not able to tell the local file path by URL address for Node.js web server. Let’s see an example 123456789101112131415161718const fs = require('fs');const path = require('path');const url = require('url');exports.static = function (request, response, staticPath) { let pathname = url.parse(request.url).pathname; pathname = pathname == '/' ? '/index.html' : pathname; let extname = path.extname(pathname); if (pathname != '/favicon.ico') { fs.readFile('./' + staticPath + pathname, async (err, data) =&gt; { if (!err) { let mime = await getFileMime(extname.split('.')[1]); response.writeHead(200, { 'Content-Type': `${mime}; charset=&quot;utf-8&quot;` }); response.end(data); } }) }} Assume user request images, the URL is probably like https://yourdomainname/images, but in the backend, in fact the Node.js read file from path './' + staticPath + pathname such as /root/web_project1/static/images. URL address doesn’t reflect file path, they are relatively independent in Node.js, by knowing that, we are going to dig a little deeper to take a look at _config.yml file and come up with idea why browser is able to render out to image file but text editor can’t. Open _config.yml file 1vim ./blog/_config.yml 12345# URL## If your site is put in a subdirectory, set url as 'http://example.com/child' and root as '/child'url: https://hermanteng19.github.ioroot: /permalink: :year/:month/:day/:title/ here list the route on your blog site which should be https://hermanteng19.github.io/ as home page and corresponding html file should be put into “/“ directory, what about images file? It is going to be “/images”, so far, we know the directory in below actually is route or web request URL, that is why text editor is not able to find the real file path. 1![mysql-connection](/images/mysql_conn_established.png) But, you might still confuse about why browser can render the image file, now let’s continue to see what’s going on web server. Enter your local hexo blog path 1ls -l ./blog you can find there is public/ folder which is your blog website “/root” folder, all web files such as html, css, js, image, font are in there, let’s enter it and take a look images/ folder is on the list, continue to drill down, mysql_conn_established.png file is on the list. So now you may be clear by Node.js route design, when web browser visits images on your blog website, it actually read the files from “/public/images/“, not from your local file absolute path like “themes/your_theme_name/source/images”. It does cause some confusion by this special type of route design, but in the other way, I would say it’s quite smart, you can design out a very nice, neat and beautiful URL for your website, which would bring very good user experience and impress website visitors. Think about no matter how ugly your local file path is like /root/jfdioaw/_jfidosjo_jfi123/oneMoreFolder/13u8030/product_1.html, but your website URL always looks like http://yourdomainname/business/product/product_1.html. At last, I want to thank CodeSheep for help me with the idea of leveraging online photo mangement tool to solve this problem!","link":"/2020/11/27/How-to-let-local-images-display-on-your-hexo-blog-website/"},{"title":"Hadoop Data Side Load from SQL Server","text":"Agile development and DevOps bring flexibilities and quick solutions to support business intelligent in timely manners. A technical platform and its associated applications and tools are able to turnaround very quick so that business analysts and data scientists would be able to leverage them to do the data modeling or machine learning, but in the other side, unlike functions buildup, data sync across different platforms is not that easy and quick especially for large organizations. Background and the gap of data modeling for Hadoop early adopterThese years, big data and Hadoop are kind of trend for next generation data technic. Many companies adopt that as major data platform, but the most of data is still allocated in RDBMS data warehouse, business intention is to leverage high quality data in SQL database to build their analytical work in Hadoop, the data consistency is the first consideration from data perspective, but it is not a easy task because data is going to migrate to different platform with different operating system (from Windows to Linux). Technically, the best solution for the project is the build the direct connection from SQL Server and SSIS to Hive by using Apache Sqoop or utilize the JVM to build JDBC connection by JAVA, but for large organization, applying a new tool on production needs a quite lot approve work; developing JDBC connection facility also needs multiple level testing, those are taking a long time. Therefore the solution is back to the foundation of the Hadoop - file system. Because SSIS cannot write to Hive directly using ODBC (before 2015 version). The alternative is to create a file with the appropriate file format and copy it directly to the Hadoop file system then use Hive command to write metadata to Hive metastore, the data will show up in the Hive table and also available in Cloudera Impala. File operation for data migration from SQL to HadoopIn our case, moving files can be a litter more complex because of crossing different operating system and platform as noted earlier. In this case, we need t a way to execute the dfs -put command on the remote server. Server tools enable us to execute the remote processes. Hadoop is build upon Linux, so bash shell script to execute the remote process. But that is quite challenged in real operation in production environment. What if the number of files is huge and cannot done by manually issuing command but have to do the batch operation automatically. What if files being moved crossing multiple platform from Windows to Linux local file system to Hadoop file system and if the data would be able to keep the original value and format in Hive or Impala table. Generate source data by CSV file from Windows sideFor multi-processing, SSIS is the sounds solution and tool set on Windows side which enables us to run SSH commands on the remote server from an Execute Process Task. Setting up a package to implement this process is relatively straight forward. Just set up a data flow as normal with a source component retrieving data from SQL Server data warehouse. Any transformation actions that need to be applied to the data can be performed. Ad the last step of the data flow, the data needs to be written to a file. The format of the file is determined by what the Hive system expects. The easiest format to work with from SSIS is a delimited format, with carriage return/line feeds delimiting rows, and a column delimiter like a comma(,) or pipe (|) separating column values. The SSIS flat file destination is designed to write these types of files. Automate file operation from Windows to Linux and HDFSOnce the file is produced, then use a file system task to copy it to a network location that is accessible to both SSIS server and Hadoop cluster. The next step is to call the process to copy the file into the HDFS. This is done through an Execute Process Task. It can be configured to use expressions to make this process more dynamic. In addition, if you are moving multiple files, it can be used inside a For loop in SSIS to repeat the process a specified number of times. To FTP data file from Windows to Linux, we can use either sftp or scp command but first you should make sure you have Hadoop access. But the problem is when we copy data to Hadoop, we need to provide the password to access the system, so the process is manual. As mentioned before, for larger scale data migration, we have to figure out the way of automation so that silent mode is needed to avoid manually provide user password then we can use script to schedule job to automate data process. Check OpenSSH Client installed and enable in your machineOpen Windows setting -&gt; App -&gt; Apps &amp; features -&gt; Optional feature, check if OpenSSH Client is on the list, if not you need to install that client tool On source side to create a public access keyThis step we need use cmd prompt or git bash, open git bash, change directory to home folder by issuing command cd $HOME, you will find the hidden folder .ssh, you can display all folder by command ls -la Issue below command to generate the public access key in order to enable the silent mode to Hadoop 1ssh-keygen -b 2048 -t rsa tap 3 time enter key to generate the key, the value for argument -b is key size and has to be the number at least 2048, value of -t is algorithm. You will find the 2 new files are generated, id_rsa stores your identification information and id_rsa.pub is your public key which we need to copy to Linux server. On target side FTP public key file to itWe can use scp command to upload public key to Linux server. 1scp -p 22 /home/yourUserName/.ssh/id_rsa.pub yourUserName@LinuxServerName:/home/yourUserName/.ssh/authorized_key You need to give password when issue above command to pass the authentication, but later this step will be bypass because of the public access key. Load file from Linux file system to HDFSEven Hadoop is installed on Linux, it has independent file operating system called HDFS, we need to issue command to transfer file between Linux and Hadoop. hdfs dfs command to manipulate Hadoop files, Hadoop is kind of remote server so we use -put argument to push file from Linux local to Hadoop; use -get to push file from Hadoop to Linux local, is this case we use put. 1hdfs dfs -put /yourLinuxFilePath/fileName.csv /HadoopFilePath/targetHiveDatabase.db Now, data file is in HDFS so that we can issue Hive command to write the table metadata into Hive metastore Configure Hive table metadata to match up with SQL ServerIn Hadoop side, we need to define Hive tables to store the data from SQL Serer. For keeping data consistency, we define the Hive table schema in terms of SQL table data type. We don’t use Ctrl-A(0x001) which is the default Hive column delimiter for flat file but use pipe bar(|) as field delimiter, because that isn’t supported well for use from SSIS in Window platform. Now let’s dig a litter deeper on Hive data type which is the critical element and the most important consideration to keep data consistent. Hive provides a layer on top of Hadoop data that resembles a RDBMS. In particular, Hive is designed to support the common operations for data warehousing scenarios. Thanks to Hive to build the bridge between Hadoop MapReduce and RDBMS so that many of these data types have equivalent values in SQL Server, but only a few are unique to Hive. Type Description Examples SQL Server Equivalent Float 4-byte single-precision floating point 25.189164 real Double 8-byte double-precision floating point 25.1897645126 float(53) Decimal A 38-digit precision number 25.1897654 decimal, numeric Boolean Boolean true or false TRUE FALSE bit Timestamp JDBC-compliant timestamp format YYYY-MM-DD HH:MM:SS:ffffffffff datetime, datetime2 define and create a Hive table is simple, the syntax just like SQL, but something different after all, it’s file system. 1234567891011121314create table yourTableName(col1 bigint, col2 int, col3 varchar(5), col4 decimal(5,2), col5 smallint, col6 tinyint, col7 timestamp, col8 float)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '|'STORED AS TEXTFILEtblproperties (&quot;skip.header.line.count&quot;=&quot;1&quot;) If data files come with header then you need to tell Hive skip the first row by tblproperties. We save above script as createTbl1.hql then use Hive beeline command to generate table 1beeline -f ./yourPath/createTbl1.hql &gt; /yourPath/hql.ot Load data file to Hive and Impala tableAs long as we push data file to Hadoop and Hive directory, we can easily to write data to Hive table by issuing beeline -e command like below 1beeline -e &quot;LOAD DATA INPATH '/HadoopFilePath/targetHiveDatabase.db/fileName.csv' INTO TABLE yourTableName&quot; the metadata will be in the Hive metastore. Cloudera Impala metastore is not sync with Hive automatically, so if you want to manipulate data in Impala, we need to issue impala command 12impala -q &quot;invalidate metadata yourTableName&quot;impala -q &quot;refresh yourTableName&quot; after that data is on both Hive and Impala table. Automate the whole processTo get a better understanding of how the process looks like, below workflow illustrates how data file read from SQL Server side and load into Hive table in HDFS cluster. now let’s create a script to bulk copy data file to Linux and save it as bulkcopy.sh 1scp -p 22 -r /yourDataFileFolder/ user@host:/home/user/dataFileFolder then for the task in Linux server side, we also create a script to push data from Linux local file system to HDFS then write data to Hive table and save it as bulkload.sh 1234hdfs dfs -put /yourLinuxFilePath/fileName.csv /HadoopFilePath/targetHiveDatabase.db/ &amp;&amp;beeline -e &quot;LOAD DATA INPATH '/HadoopFilePath/targetHiveDatabase.db/fileName.csv' INTO TABLE yourTableName&quot;impala -q &quot;invalidate metadata yourTableName&quot;impala -q &quot;refresh yourTableName&quot; so far, we have csv data file, bulkcopy.sh and bulkload.sh, now we will schedule a job to run those scripts automatically by using SSIS package last, configure the SSIS package to write log data either file or database for debugging. Both Hive table and Impala table will be good after run command invalidate metadata command in Impala shell for both HUE (GUI tool) and Linux server. Invalidate metadata command takes effect across both Impala shell and HUE web interface, but refresh command only tales effect on the environment the command run against. Data restatementIt’s quite common practice in production to reload historical data with reconcile with new or changed logic or even correct some mistake so remove partial data and then reload is also important. Than task is easy for RDBMS but what if the same situation take place in Hadoop, there is no update statement in Hive or Impala, so we need to do multiple steps: Start state: number of data, e.g. 3 files need to restate in HDFS Issue command hdfs dfs -rm to delete the 3 files with month_key (3 months) Run query from HUE, make sure 3 month data and won’t impact other month Run above scripts and SSIS package add new 3 data files back again, then the data would be back to the Hive and Impala table","link":"/2020/12/22/Hadoop-Data-Side-Load-from-SQL-Server/"},{"title":"Linux Network Management Tool nmcli","text":"There are over 60 Linux networking commands you can utilize to do all the system network configurations, some of them are well known and widely used such as ifconfig, ip addr, traceroute, netstat and ping etc.., one command is very useful but relatively few being used, that is nmcli which is used for controlling your network, just like its name network manager and also can do all the thing to configure your network like displaying network device status, create, edit, activate/deactivate and delete network connection. syntax and optionsnmcli command has 2 arguments, one is option and the other one is object. 1nmcli [options] object {command | help} You can quick check help to get the information nmcli -h OPTIONS -t[erse] terse output -p[retty] pretty output -m[ode] tabular|multiline output mode -c[olors] auto|yes|no whether to use colors in output -f[ields] &lt;field1,field2,…&gt;|all|common specify fields to output -g[et-values] &lt;field1,field2,…&gt;|all|common shortcut for -m tabular -t -f -e[scape] yes|no escape columns separators in values -a[sk] ask for missing parameters -s[how-secrets] allow displaying passwords -w[ait] set timeout waiting for finishing operations -v[ersion] show program version -h[elp] print this help OBJECT g[eneral] NetworkManager’s general status and operations n[etworking] overall networking control r[adio] NetworkManager radio switches c[onnection] NetworkManager’s connections d[evice] devices managed by NetworkManager a[gent] NetworkManager secret agent or polkit agent m[onitor] monitor NetworkManager changes From command syntax, you can tell the options can be multiple but object is only one, because nmcli only return information or do configure for one object a time. the place of options and object can not be switched, must follow option first and object second. ExamplesShow network status1nmcli -p general status nmcli permission1nmcli general permission Enable and disable network1nmcli networking on | off | connectivity Radio wifi transmission control1nmcli radio wifi | wwan | all Show local network connection1nmcli connection show Show network device status1nmcli device status Show overall network and device information1nmcli device show that command will show device, device type, network connection, gateway, route, ip4, ip6, dns Show wifi connection status1nmcli device wifi list Configure wifi connections for UbuntuBy default, Ubuntu wifi connection is disabled, so if you want to use wifi, something need to be done to be able to connect to WAN. Firstly, show your wifi device 1nmcli device status Secondly, turn on the wifi radio transmission 1nmcli radio wifi on Thirdly, show all your wifi networks 1nmcli device wifi list Finally, connect your wifi network by password 1nmcli device wifi connect &lt;your wifi network name&gt; password &lt;your password&gt;","link":"/2020/12/14/Linux-Network-Management-Tool-nmcli/"},{"title":"Remote Connect MySQL Server from Client Machine Setup","text":"You can only connect to MySQL Server from localhost after MySQL installation by default, but in production, all MySQL clients remotely connect to server, for simulating real production environment in your home network, some configurations need to be made to be able to let you connect MySQL from client machine other than localhost. Revise or create MySQL configuration file (RHEL or Centos 7)Modify or create /etc/my.cnf file1vim /etc/my.cnf add a configuration item bind-address and let it value to be your MySQL server host ip address (eg. 192.168.1.114)1bind-address=192.168.1.114 save and exit1:wq Restart MySQL service1systemctl restart mysql.service Open TCP port 3306 using iptablesSetup /sbin/iptables and let firewall opens port on 3306 for any remote machine1/sbin/iptables -A INPUT -i eth0 -p tcp --destination-port 3306 -j ACCEPT To specific client host machine to access port 3306, you can explicitly assign ip address (eg. 192.168.1.134)1/sbin/iptables -A INPUT -i eth0 -s 192.168.1.134/24 -p tcp --destination-port 3306 -j ACCEPT Finally save IPv4 firewall rules1/sbin/iptables-save &gt; /etc/sysconfig/iptables If it doesn’t work, for testing and develop environment, you can turn off firewall12systemctl stop firewalld.servicesystemctl disable firewalld.service Grant remote access to new MySQL databaseCreate a new database1create database foo; Grant remote user access to a specific database on user host machine (192.168.1.134)1grant all privileges on foo.* to herman@'192.168.1.134' identified by 'youOwnPasswd'; Grant remote access to existing MySQL database for user (herman) on its host machine (eg. 192.168.1.134)Require a set of two commands12update db set Host='192.168.1.134' where Db='mysql';update user set Host='192.168.1.134' where user='herman'; Open MySQL client tool on workstation with address 192.168.1.134 like MySQL workbenchCreate new connection using username and passwordusername: hermanpassword: youOwnPasswdport: 3306 Connection is created and foo database is on the list under user herman","link":"/2020/11/25/Remote-Connect-MySQL-Server-from-Client-Machine-Setup/"},{"title":"Linux network auto boot and restart","text":"Centos 7 network is disabled by default after installation and initialization, which causes network connection can not be made until you manually turn on it That is such annoying when you usually use SSH tool remote connect to centos workstation or server, so it’s quite necessary to turn on the network automatically every time reboot machine or VM. To accomplish that, we need to modify the network configuration file. Auto Boot NetworkVIM open config file: /etc/sysconfig/network-scripts/ifcfg-ens331vim /etc/sysconfig/network-scripts/ifcfg-ens33 Revise the ONBOOT value to be “yes” Save and quit config file by :wq vim commandRestart NetworkTo be able to make that change taking effect, network needs to be restarted by below command 1systemctl restart network.service something trick here is some blog articles mention that part is only systemctl restart, which won’t work if there is no network.service, I confuse and spend many time to figure out that, thanks to CodeSheep whose video shed the light on it and help me to solve that problem.","link":"/2020/11/25/Linux-network-auto-boot-and-restart/"},{"title":"Oracle client side configuration for 12C","text":"The client side need to do some configurations after Oracle 11g upgrade to 12C on Server in order to make database server is connectable. Before starting to configurate your clients, you have to get the below new server information from DBA Host name Port number Service name Your user name(usually it won’t be changed and replicated from old version) Password(initial password for test connection then you need to update it) and then you need to make a new connection strings to add it into ORA file(*.ora) Oracle SQL Developer configurationThe simplest way to connect to oracle 12C is by using Oracle sql client tool SQL Developer, it uses build-in JDBC driver to make connections and GUI connection wizard guide you fill in the server information without updating ora file. It’s better use version 19.1 and above. fill in all info in connection window, then click Test button to test connection Other client tool connection by ODBC driver such as SSIS and WinSqlThe first step, you need to install the ODBC driver, there is another client tool comes with ODBC driver called Oracle Database Client12cSQLDev 12.1.0.2 R02, install that in your local machine, then update your environment variable to make sure Oracle home directory being added into it. Update environment variableIf you installed both Oracle SQL Developer and Client12cSQLDev 12.1.0.2 R02, there is going to be 2 Oracle folders, one is client_32 which is 32 bit, the other one is client_1 which is 64 bit application, you need to add both of them into you environment variable. The path is usually C:\\app\\product\\12.1.0\\client_* Configurate ora file after setup oracle environment variableThe tricky thing is the ODBC driver only works well for 32 bit version not 64 bit so we can only edit the ora file for client_32. You can find out the ora file on below directory C:\\app\\product\\12.1.0\\client_32\\network\\admin\\tnsnames.ora append following string to the ora file save andclose the ora file, then go to windows ODBC data source center to finalize ODBC driver configuration. Configurate ODBC driver and DSN to test connection Data Source Name is customizable; TNS Service Name is the connection string name in ora file CAP12CPRD_32bit in this case; fill in user ID then click test connection button, a prompt window will pop out to let you input password. SSIS package connection manager configurationOpen SSIS client SSDT to configure connection manager for Oracle data source. SSDT -&gt; open a project -&gt; right click data source on right side panel -&gt; follow the wizard both preinstalled .net provider and native ole db provider for Oracle are working well server name is as the same as connection string name in ora file in this case is CAP12CPRD_32bit. Give a name for oracle new data source then connection manager can be created to Oracle database. SAS connection to Oracle 12C prerequisite: SAS grid server local DSN need to be created before testing; PC SAS login server is necessary SAS grid local DSN for Oracle 12C (ask for system admin create local DSN and return the name to you and it is going to be the value for path when you create new library for Oracle 12C) SAS EG connection to 12C: open SAS EG 7.1 and make sure it connects to grid server open a new code window and create a new library by running below statement 1libname ora12C oracle user=&quot;yourUserName&quot; password=&quot;xxxx&quot; path=sas_grid_local_DSN schema=oracle_schema make sure the program run against SASApp rather than localhost new Oracle library would be created under the SASApp dataset list","link":"/2020/12/11/Oracle-client-side-configuration-for-12C/"},{"title":"Python Environment Setup for Implementation","text":"Python is a good scripting language to boost your productivity on data analysis and BI reporting. As open source language, you can easily get the binary installation file from python official site for windows and source code on vary versions for Linux, in production, it’s better choose installation approach by source code. We also need to setup python environment after installation so that we can not only use python interpreter to develop but also make it executable by CLI and even some ETL tool such as Microsoft SSIS. Python environment variable configuration and local folder set up for your file, package and libraryIf python is installed in system-wise, then you need to create some new folders to store you python file, package and library, e.g. python install path is “D:\\Python36&quot;, then you need to add python executable interpreter to be a part of the PATH variable. Next create python environment variable PYTHONPATH with the following paths and create empty file __init__.py file in each of these folders: create a new folder under D drive “D:\\pyLib” and set that directory as value of PYTHONPATH and create __init__.py file in “D:\\pyLib” you can also create subfolder to assign different permissions for different user group create a subfolder “D:\\pyLib\\AD-group1” and create the __init__.py file in it. create a subfolder “D:\\pyLib\\AD-group2” and create the __init__.py file in it. For Linux, if you install python3 by source code and directory is /usr/local/python3, then edit ~/.bash_profile file, append the python directory into PATH 12# Python3export PATH=/usr/local/python3/bin/:$PATH then run source ~/.bash_profile let setting take effect if your system pre-installed python2 then it’s necessary to make a soft link 12ln -s /user/local/python3/bin/python3 /user/bin/python3ln -s /user/local/python3/bin/pip3 /user/bim/pip3 setup name space and package python scripts for development project to be able to importable create or edit environment variable and add your python files folder into your system directory enter your python file folder to create an empty file __init__.py file open terminal prompt type python to active python interactive console import sys execute sys.path to make sure your python file folder is recognizable by python Python readiness test in localhost for SQL database connection (Anaconda virtual environment)First check python and Ipython version by issue command python --version and ipython --version. Anaconda almost pre-installs all python prevailing and popular libraries in its virtual environment, to check library list by using command pip list Python and SQL database connection facility with supported driversDepends on what python library do you install for database connectivity, it usually comes with function to show you available drivers to connect python to your database, e.g: pyodbc, use drivers() function to list the odbc drivers Python SQL Server database connection and data taskSQL Server database can be connected both by DB API (pyodbc) and ORM (sqlalchemy), create a py script and run sql query from user input 123456789101112131415161718192021222324252627282930313233343536373839import pyodbcimport sqlalchemyfrom sqlalchemy import create_engineimport pandas as pddef pyquery(conn): cnxn = pyodbc.connect(conn) cur = cnxn.cursor() sql_cmd = input(&quot;Input your sql command with database name: &quot;) result = cur.execute(sql_cmd) for row in result: print(row) cur.close() cnxn.close() def py2mssql(): way2conn = input(&quot;Do you want to connect to db by [ORM] or [DBAPI]: &quot;) if way2conn.upper()==&quot;DPAPI&quot;: dsn_str = input(&quot;Do you want to connect to db by [connection string] or [DSN]: &quot;) if dsn_str.upper()==&quot;DSN&quot;: dsn_name=input(&quot;Please enter your DSN name: &quot;) conn_dsn='DSN{};Trusted_connection=yes'.format(dsn_name) conn=conn_dsn pyquery(conn) else: conn_str='DRIVER={SQL Server Native Client 11.0}; SERVER=YOURSERVERNAME;DATABASE=YOURDB;Trusted_connection=yes' conn=conn_str pyquery(conn) else: ser_name=input('Please enter your server name: ') db_name=input('Please enter your database name: ') tbl_name=input('Enter the table name: ') orm_dict={'servername':'{}'.format(ser_name), 'database':'{}'.format(db_name), 'driver':'driver=SQL Server Native Client 11.0'} engine=create_engine('mssql+pyodbc://'+orm_dict['servername']+'/'+orm_dict['database']+'?'+orm_dict['driver']) df=pd.read_sql_table(tbl_name, engine, index_col=0, schema='dbo') print(df.head(10)) if __name__==&quot;__main__&quot;: py2mssql() script both can be ran directly or imported (recommend) after setup PYTHONPATH variable in your account and copy that script over to the path of environment variable. Python DB2 database connection and data taskWe can only connect to DB2 by DBAPI (pyodbc), connection string doesn’t work but only DSN (PRD1 was setup as system DSN in local and server) plus user id and password. Use below script to try to connect 123456789101112131415161718192021222324252627import pyodbcimport getpassimport pandas as pddef py2edw(): uid=getpass.getuser() print(&quot;Your user id is '{}'&quot;.format(uid)) pwd=getpass.getpass(&quot;Please enter your db2 password: &quot;) dsn=input(&quot;Please enter your db2 dsn name: &quot;) conn_dsn='DSN={0}; UID={1}; PWD={2}'.format(dsn, uid, pwd) conn=pyodbc.connect(conn_dsn) cur=conn.cursor() sql_cmd= ''' select * from table ''' result=cur.execute(sql_cmd) for row in result: print(row) cur.close() df=pd.read_sql_query(sql_cmd, conn) print(df.info()) print(df) conn.close() if __name__==&quot;__main__&quot;: py2edw() one thing need to be aware is the script better runs on the shell than python interactive console because pyQT doesn’t support password masking Productionize Python script by passing in parametersIn this section we will demonstrate how you can parameterize your code in python or pyspark so that you can use these techniques before deployed your script into production for automation. It’s best practice to parameterize database names, tables names, and dates so that you can pass these values as inputs to your script. This is beneficial when writing code for values that are dynamic in nature, which can change depending on the environment and/or use case. The key module is from python standard library: sys. Assign variables through sys.argv[...] 1234567891011121314151617import pandas as pdfrom sqlalchemy import create_engineimport sysdef py2mssql(): ser_name=sys.argv[1] # sys.argv[0] is assigned to python script itself, all other parameters start from 1 db_name=sys.argv[2] tbl_name=sys.argv[3] orm_dict={'servername':'{}'.format(ser_name),'database':'{}'.format(db_name), 'driver':'driver=SQL Server Native Client 11.0'} engine=create_engine('mssql+pyodbc://'+orm_dict['servername']+'/'+ orm_dict['database']+'?'+orm_dict['driver']) df=pd.read_sql_table(tbl_name, engine, index_col=0, schema='dbo') df.to_sql('pandas_sql_test', engine, schema='dbo', if_exists='replace', index=False) if __name__==&quot;__main__&quot;: py2mssql() run python script in CLI (command line interface) by following parameter values 1ipython py2mssql_argvdf.py yourSvrNm yourDBNm yourTblNm Encapsulate into SSIS to minimize change in production deployment – python interpreterAn alternative way to apply python in production is leverage current SSIS package and embed python script in process task you can hard code the configuration or use expression (VB) through variables in package Encapsulate into SSIS to minimize change in production deployment – batch processuse batch process by .bat file also can achieve that task Call user defined module or function in python scriptIt’s very efficient to create bunch of generic module packages to contain functions to be used widely by other python scripts for specific tasks. 1. Firstly, setup python environment variable to include directories which are recognized by python 2. Create __init__.py file (can be empty) in these folders 3. Create python programs and save script should end up with if __name__==”__main__“: main() 4. Ready to import user modules and functions 1234567import pandas as pdimport py2mssql_module as dbimport systbl_name=sys.argv[1]df=pd.read_sql_table(tbl_name, db.py2mssql('serverName','databaseName'), index_col=0, schema='dbo')print(df.head(10))","link":"/2020/12/08/Python-Environment-Setup-for-Implementation/"},{"title":"Sandbox solution for BI reporting and business analytics","text":"At beginning, I’d like to share a story from my client and business partner. One day, my team worked on a big marketing project, data from all kinds of source like spreadsheet, csv, mainframe and SQL Server, we had to do cross reference all those data to generate analysis reports but the headache thing was they were isolated and no way to put them into a single unique environment to run one time query then return the results so we could only open multiple windows to compare them by eyeballs. During the project, we often composed some complex queries then ran for a long time to return the result dataset, those datasets were quite important for future further analysis and share with the whole team, but the another panic thing was we could not save those dataset into database due to insufficient access so what we did was copy and paste everything in excel spreadsheet, after for a while, we found number of excel file explode and hard to find the report among those huge files, we feed up with the tedious work and decided created a bunch of views in database but that was also not controlled by us but infrastructure team, all we could do was submit the request then followed infrastructure team’s schedule and waited for month end deployment, no matter how urgent those reports would be. That is the story, I think if you had ever experienced that, that solution might be right for you. You might get the common points from above story, it’s inefficient and even painful if you can only leverage data from data warehouse tables, that is reason why the sandbox database comes up which is a brand new data play zone on production with ability of pipeline to bring multiple sources of data based on major data warehouse you are using. In a brief mark, sandbox is aiming to build a homogeneous solution for heterogenous environment. How to build up and what is the foundation of sandbox database, now let’s take a look into it. The Major data source is so called big db, but it’s data warehouse so that user was only assigned read access which means you can do nothing but only select and query data. Now we carved out two new databases - sandbox and control db. Sandbox database brief introductionsandbox is the new data loading zone for slicing and dicing data in production, like you own backyard playground, you can do almost anything you want to do with it, so user will be granted both read and write permission. control_db that is control console station to provide access gateway for sandbox and also monitor and logging the users’ behaviors, just like a guardian to promise the safety and healthy for sandbox. Let’s take a closer look at these two databases. Sandbox, in general, user can perform 3 kinds of actions, DDL, DML as well as batch job execution based on stored procedure. With DDL command which is database define language, user can create table/view/stored procedure, alter them, even drop them. With DML command which is database manipulate language, user can insert/update/delete data from existing table and also import data from other source. The batch operation is fit for user with programming background and write multiple queries with logical sequence into it like conditional and looping function by using T-SQL scripting language in terms of stored procedure. What things should be considered beforeA very critical point for sandbox database is object ownership, which means you can only create and deal with your own database objects plus read others’ object. Image you data or analytical work were deleted by other accidently, how would you feel at the moment, so the restriction must be setup along with the sandbox database creation to promise the user data safety. Another important thing need to be considered before is user data volume control. Unlike data warehouse, data volume increasing is followed trend and stable for each year, but sandbox database size is unpredictable and totally depends on user personal behavior, in order to prevent non-critical user data dominants your server hard disk, it’s very necessary to set quota for each users. When reach to the limit then User won’t be allowed to create table User also won’t be allowed insert new data into existing table until manually clean data and release space. Creating sandbox database walk throughDefine and create a new database12345use master;goif DB_ID('sandbox') is not nulldrop database sandboxgo in real production, you might need to create a new file group and bunch of file under it for better management. 12345678910create database sandboxon primary(name=N'sandbox', filename=N'G:\\sqldata\\sandbox.mdf', size=1048kB, maxsize=5124KB, filegrowth=512KB),filegroup [yourTeam_sandbox](name=N'yourTeam_sandbox_FG1',filename=N'G:\\sqldata\\yourTeam_sandbox1.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB),(name=N'yourTeam_sandbox_FG2',filename=N'G:\\sqldata\\yourTeam_sandbox2.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB),(name=N'yourTeam_sandbox_FG3',filename=N'G:\\sqldata\\yourTeam_sandbox3.ndf',size=1048KB, maxsize=5124KB, filegrowth=512KB)log on(name=N'yourTeam_sandbox_log',filename=N'H:\\sqllog\\yourTeam_sandbox_log.ldf',size=1048KB, maxsize=5124KB, filegrowth=512KB)go make sure your sandbox and control console database have the same database ownership user 12345678use master;goalter authorization on database::sandboxto sagoalter authorization on database::control_dbto sago make sure your new created file group to be the default file group so that coming data is going to be allocated there 12345use sandbox;goif not exists (select name from sys.filegroups where is_default=1 and name=N'sandbox')alter database sandbox modify filegroup [yourTeam_sandbox] defaultgo Grant user permissionsFirst, you need to provide the basic read access to your business user 1234567use sandbox;goif exists (select * from sys.database_principals where name=N'yourDomain\\yourUserGroup')drop user [yourDomain\\yourUserGroup]elsecreate user [yourDomain\\yourUserGroup] for login [yourDomain\\yourUserGroup] with default_schema=dboexec sp_addrolemember 'db_datareader','yourDomain\\yourUserGroup'; then assign to them DDL and DML permissions 1234567891011121314151617181920use sandbox;gogrant alter any schema to [yourDomain\\yourUserGroup];gogrant create procedure to [yourDomain\\yourUserGroup];gogrant create table to [yourDomain\\yourUserGroup];gogrant create view to [yourDomain\\yourUserGroup];gogrant delete to [yourDomain\\yourUserGroup];gogrant insert to [yourDomain\\yourUserGroup];gogrant select to [yourDomain\\yourUserGroup];gogrant update to [yourDomain\\yourUserGroup];gogrant execute to [yourDomain\\yourUserGroup];go Create database level trigger on DDL query control for rules and restrictions encourage give table name starting with id number which is unique identifier for employee in your company like nameInitial2_tableName must be eligible user, identity check by control database no special characters are allowed in table name like !@#$%^&amp;- no over quota allowed not allowed user drop or delete others’ object and data let’s connect above constraints with workflow chart to be able to see the big picture First of all, we create the database trigger for enforcing object creation rules 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163create trigger ddltrg_create_tableon databasewith execute as 'dbo'for create_tableasbegin try set noncount on; declare @eventdata xml declare @orig_object_name varchar(100) declare @orig_user_name varchar(25) declare @alt_object_name varchar(100) declare @alt_user_name varchar(25) declare @valid_chars varchar(50) declare @create_status char(1) declare @create_err_msg varchar(250) declare @rename_required char(1) declare @trigger_sql varchar(max) declare @schema_name varchar(25) declare @trigger_insert varchar(max) declare @guid varchar(32) declare @overQuota_user table (userId varchar(25) null) set @guid='250' set @valid_chars = '%[^a-zA-Z0-9_]%' set @eventdata = EVENTDATA() set @orig_object_namne = @eventdata.value('(/EVENT_INSTANCE/objectName)[1]','nvarchar(100)') set @orig_user_name = upper(@eventdata.value('(/EVENT/INSTANCE/LoginName)[1]','nvarchar(25)')) set @create_status='Y' set @create_err_msg='' set @alt_user_name = right(@orig_user_name,len(@orig_user_name)-charindex('\\\\',@orig_user_name)) select @schema_name = SCHEMA_NAME(SCHEMA_ID) from sys.tables where name = @orig_object_name if (@alt_user_name = 'yourServiceAccout') return if substring(@orig_object_name,1,len(@alt_user_name)) = @orig_object_name begin set @alt_object_name = @orig_object_name set @rename_required = 'N' end else begin set @alt_object_name = @orig_object_name set @rename_required = 'Y' end /*check the user eligibility*/ if not exists (select * from control_db.dbo.Audit_Sandbox_User where User_ID = @alt_user_name) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' does not have privilege to use the sandbox database' end insert into @overQuota_user select user_id from control_db.dbo.Audit_Sandbox_User where sandbox_limit - current_usage &lt; 0 /*user control for over limit quota*/ if @alt_user_name in (select userId from @overQuota_user) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' exceeded designed space quota, ' + @alt_user_name + ' is not able to ' + N'create new table. Please delete data not needed' + N'to free up space so usage is less than quota 1024MB and rerun query again' end /*check object name eligibility*/ if (patindex(@valid_chars,@orig_object_name) &gt; 0) begin set @create_status = 'N' set @create_err_msg = 'Invalid characters in table name' end /*check object existency*/ if exists(select * from sys.objects where object_id=OBJECT_ID(@alt_object_name) and type in (N'U')) begin if @rename_required = 'Y' begin set @create_status = 'N' set @create_err_msg = 'Table: ' + @alt_object_name + ' already exists' end end /*check schema name eligibility*/ if @schema_name &lt;&gt; 'dbo' begin set @create_status = 'N' set @create_err_msg = 'Table: ' + @orig_object_name + ' not created. Schema is not specified: dbo.&lt;table name&gt;!' end if (@create_status = 'N') begin rollback print @create_err_msg end /*log sandbox event into tracking table in control_db*/ insert control_db.dbo.Audit_Sandbox_Event_Tracking (Event_Type, Event_Time, Event_User, Event_Database, Event_Object_Name ,Event_Object_Type, Event_SQK, Audit_Created_Status, Audit_Error_Message) value (@eventdata.value('(/EVENT_INSTANCE/EventType)[1]','nvarchar(50)'), @eventdata.value('(/EVENT_INSTANCE/PostTime)[1]','datetime'), @Orig_user_name, @eventdata.value('(EVENT_INSTANCE/DatabaseName)[1]','nvarchar(25)'), @orig_object_name, @eventdata.value('(/EVENT_INSTANCE/ObjectType)[1]','nvarchar(25)'), @eventdata.value('(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]','nvarchar(max)'), case when @create_status = 'Y' then 'success' else 'failed' end, @create_err_msg ) if (@create_status = 'N') return /*rename object to add user id prefix*/ begin exec sp_rename @orig_object_name, @alt_object_name print 'Table name is renamed with prefix of your user id' end /*creaet DML trigger on table level*/ set @trigger_sql = 'create trigger DML trig_'+@alt_object_name+ 'ON ' + @alt_object_name + '' set @trigger_sql = @trigger_sql + 'for delete, update, insert' set @trigger_sql = @trigger_sql + 'as' set @trigger_sql = @trigger_sql + 'begin ' set @trigger_sql = @trigger_sql + ' set nocount on;' set @trigger_sql = @trigger_sql + ' declare @user varchar(25)' set @trigger_sql = @trigger_sql + ' select @user = right(SUSER_NAME(),LEN(SUSER_NAME())' set @trigger_sql = @trigger_sql + ' - CHARINDEX(''\\\\'',SUSER_NAME()))' set @trigger_sql = @trigger_sql + ' IF @user &lt;&gt; '''+@alt_user_name+''' ' set @trigger_sql = @trigger_sql + ' begin' set @trigger_sql = @trigger_sql + ' rollback' set @trigger_sql = @trigger_sql + ' print ''User: ''+@user+'' does not have the privilege to perform a DML operation on table' +@alt_object_name + ''' ' set @trigger_sql = @trigger_sql + ' end' set @trigger_sql = @trigger_sql + ' end' exec (@trigger_sql) ; /*create over quota DML trigger on table level*/ create table #overQuota (userId varchar(25) Null) ; set @trigger_sql = 'create trigger DML trig_'+@alt_object_name+ '_insert ON ' + @alt_object_name + '' set @trigger_sql = @trigger_sql + 'after insert' set @trigger_sql = @trigger_sql + 'as' set @trigger_sql = @trigger_sql + 'begin ' set @trigger_sql = @trigger_sql + ' set nocount on;' set @trigger_sql = @trigger_sql + ' declare @user varchar(25)' set @trigger_sql = @trigger_sql + ' select @user = right(SUSER_NAME(),LEN(SUSER_NAME()) - CHARINDEX(''\\\\'',SUSER_NAME()))' set @trigger_sql = @trigger_sql + ' insert into #overQuota' set @trigger_sql = @trigger_sql + ' select user_id from control_db.dbo.Audit_Sandbox_User where sandbox_limit-current_usage&lt;0;' set @trigger_sql = @trigger_sql + ' IF @user in (select userId from #overQuota)' set @trigger_sql = @trigger_sql + ' begin' set @trigger_sql = @trigger_sql + ' rollback' set @trigger_sql = @trigger_sql + ' print ''User: ''+@user+'' is over quota on usage so it can not perform a INSERT operation on table' +@alt_object_name + ', please delete data that is not needed to free up space then rerun the query'' ' set @trigger_sql = @trigger_sql + ' end' set @trigger_sql = @trigger_sql + ' end' exec (@trigger_sql) ; drop table #overQuota ;end trybegin catch declare @err_msg varchar(900), @err_num int, @err_line int, @syserr varchar(900) select @err_msg = ERROR_MESSAGE(), @err_num = ERROR_NUMBER(), @err_line = ERROR_LINE() set @syserr = 'Ended in DDLTRIG_CREATE_TABLE with errors: Line= ' + convert(varchar(10), @err_line) + ', Error Num = ' + convert(varchar(10), @err_num) + ', Error Msg= ' + @err_msg /*save to log file or control_db table*/end catchgoenable trigger [ddltrg_create_table] on databasego One more run need to be applied is prevent user drop object which is not belong to that user. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051create trigger ddltrig_drop_tableon databasewith execute as 'dbo' for drop_tableas set nocount on; declare @eventdata xml declare @orig_object_name varchar(100) declare @orig_user_name varchar(25) declare @alt_user_name varchar(25) declare @create_status char(1) declare @create_err_msg varchar(250) set @eventdata = EVENTDATA() set @orig_object_name = @eventdata.value('(/EVENT_INSTANCE/ObjectName)[1]','nvarchar(100)') set @orig_user_name = upper(@eventdata.value('(/EVENT_INSTANCE/LoginName)[1]','nvarchar(25)')) set @creaet_status = 'Y' set @create_err_msg = '' set @alt_user_name = right(@orig_user_name, len(@orig_user_name) - charindex('\\\\', @orig_user_name)) if @alt_user_name &lt;&gt; 'domainName/yourServiceAccount' begin if not exists (select * from control_db.dbo.Audit_Sandbox_User where User_ID = @alt_user_name) begin set @create_status = 'N' set @create_err_msg = 'User: ' + @alt_user_name + ' does not have privilege to use the sandbox database' end if substring(@orig_object_name, 1, len(@alt_user_name)) &lt;&gt; @alt_user_name begin set @create_status = 'N' set @create_err_msg = 'user: ' + @alt_user_name + ' does not have privilege to drop table: ' + @orig_object_name endendif (@create_Status = 'N')begin rollback print @create_err_msgendinsert control_db.dbo.Audit_Sandbox_Event_Tracking (Event_Type, Event_Time, Event_User, Event_Database, Event_Object_Name ,Event_Object_Type, Event_SQK, Audit_Created_Status, Audit_Error_Message) value (@eventdata.value('(/EVENT_INSTANCE/EventType)[1]','nvarchar(50)'), @eventdata.value('(/EVENT_INSTANCE/PostTime)[1]','datetime'), @Orig_user_name, @eventdata.value('(EVENT_INSTANCE/DatabaseName)[1]','nvarchar(25)'), @orig_object_name, @eventdata.value('(/EVENT_INSTANCE/ObjectType)[1]','nvarchar(25)'), @eventdata.value('(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]','nvarchar(max)'), case when @create_status = 'Y' then 'success' else 'failed' end, @create_err_msg )go At last, one very important setting need to be in placed in case cause issue when [sa] user cross database reference data 1alter database sandbox set TRUSTWORTHY ON; about trustworthy for detail, you can see Microsoft official document on below link Trustworthy database property","link":"/2020/12/02/Sandbox-solution-for-BI-reporting-and-business-analytics/"}],"tags":[{"name":"Python, Pandas, SQL, ETL","slug":"Python-Pandas-SQL-ETL","link":"/tags/Python-Pandas-SQL-ETL/"},{"name":"SQL, query, SQL Server, optimization","slug":"SQL-query-SQL-Server-optimization","link":"/tags/SQL-query-SQL-Server-optimization/"},{"name":"python, automation, ETL, Excel","slug":"python-automation-ETL-Excel","link":"/tags/python-automation-ETL-Excel/"},{"name":"file system, csv","slug":"file-system-csv","link":"/tags/file-system-csv/"},{"name":"SQL Server, database, monitoring, system optimization","slug":"SQL-Server-database-monitoring-system-optimization","link":"/tags/SQL-Server-database-monitoring-system-optimization/"},{"name":"hexo, blog, online image, config, Node.js, router","slug":"hexo-blog-online-image-config-Node-js-router","link":"/tags/hexo-blog-online-image-config-Node-js-router/"},{"name":"Hadoop, Hive, Impala, HDFS, SQL Server, SSIS","slug":"Hadoop-Hive-Impala-HDFS-SQL-Server-SSIS","link":"/tags/Hadoop-Hive-Impala-HDFS-SQL-Server-SSIS/"},{"name":"Linux, centos, ubuntu, network","slug":"Linux-centos-ubuntu-network","link":"/tags/Linux-centos-ubuntu-network/"},{"name":"Linux, MySQL, Config, Database","slug":"Linux-MySQL-Config-Database","link":"/tags/Linux-MySQL-Config-Database/"},{"name":"Linux, Centos, network, config","slug":"Linux-Centos-network-config","link":"/tags/Linux-Centos-network-config/"},{"name":"oracle, SSIS, SAS, config","slug":"oracle-SSIS-SAS-config","link":"/tags/oracle-SSIS-SAS-config/"},{"name":"python, sql, db2, CLI, SSIS, config","slug":"python-sql-db2-CLI-SSIS-config","link":"/tags/python-sql-db2-CLI-SSIS-config/"},{"name":"sandbox, SQL, SQL Server, database","slug":"sandbox-SQL-SQL-Server-database","link":"/tags/sandbox-SQL-SQL-Server-database/"}],"categories":[{"name":"data, BI","slug":"data-BI","link":"/categories/data-BI/"},{"name":"Data","slug":"Data","link":"/categories/Data/"},{"name":"Data, BI","slug":"Data-BI","link":"/categories/Data-BI/"},{"name":"IT","slug":"IT","link":"/categories/IT/"},{"name":"IT, BI","slug":"IT-BI","link":"/categories/IT-BI/"},{"name":"Database, BI","slug":"Database-BI","link":"/categories/Database-BI/"}]}